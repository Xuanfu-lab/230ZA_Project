% Encoding: UTF-8

@Article{Aked-Ko-2017,
  author               = {Aked, Michael and Ko, Amie},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Time Diversification Redux},
  url                  = {https://ssrn.com/abstract=3040967},
  abstract             = {Conventional risk measures may not accurately describe the volatility investors actually experience, especially for portfolios servicing their retirement spending needs. Return volatility rises as its calculated holding period nears 1 year and falls as it lengthens to 10 years. Lower volatility at longer holding periods implies that longer-term mean reversion exists. A portfolio achieves the greatest extra-return benefit by rebalancing over the holding period of highest volatility. Time diversification is helpful, up until long-term uncertainty about the value of reinvested cash flows from dividends leads to rising volatility.},
  citeulike-article-id = {14438504},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3040967},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-09-26 22:40:12},
  timestamp            = {2020-02-25 20:57},
}

@Article{Allen-et-al-2016,
  author               = {Allen, David and McAleer, Michael and Powell, Robert and Singh, Abhay},
  date                 = {2016-06},
  journaltitle         = {Journal of Risk and Financial Management},
  title                = {Down-Side Risk Metrics as Portfolio Diversification Strategies across the Global Financial Crisis},
  doi                  = {10.3390/jrfm9020006},
  number               = {2},
  pages                = {6+},
  volume               = {9},
  abstract             = {This paper features an analysis of the effectiveness of a range of portfolio diversification strategies, with a focus on down-side risk metrics, as a portfolio diversification strategy in a European market context. We apply these measures to a set of daily arithmetically-compounded returns, in U.S. dollar terms, on a set of ten market indices representing the major European markets for a nine-year period from the beginning of 2005 to the end of 2013. The sample period, which incorporates the periods of both the Global Financial Crisis (GFC) and the subsequent European Debt Crisis (EDC), is a challenging one for the application of portfolio investment strategies. The analysis is undertaken via the examination of multiple investment strategies and a variety of hold-out periods and backtests. We commence by using four two-year estimation periods and a subsequent one-year investment hold out period, to analyse a naive 1/N diversification strategy and to contrast its effectiveness with Markowitz mean variance analysis with positive weights. Markowitz optimisation is then compared to various down-side investment optimisation strategies. We begin by comparing Markowitz with CVaR, and then proceed to evaluate the relative effectiveness of Markowitz with various draw-down strategies, utilising a series of backtests. Our results suggest that none of the more sophisticated optimisation strategies appear to dominate naive diversification.},
  citeulike-article-id = {14412283},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/jrfm9020006},
  citeulike-linkout-1  = {http://www.mdpi.com/1911-8074/9/2/6},
  citeulike-linkout-2  = {http://www.mdpi.com/1911-8074/9/2/6/pdf},
  day                  = {21},
  groups               = {Diversification_Measure, Diversified_Invest, Effective_Dim_Diversif, Invest_Diversif},
  posted-at            = {2017-08-10 23:14:01},
  timestamp            = {2020-02-25 20:57},
}

@Article{Baitinger-et-al-2015,
  author               = {Baitinger, Eduard and Kutsarov, Iliya and Maier, Thomas and Storr, Marcus and Wan, Tao},
  date                 = {2015-03},
  journaltitle         = {Credit and Capital Markets - Kredit und Kapital},
  title                = {A Wholistic Approach to Diversification Management: The Diversification Delta Strategy Applied to Non-Normal Return Distributions},
  doi                  = {10.3790/ccm.48.1.89},
  issn                 = {2199-1227},
  number               = {1},
  pages                = {89--119},
  volume               = {48},
  abstract             = {In this paper we study a higher moment diversification measure, the so-called diversification delta (Vermorken et al. (2012)), in a dynamic portfolio optimization context. Particularly, we set up an investment strategy that dynamically maximizes the diversification delta for a given set of assets. Thus, we label the resulting optimized portfolio structure as Maximum Diversification Delta Portfolio (MDDP). Our out-of-sample empirical study reveals that considering crisis-periods, the MDDP is superior to popular investment strategies, such as Minimum-Variance-Portfolio, Risk-Parity-Portfolio and Equally-Weighted-Portfolio, in terms of risk adjusted returns, risk moments and certainty equivalents. However, in line with other diversification measures the MDDP is no longer superior in upward trending markets.},
  citeulike-article-id = {14148591},
  citeulike-linkout-0  = {http://dx.doi.org/10.3790/ccm.48.1.89},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:02:36},
  timestamp            = {2020-02-25 20:57},
}

@Article{Bianchi-et-al-2016a,
  author               = {Bianchi, Robert and Drew, Michael and Walk, Adam},
  date                 = {2016},
  journaltitle         = {Financial Planning Research Journal},
  title                = {The Time Diversification Puzzle: A Survey},
  url                  = {https://www.griffith.edu.au/__data/assets/pdf_file/0025/205729/time-diversification-puzzle-bianchi-drew-walk.pdf},
  abstract             = {Since Samuelson's (1969) theoretical proof that risk and time are unrelated, a half century of debate and controversy has ensued, leaving time diversification as one of the most enduring puzzles of modern finance. The most conspicuous aspect of the debate is the questionable assumptions that underlie much of the analysis. Thus we are left with an unsatisfying debate conducted in a paradigm where terminal wealth is usually a function only of returns, and where time-weighted measures are assumed to adequately evaluate performance. This paper reviews the major streams in the time diversification literature and argues that more realistic analysis using defensible assumptions is likely to lead to better prescriptions for improved retirement investing},
  citeulike-article-id = {14514122},
  groups               = {Diversification_Measure, Invest_Diversif},
  posted-at            = {2018-01-09 16:42:45},
  timestamp            = {2020-02-25 20:57},
}

@TechReport{Carli-et-al-2014,
  author      = {Tiffanie Carli and Romain Deguest and Lionel Martellini},
  date        = {2014},
  institution = {EDHEC-Risk Institute},
  title       = {Improved Risk Reporting with Factor-Based Diversification Measures},
  url         = {https://risk.edhec.edu/publications/improved-risk-reporting-factor-based-diversification-measures},
  abstract    = {This paper analyses various measures of portfolio diversification, and explores the implication in terms of advanced risk reporting techniques. We use the minimal linear torsion approach (Meucci et al. (2013)) to turn correlated constituents into uncorrelated factors, and focus on the effective number of (uncorrelated) bets (ENB), the entropy of the distribution of risk factor contribution to portfolio risk, as a meaningful measure of the degree of diversification in a portfolio.

In an attempt to assess whether a relationship exists between the degree of diversification of a portfolio and its performance in various market conditions, we empirically analyse the diversification of various equity indices and pension fund policy portfolios. We find strong evidence of a significantly positive time-series and cross-sectional relationship between the ENB risk diversification measure and performance in bear markets.

This relationship, however, is highly linear, and the top performing portfolios in severe bear markets are typically portfolios concentrated in safe assets, as opposed to well-diversified portfolios. We also find statistical and economic evidence that this diversification measure has predictive power for equity market returns, a predictive power which becomes substantial over long holding period.

Overall our results suggest that the ENB measure could be a useful addition to the list of risk indicators reported for equity and policy portfolios.},
  groups      = {Diversification_Measure, Effective_Dim_Diversif, Invest_Diversif},
  owner       = {Anne},
  timestamp   = {2020-02-25 20:57},
}

@Article{Carmichael-et-al-2015,
  author               = {Carmichael, Benoit and Koumou, Gilles and Moran, Kevin},
  date                 = {2015-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Unifying Portfolio Diversification Measures Using Rao's Quadratic Entropy},
  url                  = {https://ssrn.com/abstract=2610814},
  abstract             = {This paper extends the use of Rao (1982b)'s Quadratic Entropy (RQE) to modern portfolio theory. It argues that the RQE of a portfolio is a valid, flexible and unifying approach to measuring portfolio diversification. The paper demonstrates that portfolio's RQE can encompass most existing measures, such as the portfolio variance, the diversification ratio, the normalized portfolio variance, the diversification return or excess growth rates, the Gini-Simpson indices, the return gaps, Markowitz's utility function and Bouchaud's general free utility. The paper also shows that assets selected under RQE can protect portfolios from mass destruction (systemic risk) and an empirical illustration suggests that this protection is substantial.},
  citeulike-article-id = {13997244},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2610814},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2610814code361712.pdf?abstractid=2610814 and mirid=1},
  day                  = {26},
  groups               = {Diversification_Measure, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-04 22:58:50},
  timestamp            = {2020-02-25 20:57},
}

@Article{Cesarone-Colucci-2018,
  author         = {Cesarone, Francesco and Colucci, Stefano},
  date           = {2018-02},
  journaltitle   = {Journal of the Operational Research Society},
  title          = {Minimum risk versus capital and risk diversification strategies for portfolio construction},
  doi            = {10.1057/s41274-017-0216-5},
  issn           = {0160-5682},
  number         = {2},
  pages          = {183--200},
  volume         = {69},
  abstract       = {In this paper, we propose an extensive empirical analysis on three categories of portfolio selection models with very different objectives: minimization of risk, maximization of capital diversification, and uniform distribution of risk allocation. The latter approach, also called Risk Parity or Equal Risk Contribution (ERC), is a recent strategy for asset allocation that aims at equally sharing the risk among all the assets of the selected portfolio. The risk measure commonly used to select ERC portfolios is volatility. We propose here new developments of the ERC approach using Conditional Value-at-Risk (CVaR) as a risk measure. Furthermore, under appropriate conditions, we also provide an approach to find a CVaR ERC portfolio as a solution of a convex optimization problem. We investigate how these classes of portfolio models (Minimum-Risk, Capital-Diversification, and Risk-Diversification) work on seven investment universes, each with different sources of risk, including equities, bonds, and mixed assets. Then, we highlight some strengths and weaknesses of all portfolio strategies in terms of various performance measures.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 20:57},
}

@Article{Cesarone-et-al-2018,
  author         = {Cesarone, Francesco and Moretti, Jacopo and Tardella, Fabio},
  date           = {2018-02-07},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Why Small Portfolios Are Preferable and How to Choose Them},
  url            = {https://ssrn.com/abstract=3154353},
  abstract       = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. However, this principle does not necessarily translate into a request for investing in all the assets of the investment universe. Indeed, following a line of research started by Evans and Archer almost 50 years ago, we provide here further evidence that small portfolios are sufficient to achieve almost optimal in-sample risk reduction with respect to variance and to some other popular risk measures, and very good out-of-sample performances. While leading to similar results, our approach is significantly different from the classical one pioneered by Evans and Archer. Indeed, we describe models for choosing the portfolio of a prescribed size with the smallest possible risk, as opposed to the random portfolio choice investigated in most of the previous works. We find that the smallest risk portfolios generally require no more than 15 assets. Furthermore, it is almost always possible to find portfolios that are just 1\% more risky than the smallest risk portfolios and contain no more than 10 assets. The preference for small optimal portfolios is also justified by recent theoretical results on the estimation errors for the parameters required by portfolio selection models. Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure},
  timestamp      = {2020-02-25 20:57},
}

@Article{Cesarone-Moretti-2016,
  author         = {Cesarone, Francesco and Moretti, Jacopo},
  date           = {2016},
  journaltitle   = {Economics Bulletin},
  title          = {Optimally chosen small portfolios are better than large ones},
  number         = {4},
  translator     = {Tardella, Fabio},
  volume         = {36},
  abstract       = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. However, this principle does not necessarily translate into a request for investing in all the assets of the investment universe. Indeed, following a line of research started by Evans and Archer almost fifty years ago, we provide here further evidence that small portfolios are sufficient to achieve almost optimal in-sample risk reduction with respect to variance and to some other popular risk measures, and very good out-of-sample performances. While leading to similar results, our approach is significantly different from the classical one pioneered by Evans and Archer. Indeed, we describe models for choosing the portfolio of a prescribed size with the smallest possible risk, as opposed to the random portfolio choice investigated in most of the previous works. We find that the smallest risk portfolios generally require no more than 15 assets. Furthermore, it is almost always possible to find portfolios that are just 1\% more risky than the smallest risk portfolios and contain no more than 10 assets. Furthermore, the optimal small portfolios generally show a better performance than the optimal large ones. Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure},
  timestamp      = {2020-02-25 20:57},
}

@Article{Chollete-et-al-2012,
  author       = {Chollete, L. and de la Pena, V. and Lu, C.},
  date         = {2012},
  journaltitle = {Journal of Banking and Finance},
  title        = {International diversification: An extreme value approach},
  number       = {3},
  pages        = {871--885},
  url          = {https://www.sciencedirect.com/science/article/pii/S0378426611002767},
  volume       = {36},
  abstract     = {International diversification has costs and benefits, depending on the degree of asset dependence. We study international diversification with two dependence measures: correlations and extreme dependence. We discover that dependence has typically increased over time, and document mixed evidence on heavy tails in individual countries.

Moreover, we uncover three additional findings related to dependence. First, the timing of downside risk differs depending on the region. Surprisingly, recent Latin American returns exhibit little downside risk. Second, Latin America exhibits a great deal of correlation complexity. Third, according to the empirical results, correlation does not vary with returns, but extreme dependence does vary monotonically with regional returns.

Our results are consistent with a tradeoff between international diversification and systemic risk. They also suggest international limits to diversification, and that international investors demand some compensation for joint downside risk during extreme events.},
  groups       = {Diversification_Measure, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 20:57},
}

@Article{Ayres-Nalebuff-2013,
  author               = {Ayres, Ian and Nalebuff, Barry},
  date                 = {2013-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Diversification Across Time},
  doi                  = {10.3905/jpm.2013.39.2.073},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {73--86},
  volume               = {39},
  abstract             = {Young people who buy stock on margin and reduce their equity exposure as they age can reduce lifetime portfolio risk. For example, an initially leveraged portfolio produces the same mean accumulation as a constant 74 percent stock allocation with a 21 percent smaller standard deviation. Since the means are equal, the reduced volatility doesn't depend on the equity premium.

A leveraged life-cycle strategy also lets investors come closer to their utility-maximizing equity allocation. Monte Carlo simulations show that the gains continue even with equity premiums well below historical levels.},
  citeulike-article-id = {13972068},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.2.073},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:43:35},
  timestamp            = {2020-02-25 21:00},
}

@PhdThesis{Bektic-2018,
  author         = {Bektic, Demir},
  date           = {2018},
  institution    = {Technical University of Darmstadt},
  title          = {Factor-based Portfolio Management with Corporate Bonds},
  type           = {phdthesis},
  url            = {https://tuprints.ulb.tu-darmstadt.de/7272/},
  abstract       = {Over the past 50 years financial asset pricing theories have evolved from simple single-factor models to more complex multi-factor models. Initially, Sharpe (1964) Capital Asset Pricing Model (CAPM) postulated that security markets can be described by a single factor (market beta). The basic premise of the model is that market participants require a risk premium for investing in high-beta assets that are typically considered more risky than low-beta assets. However, in the aftermath of the 2008 global financial crisis, two major trends emerged in the investment industry that laid the groundwork for the rise of factor-based investment strategies: 1) Investors started to evaluate and implement portfolio diversification in terms of underlying systematic risk factors given the failure of active management to provide adequate downside protection. 2) Investors demanded cost-effective, transparent and systematic alternative investment vehicles that could capture most or at least parts of active managers excess return. As a consequence, factor-based investing has grown in popularity and rapidly attracted academics, asset managers and institutional investors.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Multifactor_Invest},
  timestamp      = {2020-02-25 21:00},
}

@Article{Benichou-et-al-2017,
  author               = {Benichou, Raphael and Lemperiere, Yves and Serie, Emmanuel and Kockelkoren, Julien and Seager, Philip and Bouchaud, Jean P. and Potters, Marc},
  date                 = {2017-06},
  journaltitle         = {Journal of Investment Strategies},
  title                = {Agnostic risk parity: taming known and unknown unknowns},
  doi                  = {10.21314/jois.2017.083},
  issn                 = {2047-1238},
  number               = {3},
  pages                = {1--12},
  volume               = {6},
  abstract             = {Markowitz' celebrated optimal portfolio theory generally fails to deliver out-of-sample diversification. In this note, we propose a new portfolio construction strategy based on symmetry arguments only, leading to "Eigenrisk Parity"portfolios that achieve equal realized risk on all the principal components of the covariance matrix. This holds true for any other definition of uncorrelated factors. We then specialize our general formula to the most agnostic case where the indicators of future returns are assumed to be uncorrelated and of equal variance. This "Agnostic Risk Parity"(AGP) portfolio minimizes unknown-unknown risks generated by over-optimistic hedging of the different bets. AGP is shown to fare quite well when applied to standard technical strategies such as trend following.},
  citeulike-article-id = {14386377},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jois.2017.083},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Risk},
  posted-at            = {2017-07-02 23:52:46},
  timestamp            = {2020-02-25 21:00},
}

@Article{Berger-et-al-2013a,
  author               = {Berger, Dave and Pukthuanthong, Kuntara and Yang, Jimmy J.},
  date                 = {2013-07-31},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Is the Diversification Benefit of Frontier Markets Realizable by Mean-Variance Investors? The Evidence of Investable Funds},
  doi                  = {10.3905/jpm.2013.39.4.036},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {36--48},
  volume               = {39},
  abstract             = {The authors investigate whether the diversification benefits of frontier markets are realizable. They focus on investable frontier exchange-traded funds (ETFs) and their corresponding indices. Their analysis ncludes directly measuring the economic benefits of frontier-market diversification, as well as considering frontier-market trading dynamics. Evidence indicates that frontier markets offer diversification benefits through risk-reducing potential. The authors find that frontier market volatility tends to be largely idiosyncratic, which supports the risk-reducing role of frontier markets. Their comparison of funds and indices indicates that, to the extent that frontier-market indices offer hypothetical benefits, traders can obtain these benefits by using investable funds.},
  citeulike-article-id = {14504915},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.4.036},
  day                  = {31},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-18 21:50:53},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bernardi-et-al-2018,
  author               = {Bernardi, Simone and Leippold, Markus and Lohre, Harald},
  date                 = {2018-01},
  journaltitle         = {European Financial Management},
  title                = {Maximum diversification strategies along commodity risk factors},
  doi                  = {10.1111/eufm.12122},
  issn                 = {1354-7798},
  number               = {1},
  pages                = {53--78},
  volume               = {24},
  abstract             = {Pursuing risk-based allocation across a universe of commodity assets, we find diversified risk parity (DRP) strategies to provide convincing results. DRP strives for maximum diversification along uncorrelated risk sources. A straightforward way to derive uncorrelated risk sources relies on principal components analysis (PCA). While the ensuing statistical factors can be associated with commodity sector bets, the corresponding DRP strategy entails excessive turnover because of the instability of the PCA factors. We suggest an alternative design of the DRP strategy relative to common commodity risk factors that implicitly allows for a uniform exposure to commodity risk premia.},
  citeulike-article-id = {14521503},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/eufm.12122},
  groups               = {Diversified_Invest, Invest_Risk, Invest_Cmdty, Invest_Diversif},
  posted-at            = {2018-01-22 17:16:36},
  timestamp            = {2020-02-25 21:00},
}

@Article{Binstock-et-al-2017,
  author               = {Binstock, Jay and Kose, Engin and Mazzoleni, Michele},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Diversification Strikes Again: Evidence from Global Equity Factors},
  url                  = {https://ssrn.com/abstract=3036423},
  abstract             = {The benefits of country diversification are well established. This article shows that the same benefits extend to equity factors, such as value, size, momentum, investment, and profitability. Specifically, country factor portfolios reflect both common variation, which we define as the global factor, and local variation. On average, a US investor could enjoy a 30 percent reduction in portfolio volatility by investing globally. We also document three other properties of equity factors. Like major asset classes, greater market integration is associated with greater factor co-movement, and factor portfolios of different countries tend to be more correlated during bear stock markets. However, unlike asset classes, the correlations of factor portfolios across countries have not been increasing over the last two decades, making global equity factors a particularly desirable addition to a portfolio.},
  citeulike-article-id = {14433817},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3036423},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-09-18 20:32:50},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bock-2018,
  author         = {Bock, Johannes},
  date           = {2018-11-20},
  journaltitle   = {arXiv Electronic Journal},
  title          = {An updated review of (sub-)optimal diversification models},
  url            = {https://arxiv.org/abs/1811.08255},
  abstract       = {In the past decade many researchers have proposed new optimal portfolio selection strategies to show that sophisticated diversification can outperform the naive 1/N strategy in out-of-sample benchmarks. Providing an updated review of these models since DeMiguel et al. (2009b), I test sixteen strategies across six empirical datasets to see if indeed progress has been made. However, I find that none of the recently suggested strategies consistently outperforms the 1/N or minimum-variance approach in terms of Sharpe ratio, certainty-equivalent return or turnover. This suggests that simple diversification rules are not in fact inefficient, and gains promised by optimal portfolio choice remain unattainable out-of-sample due to large estimation errors in expected returns. Therefore, further research effort should be devoted to both improving estimation of expected returns, and possibly exploring diversification rules that do not require the estimation of expected returns directly, but also use other available information about the stock characteristics.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:00},
}

@Article{Boigner-Gadzinski-2015,
  author               = {Boigner, Philip and Gadzinski, Gregory},
  date                 = {2015-03},
  journaltitle         = {Journal of Asset Management},
  title                = {Diversification with risk factors and investable hedge fund indices},
  doi                  = {10.1057/jam.2015.10},
  issn                 = {1470-8272},
  number               = {2},
  pages                = {101--116},
  volume               = {16},
  abstract             = {This article complements existing studies on dynamic portfolio construction by implementing a wide spectrum of optimization methodologies on four types of investments: traditional asset classes, risk factors and premia, and hedge fund investable indices. Portfolios are constructed using four different objectives with weights re-allocated every month during the period 2002-2013.

We show evidence that including hedge fund investable indices in a traditional portfolio is effective in mitigating volatility and drawdown. Risk factors portfolios do not benefit from the inclusion of hedge funds though. Moreover, we provide guidance on which allocation techniques should be used given the nature of the assets in one's portfolio.},
  citeulike-article-id = {13967863},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2015.10},
  groups               = {Diversified_Invest, Hedge_Funds, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-05 12:59:13},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bouchey-et-al-2017,
  author         = {Bouchey, Paul and Li, Tianchuan and Nemtchinov, Vassilii},
  date           = {2017-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {Systematic Diversification Using Beta},
  url            = {https://joi.pm-research.com/content/26/3/144},
  abstract       = {Higher-risk investments deserve higher expected returns to compensate for the extra risk, or so theory tells us. Historically, this has not always been the case for U.S. and other developed-market stocks. This anomaly, which is now well established by academics, has started to gain traction with investors. This is demonstrated by the large flows into the numerous low-beta and low-volatility strategies that were established in the wake of the Global Financial Crisis. The authors examine the beta anomaly in the academic literature and provide an empirical analysis for stocks in the U.S., developed, and emerging markets. Their primary finding is that beta is not a strong predictor of expected returns, but it is useful when used to help reduce risk in a portfolio. The authors present results for an investment strategy that filters out the highest-beta stocks while controlling for concentration risks by country and sector. The study finds mixed results for beta as an anomaly: Low beta outperforms for international markets, underperforms in emerging markets, and is flat in the U.S. markets. None of these differences appears to be statistically significant. The authors find beta is very useful, however, as a tool for controlling risk, especially in the context of strategies that diversify across countries and sectors.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 21:00},
}

@Article{Carl-2017,
  author               = {Carl, Ulrich},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Power of Equity Factor Diversification},
  url                  = {https://ssrn.com/abstract=2915443},
  abstract             = {This paper analyses the diversification properties of country equity factors across six equity factors and twenty developed markets from 1991 to 2015. The factors considered are the market excess return, size, value, momentum, low beta and quality. I find substantial diversification benefits along the country dimension as well as the factor dimension. In a portfolio setting, country diversification significantly reduces the volatility compared to single country investing for each of the six equity factors. Factor diversification works in each of the twenty markets by means of reducing the portfolio volatility.},
  citeulike-article-id = {14511760},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2915443},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-03 21:20:24},
  timestamp            = {2020-02-25 21:00},
}

@Article{Cesarone-et-al-2014,
  author               = {Cesarone, Francesco and Moretti, Jacopo and Tardella, Fabio},
  date                 = {2014},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Does Greater Diversification Really Improve Performance in Portfolio Selection?},
  doi                  = {10.2139/ssrn.2473630},
  issn                 = {1556-5068},
  abstract             = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. This seems to require that in a given working universe, or market, the investment should be spread among all (or almost all) the available assets. Indeed, this is what some classical investment strategies, like Equally-Weighted portfolios, or more recent and refined ones, like Risk Parity, actually recommend.

The purpose of this work consists in giving some empirical evidence of the fact that diversifying through the use of larger portfolios is not the best way to achieve an improvement in out-of-sample performance. More precisely, we investigate the role of the restriction on the number of assets in a portfolio (a cardinality constraint) on the in-sample and out-of-sample outcomes of the Equally-Weighted approach and of some well-known portfolio selection models that minimize risk through the use of Variance, Semi-Mean Absolute Deviation, and Conditional Value-at-Risk.

Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  citeulike-article-id = {14320281},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2473630},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-26 19:16:17},
  timestamp            = {2020-02-25 21:00},
}

@Article{Chambers-Zdanowicz-2014,
  author               = {Chambers, Donald R. and Zdanowicz, John S.},
  date                 = {2014-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Limitations of Diversification Return},
  doi                  = {10.3905/jpm.2014.40.4.065},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {65--76},
  volume               = {40},
  abstract             = {Diversification return is the amount by which the geometric mean return (i.e., average compounded return) of a portfolio exceeds the weighted average of the geometric means of the portfolio's constituent assets. Diversification return has been touted as a source of added return, even if markets are informationally efficient. Portfolio rebalancing has been advocated as a valuable source of diversification return.

The authors demonstrate that diversification return is not a source of increased expected value. However, portfolio rebalancing can be an effective mean-reverting strategy. Any enhanced expected value from rebalancing emanates from mean-reversion, rather than from diversification or variance reduction.},
  citeulike-article-id = {13972189},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2014.40.4.065},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 09:05:15},
  timestamp            = {2020-02-25 21:00},
}

@Article{Choueifaty-Coignard-2008,
  author       = {Choueifaty, Y. and Coignard, Y.},
  date         = {2008},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Toward maximum diversification},
  number       = {1},
  pages        = {40--51},
  url          = {https://jpm.pm-research.com/content/35/1/40},
  volume       = {35},
  abstract     = {Along with the ongoing effort to build market cap-independent portfolios, the authors explore the properties of diversification as a driver of portfolio construction. They introduce a measure of the diversification of a portfolio that they term the diversification ratio. The measure is then employed to build a risk-efficient portfolio, or the Most- Diversified Portfolio. The theoretical properties of the resulting portfolios are discussed and compared to other popular methodologies, such as market-cap weights, equal weights, and minimum variance. The empirical results confirm that these popular methodologies are dominated by risk-efficient portfolios in many aspects. The implication is that in the long run, actively managed portfolios that maximize diversification are strong candidates for achieving consistently better results than commonly used passive index tracking methodologies. The message is clear- investors and their trustees cannot afford to ignore the benefits of maximal diversification.},
  groups       = {Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Christophe-2017,
  author               = {Christophe, Stephen E.},
  date                 = {2017-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Time to Stay Home?Global Diversification during the Past 25 Years},
  doi                  = {10.3905/jwm.2017.2017.1.053},
  issn                 = {1534-7524},
  abstract             = {Over the past 25 years, as financial markets have become increasingly integrated, the role of foreign equities in a well-diversified portfolio has become increasingly uncertain. We present evidence that a globally diversified portfolio underperforms, on average, a U.S.-only allocation. Investors should not be overly optimistic about the potential benefits of international portfolio diversification.},
  citeulike-article-id = {14338557},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.2017.1.053},
  day                  = {07},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-17 11:21:53},
  timestamp            = {2020-02-25 21:00},
}

@Article{Chua-et-al-2009,
  author       = {Chua, D. B. and Kritzman, M. and Page, S.},
  date         = {2009},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The Myth of Diversification},
  number       = {1},
  pages        = {26--35},
  url          = {https://jpm.pm-research.com/content/36/1/26},
  volume       = {36},
  abstract     = {Perhaps the most universally accepted precept of prudent investing is to diversify, yet this precept grossly oversimplifies the challenge of portfolio construction. Correlations, as typically measured over the full sample of returns, often belie an asset's diversification properties in market environments when diversification is most needed. Moreover, upside diversification is undesirable. The authors first describe the mathematics of conditional correlations assuming returns are normally distributed. Then they present empirical results across a wide variety of assets, which reveal that, unlike the theoretical conditional correlations, empirical correlations are significantly asymmetric. Finally, the authors show that a portfolio construction technique called full-scale optimization produces portfolios in which the component assets exhibit relatively lower correlations on the downside and higher correlations on the upside than mean-variance optimization portfolios.},
  groups       = {PortfOptim_FullScale, Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Clarke-et-al-2013,
  author       = {Roger Clarke and Harindra de Silva and Steven Thorley},
  date         = {2013},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Risk Parity, Maximum Diversification, and Minimum Variance: An Analytic Perspective},
  url          = {https://jpm.pm-research.com/content/39/3/39},
  abstract     = {Analytic solutions to risk parity, maximum diversification, and minimum variance portfolios provide useful perspectives about their construction and composition. Individual asset weights depend on both systematic and idiosyncratic risk in all three risk-based portfolios, but systematic risk eliminates many investable assets in long-only, constrained, maximum-diversification, and minimum-variance portfolios. On the other hand, risk-parity portfolios include all investable assets, and idiosyncratic risk has little effect on weight magnitude.

The algebraic forms for optimal asset weights derived in this article yield generalizable properties of risk-based portfolios, in contrast to empirical simulations that employ a specific set of historical returns, proprietary risk models, and multiple constraints. These analytic solutions reveal precisely how various kinds of predicted risk affect the relative magnitude of security weights in each type of risk-based portfolio construction.},
  groups       = {Risk_Budgeting, Diversified_Invest, Invest_Risk, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Bergin-Pyun-2016,
  author               = {Bergin, Paul R. and Pyun, Ju H.},
  date                 = {2016-04},
  journaltitle         = {Journal of International Money and Finance},
  title                = {International portfolio diversification and multilateral effects of correlations},
  doi                  = {10.1016/j.jimonfin.2015.12.012},
  issn                 = {0261-5606},
  pages                = {52--71},
  volume               = {62},
  abstract             = {Bilateral asset holdings depend on the correlation with all other countries. Higher stock return correlations lower bilateral equity asset holdings. Multilateral effects of correlations bias estimates. Not only are investors biased toward home assets, but when they do invest abroad, they appear to favor countries with returns more correlated with home assets. Often attributed to a preference for familiarity, this 'correlation puzzle' further reduces effective diversification. We use a multi-country general equilibrium model of portfolio choice to study how bilateral equity holdings are affected by return correlations among alternative destination and source countries. From the theoretical model, we develop an empirical approach to estimate a gravity equation for equity holdings that incorporates the overall covariance structure in a theoretically rigorous yet tractable manner. Estimation using this approach resolves the correlation puzzle, and finds that international investors do seek the diversification benefits of low cross-country correlations, as theory would predict.},
  citeulike-article-id = {14339368},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jimonfin.2015.12.012},
  groups               = {Effective_Dim_Diversif, Invest_Diversif},
  posted-at            = {2017-04-19 06:32:43},
  timestamp            = {2020-02-25 21:00},
}

@InCollection{DeSilva-et-al-2017,
  author               = {{De Silva}, Harindra and McMurran, Gregory M. and Miller, Megan N.},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversification and the Volatility Risk Premium},
  doi                  = {10.1016/b978-1-78548-201-4.50014-3},
  isbn                 = {9781785482014},
  pages                = {365--387},
  publisher            = {Elsevier},
  abstract             = {The volatility risk premium (VRP) found in options has paid off persistently across different assets, different asset classes and over time. A consistent short volatility position using options or volatility swaps has produced attractive risk-adjusted returns because of exposure to VRP. In this chapter, we have extended the study of the VRP to include not only equity indices but also commodities, government bonds and currencies. Using volatility swap returns as a measure of the payoff to the VRP, we see that the returns to a short volatility position are correlated to the volatility of the underlying instrument and to other VRPs in the same asset class. We also find that the returns are relatively uncorrelated to the VRPs of other asset classes and to the traditional equity factors represented by pure factor portfolios (PFPs). Finally, we show that the multiasset class VRP portfolio studied in this chapter has very competitive risk-adjusted returns},
  citeulike-article-id = {14499078},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50014-3},
  groups               = {Diversified_Invest, RiskPremia_FixedIncome, RiskPremia_Other, Invest_Diversif, [nbkcbu3:]},
  posted-at            = {2017-12-08 00:25:33},
  timestamp            = {2020-02-25 21:06},
}

@Article{Dickson-2016a,
  author               = {Dickson, Mike},
  date                 = {2016-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Naive Diversification Isn't So Naive After All},
  url                  = {https://ssrn.com/abstract=2713501},
  abstract             = {I conduct a horse-race of 15 portfolio construction techniques over 8 empirical datasets comprised of individual stocks. I also conduct a robust Monte Carlo analysis that confirms that recent extensions of mean-variance optimization due to Kirby and Ostdiek (2012) are successful in curbing estimation risk and turnover. Despite these facts, my results indicate that no strategy consistently outperforms naive diversification in terms of mean excess return, Sharpe ratio, and turnover. I introduce a statistic, the time series average of the cross-sectional mean absolute deviation of risk and return, to explain why I observe these results. Data limitations and dataset characteristics contribute the most to the performance of a candidate strategy. I also propose several extensions to active timing strategies and include new characteristics in a parametric portfolio choice framework. Naive diversification continues to prevail, suggesting practical optimization techniques are inferior to naive diversification when forming portfolios of individual stocks.},
  citeulike-article-id = {14134601},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2713501},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2713501code2352812.pdf?abstractid=2713501 and mirid=1},
  day                  = {19},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-09-11 12:16:21},
  timestamp            = {2020-02-25 21:06},
}

@Article{Diyarbakrloglu-Satman-2013,
  author               = {Diyarbakrloglu, Erkin and Satman, Mehmet H.},
  date                 = {2014-01},
  journaltitle         = {Journal of Asset Management},
  title                = {The Maximum Diversification Index},
  doi                  = {10.1057/jam.2013.28},
  issn                 = {1470-8272},
  number               = {6},
  pages                = {400--409},
  volume               = {14},
  abstract             = {We propose a new method to assess the risk diversification potential of a given investment set, using only the information content of the covariance matrix of returns. Namely, we extend Rudin and Morgan's (2006) work to numerically solve for the 'Maximum Diversification Index' by means of a genetic algorithm.

Using stock returns data from the S and P-500 index, we show that the MDI can be efficiently implemented to delimit a large set of investable assets by eliminating those subjects that do not improve the diversification characteristics of the underlying portfolio pool. Indeed, a subset of the S and P-500 stocks obtained using the MDI procedure preserves the mean-variance properties of the initial dataset as shown by the ex-post efficient frontiers.},
  citeulike-article-id = {13968912},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2013.28},
  day                  = {16},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-06 06:31:14},
  timestamp            = {2020-02-25 21:06},
}

@Article{duPlessis-vanRensburg-2017,
  author               = {{du Plessis}, Hannes and {van Rensburg}, Paul},
  date                 = {2017-06},
  journaltitle         = {Investment Analysts Journal},
  title                = {Diversification and the realised volatility of equity portfolios},
  doi                  = {10.1080/10293523.2017.1335367},
  pages                = {1--22},
  abstract             = {In Markowitz's (1952) portfolio theory, a reduction in volatility for a given level of expected return is implied as being equivalent to an increase in diversification. The recent development of risk-based portfolio construction methods, which emphasise diversification separately from volatility reduction, challenges this equivalence. Using a point-in-time database of liquid equities listed on the Johannesburg Stock Exchange between 1998 and 2016, a numerical simulation technique is employed to study the behaviour of a range of diversification measures as a portfolio-level attribute and assess and compare their usefulness in estimating out-of-sample portfolio volatility. The empirical performance of maximum diversification portfolios based on each measure is then investigated. It is found that a portfolio?s diversification level is a significant predictor of future portfolio risk beyond that of historic volatility, and that the empirical performance of maximum diversification portfolios, attractive in all cases, depends critically on the definition of diversification applied.},
  citeulike-article-id = {14388871},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10293523.2017.1335367},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10293523.2017.1335367},
  day                  = {26},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-07-06 13:37:32},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 21:06},
}

@TechReport{Durante-et-al-2015a,
  author               = {Durante, Fabrizio and Foscolo, Enrico and Pappada, Roberta and Wang, Hao},
  date                 = {2015},
  institution          = {DEAMS Research Paper Series},
  title                = {A portfolio diversification strategy via tail dependence measures},
  url                  = {https://www.openstarts.units.it/handle/10077/11865},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures a first-step cluster analysis is carried out for discerning between assets with the same performance during risky scenarios. Then a mean-variance efficient frontier is computed by fixing a number of assets per portfolio and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected index components in trouble periods may improve the risk-averse investor portfolio performance.},
  citeulike-article-id = {14150078},
  citeulike-linkout-0  = {https://www.openstarts.units.it/dspace/bitstream/10077/11865/1/DEAMSRP20153DuranteFoscoloPappadaWang.pdf},
  groups               = {Networks and investment management, Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:48:29},
  timestamp            = {2020-02-25 21:06},
}

@Article{Facchinato-Pola-2014,
  author       = {Simone Facchinato and Gianni Pola},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {Managing uncertainty with diversification across macroeconomic scenarios (DAMS): from asset segmentation to portfolio},
  url          = {http://research-center.amundi.com/page/Publications/Discussion-Paper/2014/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  abstract     = {Recent history has provided an excellent laboratory to test the robustness of investment processes. Despite claims of diversification, most balanced portfolios and pension funds were concentrated on equity risk and, consequently, key investment decisions ultimately consisted in a single binary bet: buy or sell equity. This led to pro-cyclical returns and generated a broad debate on the effectiveness of active management in generating performance in difficult market conditions.

In 2011 AMUNDI Italy decided to revise the asset allocation process starting with a reinterpretation of portfolio diversification in terms of Diversification Across Macroeconomic Scenarios (DAMS). The main ambitions of DAMS are: (i) to explain complex patterns of large investment universes in terms of a limited number of factors and (ii) to catch up the market risk premium without being exposed to specific macroeconomic dynamics and asset idiosyncratic risk. In a previous study we illustrated the DAMS principle and implications in terms of asset segmentation.

The aim of this paper is to move towards a new framework for multi-asset portfolio management, what we call DAMS second generation. DAMS first generation is enriched with new concepts and tools that enable us (i) to infer market expectations on relevant macroeconomic factors (growth and inflation) and global risk premium, and (ii) to properly manage portfolios via strategic and tactical asset allocation.},
  groups       = {Diversified_Invest, Scenario_Market, Scenario_Portfolio, Invest_Diversif},
  howpublished = {Available at http://research-center.amundi.com/page/Publications/Discussion-Paper/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  organization = {Amundi},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Flint-et-al-2016c,
  author               = {Flint, Emlyn J. and Chikurunhe, Florence and Seymour, Anthony J.},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Cost of a Free Lunch: Dabbling in Diversification},
  url                  = {https://ssrn.com/abstract=2767436},
  abstract             = {It's often said that diversification is the only 'free lunch' available to investors; meaning that a properly diversified portfolio reduces total risk without necessarily sacrificing expected return. However, achieving true diversification is easier said than done, especially when we don't fully know what we mean when we're talking about diversification. While the qualitative purpose of diversification is well-known, a satisfactory quantitative definition of portfolio diversification is not. In this report, we summarise a wide range of diversification measures, focussing our efforts on those most commonly used in practice. We categorise each measure based on which portfolio aspect it focusses on: cardinality, weights, returns, risk or higher moments. We then apply these measures to a range of South African equity indices, thus giving a diagnostic review of historical local equity diversification and, perhaps more importantly, providing a description of the investable opportunity set available to fund managers in this space. Finally, we introduce the idea of diversification profiles. These regime-dependent profiles give a much richer description of portfolio diversification than their single-value counterparts and also allow one to manage diversification proactively based on one's view of future market conditions.},
  citeulike-article-id = {14186293},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2767436},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-17 22:18:46},
  timestamp            = {2020-02-25 21:06},
}

@Article{Flores-et-al-2017,
  author               = {Flores, Yuri Salazar and Bianchi, Robert J. and Drew, Michael E. and Truck, Stefan},
  date                 = {2017-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Diversification Delta: A Different Perspective},
  doi                  = {10.3905/jpm.2017.43.4.112},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {112--124},
  volume               = {43},
  abstract             = {In a 2012 article published in The Journal of Portfolio Management, Vermorken, Medda, and Schroder introduce a new measure of diversification, the Diversification Delta (DD), based on the entropy of the portfolio return distribution. Entropy as a measure of uncertainty has been used successfully in several frameworks and takes into account the entire statistical distribution, rather than just the first two moments. In this article, the authors highlight some drawbacks of the DD measure and go on to propose an alternative measure based on exponential entropy that overcomes the identified shortcomings. The authors present the properties of this new measure and propose it as an alternative for portfolio optimization that incorporates higher moments of asset returns, such as skewness and excess kurtosis.},
  citeulike-article-id = {14400399},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.4.112},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-07-27 09:47:48},
  timestamp            = {2020-02-25 21:06},
}

@Article{Fragkiskos-2014,
  author       = {Fragkiskos, A.},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {What is portfolio diversification?},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2331475},
  abstract     = {There is considerable controversy concerning what exactly portfolio diversification is and under what circumstances is it beneficial to investors, particularly in the wake of the most recent financial crisis in 2008. This paper gathers the various approaches on portfolio diversification throughout history, placing a higher emphasis on recent developments. The goal of this paper is not to provide an exhaustive list of diversification strategies, but rather to highlight the most commonly used ones, provide the motivation behind each approach, and show how they compare with real data.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2331475},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Geczy-2016,
  author               = {Geczy, Christopher},
  date                 = {2016-11},
  journaltitle         = {The Journal of Private Equity},
  title                = {The New Diversification: Open Your Eyes to Alternatives},
  doi                  = {10.3905/jpe.2016.20.1.072},
  issn                 = {1096-5572},
  number               = {1},
  pages                = {72--81},
  volume               = {20},
  abstract             = {During the 2008 financial crisis, many portfolios considered widely diversified failed to fulfill their expected function of protecting against large drawdowns. Historically, correlations among various types of stocks and bonds have usually increased during financial shocks, but the diversification shortcomings of standard portfolio allocations still surprised investors. Six years later, managers have a more sophisticated understanding of portfolio drawdown risk and how to mitigate it through diversification. In this article, the author advocates a focus on the risk exposures within a portfolio and inclusion of risk diversifiers-often sourced through so-called alternatives-to design portfolios more resistant to volatility spikes and major shocks.},
  citeulike-article-id = {14217774},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpe.2016.20.1.072},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-30 19:39:57},
  timestamp            = {2020-02-25 21:06},
}

@Article{Hardle-et-al-2018,
  author               = {Hardle, Wolfgang K. and Lee, David K. and Nasekin, Sergey and Petukhina, Alla},
  date                 = {2018},
  journaltitle         = {Journal of Asset Management},
  title                = {Tail Event Driven ASset allocation: evidence from equity and mutual funds' markets},
  doi                  = {10.1057/s41260-017-0060-9},
  pages                = {1--15},
  abstract             = {The correlation structure across assets and opposite tail movements are essential to the asset allocation problem, since they determine the level of risk in a position. Correlation alone is not informative on the distributional details of the assets. Recently introduced TEDAS-Tail Event Driven ASset allocation approach determines the dependence between assets at different tail measures. TEDAS uses adaptive Lasso-based quantile regression in order to determine an active set of negative coefficients. Based on these active risk factors, an adjustment for intertemporal correlation is made. In this research, authors aim to develop TEDAS, by introducing three TEDAS modifications differing in allocation weights' determination: a Cornish-Fisher Value-at-Risk minimization, Markowitz diversification rule or naive equal weighting. TEDAS strategies significantly outperform other widely used allocation approaches on two asset markets: German equity and Global mutual funds.},
  citeulike-article-id = {14479426},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-017-0060-9},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-017-0060-9},
  groups               = {Diversified_Invest},
  posted-at            = {2017-11-20 20:16:48},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-02-25 21:06},
}

@Article{Heinze-2016,
  author               = {Heinze, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Markowitz {3.0}: Including Diversification Targets in Portfolio Optimization via Diversification Functions},
  doi                  = {10.2139/ssrn.2805368},
  issn                 = {1556-5068},
  abstract             = {Given Markowitz's mean-risk model, maximization of diversification is established as an additional investment target next to return maximization and risk minimization. This widens the opportunity to transfer market views into the model by additional diversification parameters and should therefore lead to an improved mapping of economic reality. The main focus is on the introduction of diversification functions which make diversification quantifiable and which are used as third objective in the optimization. Thus, the resulting efficient frontier extends to a three dimensional surface which includes the original efficient frontier according to Markowitz. Starting with the original Markowitz model through improvements in stochastic modelling in terms of risk measures, copulas, fat tails, etc., leaving the pure return/risk context can be interpreted as a third model generation.},
  citeulike-article-id = {14332615},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2805368},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-06 01:00:31},
  timestamp            = {2020-02-25 21:06},
}

@Article{Hjalmarsson-2011,
  author               = {Hjalmarsson, Erik},
  date                 = {2011-11},
  journaltitle         = {The Journal of Investing},
  title                = {Portfolio Diversification Across Characteristics},
  doi                  = {10.3905/joi.2011.20.4.084},
  issn                 = {1068-0896},
  number               = {4},
  pages                = {84--88},
  volume               = {20},
  abstract             = {This article studies long short portfolio strategies formed on seven different stock characteristics representing various measures of past returns, value, and size. Each individual characteristic results in a profitable portfolio strategy, but these single-characteristic strategies are dominated by a diversified strategy that places equal weight on each of the single-characteristic strategies. The benefits of diversifying across characteristic-based long short strategies are substantial and can be attributed to the mostly low, and sometimes substantially negative, correlation between the returns on the single-characteristic strategies.},
  citeulike-article-id = {13970992},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.4.084},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:25:40},
  timestamp            = {2020-02-25 21:06},
}

@Article{Homescu-2014b,
  author       = {C. Homescu},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {Many risks, one (optimal) portfolio},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2473776},
  abstract     = {This study investigates how to obtain a portfolio which would provide above average returns while remaining robust to most risk exposures. Emphasis is placed on risk management, given our perspective (shared by many other practitioners), that retaining above average portfolio performance in current market environment depends strongly on having an effective risk management process.

We rely on a comprehensive survey of the literature to describe stylized facts of market returns and main categories of asset allocation methodologies, including Modern Portfolio Theory, Black-Litterman model, factor-based and risk-based strategies. Furthermore, we present both criticisms and defenses of strategies, together with potential issues identified by practitioners and corresponding solutions (if they do exist).

We outline recent enhancements to various types of portfolio strategies, and analyze how to incorporate (in the asset allocation framework) constraints, regularization, personal views, stylized features of empirical market data, and forward information given by financial options market data.

More prominence is given to strategies (risk parity, risk factors, factor investing, smart beta, dynamic, etc.) that were shown to deliver better portfolio performance in terms of returns, diversification, risk, etc. We also discuss a wide ranging collection of performance measures proposed in the literature for quantifying portfolio return, risk and diversification, identify which such measures are most popular with practitioners, and which corresponding strategies have best results (as shown in the literature).

Since a major topic of this study is managing risks, we provide details on the types of risk that portfolios may be exposed to, on approaches and strategies to handle such exposures, with highlighting of tail risk management. Portfolio insurance is also discussed.

We also describe practical aspects needed for a successful portfolio management, including robust estimation of covariances, correlations and model parameters, numerical optimization methods, key questions and issues identified by practitioners, Monte Carlo simulation, comprehensive testing framework, stress testing, available software implementations (usually in R), etc.

To summarize, the study analyzes all ingredients that are required, in our opinion, to deliver portfolios with above average performances and resilient to most risks, and concentrates on the strategies which have emerged as frontrunners in the last few years, both in the literature and in the market.},
  groups       = {Mean_Variance, Black_Litterman, Risk_Budgeting, PortfOptim_Robust, Invest_Risk, Factor_Types, Factor_Selection, Factor_Test, Invest_SmartBeta, DAA, Diversified_Invest, OBPI},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2473776},
  owner        = {cristi},
  timestamp    = {2020-02-25 21:06},
}

@Article{Hwang-et-al-2018,
  author               = {Hwang, Inchang and Xu, Simon and In, Francis},
  date                 = {2018-02},
  journaltitle         = {European Journal of Operational Research},
  title                = {Naive versus optimal diversification: Tail risk and performance},
  doi                  = {10.1016/j.ejor.2017.07.066},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {372--388},
  volume               = {265},
  abstract             = {It is well documented in portfolio optimization that naive diversification outperforms optimal mean-variance diversification because the latter is subject to severe estimation error. Our study provides an alternative explanation for the outperformance of naive diversification by examining the tail risk of naive diversification relative to optimal mean-variance diversification. We utilize a rolling-sample approach and compare the out-of-sample performance and tail risk of various optimal strategies to that of the naive diversification strategy. Using portfolios consisting of individual stocks, we show that for portfolios containing relatively small number of stocks, naive diversification outperforms optimal mean-variance diversification and is less exposed to tail risk. However, for relatively large number of stocks in the portfolio, naive diversification maintains its superior performance but increases tail risk and results in more concave portfolio returns. These results imply that the outperformance of naive diversification acts as compensation for the increase in tail risk and concavity.},
  citeulike-article-id = {14500767},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.07.066},
  groups               = {Diversified_Invest, Invest_Risk, Invest_TailRisk, Invest_Diversif},
  posted-at            = {2017-12-11 09:23:28},
  timestamp            = {2020-02-25 21:06},
}

@Article{Ilmanen-Kizer-2012,
  author       = {Ilmanen, Antti and Jared Kizer},
  date         = {2012},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The Death of Diversification Has Been Greatly Exaggerated},
  pages        = {15--27},
  url          = {https://www.aqr.com/Insights/Research/Journal-Article/The-Death-of-Diversification-Has-Been-Greatly-Exaggerated},
  volume       = {38},
  abstract     = {Asset-class correlations generally tend to rise during crises. That certainly was true in the 2007-2009 financial crisis, and since then correlations have generally remained elevated as markets switch between binary risk-on/risk-off environments. However, we believe it would be wrong to interpret these developments as conclusive evidence of the death of diversification.

First, academics (Asness, Israelov and Liew [2011]) have stressed that while diversification often fails in short-term panics - especially one as systemic as the 2007-2009 crisis - it does effectively reduce downside risks over longer horizons. Second, high-quality bonds have fairly consistently provided positive returns during stressful market environments. Third, in this article, we argue and show that factor diversification has been more effective than asset-class diversification in general and, in particular, during crises. The last two arguments challenge the concentration in equity risk found in most institutional portfolios, which is also a central argument in favor of more risk-balanced, so-called risk parity, portfolios.

Traditional asset-class diversification involves allocating nominal dollars to various asset classes and their subsets. Several large institutions have begun to explore an alternative perspective of factor allocation, asking: What are the most important factors driving our portfolio returns? This perspective involves at least two changes. First, focus is shifted from dollar allocations to risk allocations. This change often reveals the dominant role of the most volatile asset classes and the portfolio's dependence on equity market direction. Second, portfolio analysis is extended beyond asset classes to dynamic strategy styles or to underlying risk factors. Fundamental factors such as growth, inflation and liquidity are naturally interesting, but they are inherently hard to measure. Most investors prefer investable factors and therefore use market-based proxies - equities for growth, Treasuries for deflation and commodities for inflation.},
  groups       = {Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Iraya-Wafula-2018,
  author         = {Iraya, Cyrus and Wafula, Fernandes Juma},
  date           = {2018-02-28},
  journaltitle   = {European Scientific Journal},
  title          = {Does portfolio diversification affect performance of balanced mutual funds in kenya?},
  doi            = {10.19044/esj.2018.v14n4p158},
  issn           = {1857-7881},
  number         = {4},
  volume         = {14},
  abstract       = {Literature provides conflicting results on the effect of diversification on performance of mutual funds with some studies showing a positive relationship (Markowitz, 1952; Muriithi, 2005; Kagunga, 2010), others negative (Chang AND Elyasiani, 2008; Fiegenbaum and Thomas, 1998) and still others showing that there is no relationship between the two variables (Loeb, 1950). It is with this background that this study sought to establish the effect of diversification on performance of mutual funds in Kenya. The study took a descriptive research design approach on weekly performance of a sample of 7 balanced mutual funds for the year 2013.The study used secondary data sources available at the Capital Market Authority offices and from each mutual funds. The portfolio return was determined by computing the changes in prices of the balanced fund as traded at the Nairobi Securities Exchange (NSE) while diversification was determined from the level of Unsystematic Risk in the Performance. The study used the Ordinary Least Squares (OLS) multiple linear regression equation. Control variables of the size and age of the fund were introduced in the regression model. The results indicated the existence of a positive relationship between the Unsystematic Risk and Performance of balanced mutual funds with a beta coefficient of 0.069 (t=4.971, p 0.5. This implies that the lower the diversification the higher the performance of mutual funds.},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:06},
}

@Article{Kanuri-et-al-2018,
  author               = {Kanuri, Srinidhi and Malhotra, Davinder and Malm, James},
  date                 = {2018-01-23},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Evaluating the Performance and Diversification Benefits of Emerging-Market Exchange-Traded Funds},
  doi                  = {10.3905/jwm.2018.20.4.085},
  issn                 = {1534-7524},
  number               = {4},
  pages                = {85--90},
  url                  = {https://jwm.pm-research.com/content/20/4/85},
  volume               = {20},
  abstract             = {This study evaluates the performance and diversification benefits for U.S. investors of emerging-market exchange-traded funds (ETFs) since their inception in January 2003 through June 2015 by comparing their absolute and risk-adjusted performance with the iShares Core SandP 500 ETF (IVV). The authors find that the emerging-market ETF portfolio has very low correlations with IVV during the period of the study. Emerging-market ETF portfolios delivered better absolute performance (returns and wealth) but also had much higher risk (standard deviation of returns). However, the risk-adjusted performance (Sharpe and Omega ratios) of IVV was better than that of the emerging-market ETF portfolio. They also look at the effect of adding some emerging-market ETFs to IVV during the period of study. The authors find that adding some emerging-market ETFs to IVV leads to higher absolute returns, better risk-adjusted performance (Sharpe and Omega ratios), higher cumulative returns, and increased wealth for U.S. investors. Results are statistically significant at 1\% in all cases. Therefore, U.S. investors should add some emerging-market ETFs to their domestic allocation based on their risk tolerance for better performance (absolute and risk-adjusted).},
  citeulike-article-id = {14525342},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2018.20.4.085},
  day                  = {23},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-28 16:35:16},
  timestamp            = {2020-02-25 21:10},
}

@Article{KaradedeBouras-Laopodis-2015,
  author               = {Karadede-Bouras, Markella and Laopodis, Nikiforos T.},
  date                 = {2015-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Dynamics among Traditional and Alternative Assets: Implications for Diversification and Risk},
  doi                  = {10.3905/jwm.2015.18.2.013},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {13--34},
  volume               = {18},
  abstract             = {The authors examine the dynamic correlations and implications of various fi nancial asset classes, such as equities, bonds, ETFs, commodities, and real estate, in the United States from 1990 to 2013 and find that the correlations have varied across time.

They detect no evidence of contagion but rather of herding behavior among these assets. The variation was more pronounced during market declines, but differed in extent across economic expansions and contractions. Finally, shocks from one asset class to another were not persistent, meaning that the assets were able to absorb the shocks and quickly return to normalcy.

The implications for portfolio decisions are clear: Even well-diversified portfolios must be updated to reflect changing economic and financial environments, and past asset behavior does not imply similar future behavior.},
  citeulike-article-id = {13968108},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2015.18.2.013},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-05 18:07:15},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kind-Poonia-2014,
  author               = {Kind, Christoph and Poonia, Muddit},
  date                 = {2014-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Diversification Management of a Multi-Asset Portfolio},
  url                  = {https://ssrn.com/abstract=2410153},
  abstract             = {It is a well-known fact in finance that classical mean-variance optimization often leads to highly concentrated portfolios. Giving equal weights to all portfolio assets will instead allow for maximum nominal diversification. More sophisticated ways of nominal diversification are the maximum diversification approach proposed by Choueifaty and Coignard (2008) or the equal weighting of total risk contributions known as risk parity . Instead of looking for nominal diversification, investors may prefer a diversification of the risk factors that drive portfolio returns. In recent papers, risk factors have been modelled by principal components following Partovi and Caputo (2004). Meucci et al. (2013) show that principal components may not be the best way to model risk factors and propose minimum torsion bets instead. The present paper discusses different ways of managing diversification and backtests these strategies in a multi-asset portfolio.},
  citeulike-article-id = {13997420},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2410153},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2410153code1500322.pdf?abstractid=2410153 and mirid=1},
  day                  = {18},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:54:23},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kupiec-2016,
  author               = {Kupiec, Paul},
  date                 = {2016},
  journaltitle         = {Journal of Investment Management},
  title                = {Portfolio Diversification In Concentrated Bond And Loan Portfolios},
  number               = {2},
  url                  = {https://www.joim.com/portfolio-diversification-in-concentrated-bond-and-loan-portfolios/},
  volume               = {14},
  abstract             = {I develop an algorithm to approximate the loss rate distribution for fixed income portfolios with obligor concentrations. The approximation requires no advanced mathematics or statistics, only the summation of large exposures and the evaluation of binomial probabilities. The approximation is model-independent and can be used after removing default dependence using any risk modeling approach. It is especially useful for capital calculations given its inherent accuracy in the upper tail of the cumulative portfolio loss rate distribution. The approximation provides a simple way to calculate the capital benefits of risk mitigation or the capital needed when a marginal credit is added to a concentrated portfolio},
  citeulike-article-id = {14486905},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-01 22:20:00},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lassance-et-al-2018,
  author         = {Lassance, Nathan and DeMiguel, Victor and Vrins, Frederic Daniel},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Optimal portfolio diversification via independent component analysis},
  doi            = {10.2139/ssrn.3285156},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3285156},
  abstract       = {A popular approach for enhancing diversification in portfolio selection is to rely on the factor-risk-parity portfolio, which is often defined as the portfolio whose return variance is spread equally among the principal components (PCs) of asset returns. Although PCs are useful for dimensionality reduction, they are arbitrary because any rotation of the PC basis yields an equally uncorrelated basis. This is problematic because we theoretically demonstrate that any portfolio is the factor-risk-parity portfolio corresponding to a specific uncorrelated basis. To overcome this problem, we rely on the factor-risk-parity portfolio based on the independent components (ICs), which are the rotation of the PCs that are maximally independent, and thus, account for higher-order moments. We propose a shrinkage portfolio that is obtained by combining the minimum-variance portfolio and the IC-risk-parity portfolio. We also show how to exploit the near independence of the ICs to parsimoniously estimate the factor-risk-parity portfolio with respect to Value-at-Risk. Finally, we empirically demonstrate that shrinkage portfolios based on the IC basis outperform those based on the PC basis, as well as the minimum-variance portfolio.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@PhdThesis{Lee-2016a,
  author               = {Lee, Yongjae},
  date                 = {2016},
  institution          = {KAIST},
  title                = {Demystifying Diversification Strategies by using Portfolio Optimization Techniques},
  url                  = {https://www.researchgate.net/publication/309563859_Demystifying_Diversification_Strategies_by_using_Portfolio_Optimization_Techniques},
  abstract             = {In this dissertation, we study a series of diversification strategies in order to improve the understanding of the diversification of investments that was mathematically established by Harry Markowitz. Even though Modern Portfolio Theory considers a trade-off between generating high returns and lowering risks, investment processes inspired by the concept of diversification are generally only addressed with their diversification benefits. Therefore, we analyze the diversification strategies by using portfolio optimization techniques that primarily originated from the mean-variance framework, which considers returns as well as risks with equal importance, in order to more fully understand the quantifiable consequences of various diversification strategies. First, we investigate passive investing and performance benchmarking through analyzing the two most popular equity benchmark portfolios: the cap-weighted portfolio, and the equally-weighted portfolio. As conventional portfolio performance evaluations occur relative to benchmarks, the performance evaluation of the benchmark itself has never been a trivial issue. Thus, an alternative methodology for portfolio performance evaluation that can be conducted without peer information is proposed, and we find little or no evidence of either benchmark portfolio performing better than the average portfolio. In terms of performance benchmarking, however, equally-weighted portfolios exhibit more desirable properties than cap-weighted portfolios. Second, we examine the quantitative properties of asset allocation and asset classification. We derive and compare the closed form expressions for the portfolio performances of asset allocation and direct security selection, and we find that the majority of investors can benefit from employing asset allocation. Furthermore, our analysis indicates that the design of asset classes is a critical factor in determining the portfolio performance of employing asset allocation. Hence, we further test the two most widely used within-stock asset classification schemes, i.e. style and industry classifications, and find that the asset designs should not focus on diversification benefits only.

Third, we discuss the viability of robo-advising, which was recently developed during the ongoing expansion of financial technology (FinTech). Robo-advising attempts to lower the entry barrier to financial advising through utilizing automated but personalized algorithms, in order to attract investors with smaller accounts, who are ineligible to receive traditional financial advising services. We investigate the relationship between portfolio size and risk in order to examine the viability of roboadvisers in providing diversification benefits with limited portfolio sizes. The results indicate that a substantial investment is not necessary to gain diversification benefits.},
  citeulike-article-id = {14525362},
  groups               = {Diversified_Invest, AssetAlloc_vs_SecSelect, FinTech_WealthTech, Invest_Diversif},
  posted-at            = {2018-01-28 17:17:54},
  timestamp            = {2020-02-25 21:10},
}

@Article{Linder-2018,
  author         = {Linder, John},
  date           = {2018-05-31},
  journaltitle   = {The Journal of Investing},
  title          = {Rebalancing-Diversification Return: The Opportunity Cost of Illiquid Investments},
  doi            = {10.3905/joi.2018.27.2.057},
  issn           = {1068-0896},
  number         = {2},
  pages          = {57--65},
  volume         = {27},
  abstract       = {Institutional investors expect a return premium for illiquidity when an investment is private and cannot be sold easily in an established liquid market. For investors that have a very long investment horizon some might argue a permanent (or perpetual) investment portfolio in private markets with any such level of expected return premium might seem to be dominant to a markets only construct. However, how much should this premium be? The author hypothesizes that the illiquidity premium observed is directly related to the risk-equivalent liquid markets diversification-rebalancing returns forgone in pursuing illiquid investments. The author posits the excess return to illiquidity available to the long-term, non-liquidity constrained investor, is an investor-specific opportunity cost of illiquidity, and by logical extension, he proposes, an (efficient) market cost to illiquidity hypothesis. Finally, he examines the private equity industry benchmarking convention for performance evaluation-cap stocks + 300 bps this paradigm.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Rebalancing, Invest_Liquidity, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@Article{Lohre-et-al-2012a,
  author               = {Lohre, Harald and Neugebauer, Ulrich and Zimmer, Carsten},
  date                 = {2012},
  journaltitle         = {The Journal of Investing},
  title                = {Diversified Risk Parity Strategies for Equity Portfolio Selection},
  doi                  = {10.3905/joi.2012.21.3.111},
  number               = {3},
  volume               = {21},
  abstract             = {This article investigates a new way of equity portfolio selection that provides maximum diversification along the uncorrelated risk sources inherent in the SandP 500.This diversified risk parity strategy is distinct from prevailing risk-based portfolio construction paradigms. Especially, the strategy is characterized by a concentrated allocation that actively adjusts to changes in the underlying risk structure. In addition, x-raying the risk and diversification characteristics of traditional risk-based strategies like 1/N, minimum-variance, risk parity, or the most-diversified portfolio, the authors find the diversified risk parity strategy to be superior. Although most of these alternatives crucially pick up risk-based pricing anomalies like the low-volatility anomaly, the diversified risk parity strategy more effectively exploits systematic factor tilts.},
  citeulike-article-id = {14322318},
  citeulike-linkout-0  = {http://www.iijournals.com/doi/abs/10.3905/joi.2012.21.3.111},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-29 09:07:28},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lozano-2013,
  author       = {Lozano, Martin},
  date         = {2013},
  journaltitle = {SSRN Electronic Journal},
  title        = {Diversification: A Bittersweet Story},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2373738},
  abstract     = {We all have heard or even follow the proverb: Don't put all your eggs in one basket . Clever advice indeed, though it is mute about how we should distribute the eggs. Ideally, the best allocation advice is the one which minimize the number of broken eggs.

In Finance we do something similar, although we deal with wealth and assets instead of eggs and baskets. In particular, portfolio theory suggests diversification strategies aimed to reduce the overall portfolio risk by combining several assets like real state, stocks, bonds, commodities, and foreign currency, instead of concentrating in only one. In sum, we try to generate informed financial decisions within an uncertain environment.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2373738},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Martellini-Milhau-2017,
  author         = {Martellini, Lionel and Milhau, Vincent},
  date           = {2017-12-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Proverbial Baskets Are Uncorrelated Risk Factors! A Factor-Based Framework for Measuring and Managing Diversification in Multi-Asset Investment Solutions},
  doi            = {10.3905/jpm.2018.44.2.008},
  url            = {https://jpm.pm-research.com/content/44/2/8},
  abstract       = {Multi-asset investment solutions have become increasingly popular among sophisticated institutional investors focusing on efficient harvesting of risk premia across and within asset classes. One key challenge in the construction of diversified multi-asset portfolio strategies is that even a seemingly well-balanced allocation to many asset classes can eventually translate into a portfolio with a very concentrated set of underlying risk exposures. The authors suggest using a factor-based framework to more effectively measure and manage diversification in multi-asset portfolios.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Risk, Effective_Dim_Diversif, MultiFactor_Invest, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 21:10},
}

@Article{Meucci-et-al-2014,
  author       = {Meucci, A. and Santangelo, A. and Deguest, R.},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {Measuring portfolio diversification based on optimized uncorrelated factors},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2276632},
  abstract     = {We measure diversification in terms of the Effective Number of Minimum-Torsion Bets , namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. This way we introduce a novel notion of absolute risk contributions , which generalizes the marginal contributions to risk in traditional risk parity. We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk.

We present a case study in the SandP 500.},
  groups       = {Diversified_Invest, Effective_Dim_Diversif, PortfOptim_Factor},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2276632},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Meucci-et-al-2015a,
  author         = {Meucci, Attilio and Santangelo, Alberto and Deguest, Romain},
  date           = {2015},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Risk Budgeting and Diversification Based on Optimized Uncorrelated Factors},
  doi            = {10.2139/ssrn.2276632},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=2276632},
  abstract       = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies. The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification. We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the SandP 500, and a factor-based investment in the five Fama-French factors.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Effective_Dim_Diversif, PortfOptim_Factor},
  timestamp      = {2020-02-25 21:10},
}

@Article{Miebs-2012,
  author       = {Miebs, Felix},
  date         = {2012},
  journaltitle = {SSRN Electronic Journal},
  title        = {Diversifying Diversification Strategies: Model Averaging in Portfolio Optimization},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2011969},
  abstract     = {The literature on portfolio optimization in the presence of parameter uncertainty has suggested several approaches to mitigate the impact of estimation error on portfolio performance. However, empirical evidence finds no single approach that can achieve a consistently higher risk-adjusted performance than 1/N. In this paper, I propose three averaging rules that synthesize the established approaches in order to mitigate the impact of estimation error on portfolio performance.

The evaluation of the proposed averaging rules on empirical and simulated datasets shows that each rule achieves a consistently higher risk-adjusted performance than 1/N, while all individual portfolio strategies considered in the averaging exercise do not. I find that the observed performance gains are economically and statistically significant. The performance gains are attributable to persistent diversification effects between the portfolio strategies under consideration, as well as to empirical characteristics in portfolio returns that are exploited by one of the averaging rules.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2011969},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Murtazashvili-Vozlyublennaia-2013,
  author               = {Murtazashvili, Irina and Vozlyublennaia, Nadia},
  date                 = {2013-06},
  journaltitle         = {Journal of Financial Research},
  title                = {Diversification strategies: do limited data constrain investors?},
  doi                  = {10.1111/j.1475-6803.2013.12008.x},
  issn                 = {0270-2592},
  number               = {2},
  pages                = {215--232},
  volume               = {36},
  abstract             = {We demonstrate that the mean-variance optimal portfolio does not outperform (out of sample) the naive 1/N diversification strategy even if securities are grouped into indexes or broad asset classes. This finding is due to insufficient data on past returns, which limit investors' ability to accurately estimate the means and covariance structure of securities. The resulting high estimation errors eliminate the benefits of using the means and covariance matrix as compared to the naive strategy in portfolio optimization. Using value-weighted indexes, characteristic-sorted portfolios, or portfolios defined by principal components as underlying assets in mean-variance optimization does not help. At the same time, increasing data frequency or adding data on past earnings may in some cases make mean-variance optimization useful.},
  citeulike-article-id = {14514968},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1475-6803.2013.12008.x},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-10 23:39:21},
  timestamp            = {2020-02-25 21:10},
}

@Article{Nystrup-et-al-2018a,
  author               = {Nystrup, Peter and Hansen, Bo W. and Larsen, Henrik O. and Madsen, Henrik and Lindstrom, Erik},
  date                 = {2018-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Dynamic Allocation or Diversification: A Regime-Based Approach to Multiple Assets},
  doi                  = {10.3905/jpm.2018.44.2.062},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {62--73},
  volume               = {44},
  abstract             = {This article investigates whether regime-based asset allocation can effectively respond to changes in financial regimes at the portfolio level in an effort to provide better long-term results when compared to a static 60/40 benchmark. The potential benefit from taking large positions in a few assets at a time comes at the cost of reduced diversification. The authors analyze this trade-off in a multi-asset universe with great potential for static diversification. The regime-based approach is centered around a regime-switching model with time-varying parameters that can match financial markets' behavior and a new, more intuitive way of inferring the hidden market regimes. The empirical results show that regime-based asset allocation is profitable, even when compared to a diversified benchmark portfolio. The results are robust because they are based on available market data with no assumptions about forecasting skills.},
  citeulike-article-id = {14510367},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.062},
  day                  = {22},
  groups               = {BenchmarkInvest, Diversified_Invest, Regime_Invest, Invest_Dynamic, Invest_Regime, FrcstQWIM_MedLngTerm, Invest_Diversif},
  posted-at            = {2017-12-30 13:06:32},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kashyap-2017a,
  author               = {Kashyap, Ravi},
  date                 = {2017-12-01},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Combining Dimension Reduction, Distance Measures and Covariance},
  eprint               = {1603.09060},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.09060},
  abstract             = {We develop a novel methodology based on the marriage between the Bhattacharyya distance, a measure of similarity across distributions of random variables, and the Johnson Lindenstrauss Lemma, a technique for dimension reduction. The resulting technique is a simple yet powerful tool that allows comparisons between data-sets representing any two distributions. The degree to which different entities, (markets, groups of securities, etc.), have different measures of their corresponding distributions tells us the extent to which they are different, aiding participants looking for diversification or looking for more of the same thing. We demonstrate a relationship between covariance and distance measures based on a generic extension of Stein's Lemma. We consider an asset pricing application and then briefly discuss how this methodology lends itself to numerous marketstructure studies and even applications outside the realm of finance / social sciences by illustrating a biological application.},
  citeulike-article-id = {14510843},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.09060},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.09060},
  day                  = {1},
  groups               = {Diversification_Measure, Dimens_Reduc},
  posted-at            = {2018-01-02 01:13:37},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lee-2011,
  author       = {Lee, W.},
  date         = {2011},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Risk-Based Asset Allocation: A New Answer to an Old Question?},
  doi          = {10.3905/jpm.2011.37.4.011},
  number       = {4},
  pages        = {11--28},
  url          = {https://jpm.pm-research.com/content/37/4/11},
  volume       = {37},
  abstract     = {In recent years, we have witnessed an alarmingly large and growing amount of literature on portfolio construction approaches focused on risks and diversification rather than on estimating expected returns. Numerous simulations applied to different universes have been documented in support of these approaches based on their apparent outperformance versus passive market capitalization-weighted or static fixed-weight portfolios. Many studies attribute the better performance of these risk-based asset allocation approaches to superior diversification.

Given the absence of clearly defined investment objective functions behind these approaches as well as the metrics used by these studies to evaluate ex post performance, Lee puts these approaches into the same context of mean-variance efficiency in an attempt to understand their theoretical underpinnings. In doing so, he hopes to shed some light on what these approaches attempt to achieve and on the characteristics of the investment universe, if indeed these approaches are meant to approximate mean-variance efficiency. Rather than adding to the already large collection of simulation results, Lee uses some simple examples to compare and contrast the portfolio and risk characteristics of these approaches. He also reiterates that any portfolio which deviates from the market capitalization-weighted portfolio is an active portfolio.

He concludes that there is no theory to predict, ex ante, that any of these risk-based approaches should outperform.},
  groups       = {Diversification_Measure, ExAnte_ExPost},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Mignacca-2018,
  author         = {Mignacca, Domenico},
  date           = {2018-05-03},
  journaltitle   = {SSRN Electronic Journal},
  title          = {A New Measure of Diversification: The M-DiX},
  url            = {https://ssrn.com/abstract=3172722},
  abstract       = {Diversification is a core concept in Asset Management. Yet diversification can mean different things to different people and there no consensus on how it is measured nor is there a broadly accepted metric for reporting of diversification. Sometimes, there is confusion in understanding diversification and how it differs from hedging. We may say that diversification and hedging both have the same objective i.e. reducing the risk of a portfolio, but diversification is obtained using correlated (in absolute value) securities, while hedging is achieved with correlated securities. In this paper we propose a new index to measure the diversification of a portfolio. Specifically, we outline a two-dimensional risk decomposition that we use to calculate our diversification index: the M-DiX.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@Article{Oyenubi-2016,
  author               = {Oyenubi, Adeola},
  date                 = {2016},
  journaltitle         = {Computational Economics},
  title                = {Diversification Measures and the Optimal Number of Stocks in a Portfolio: An Information Theoretic Explanation},
  doi                  = {10.1007/s10614-016-9600-5},
  pages                = {1--29},
  abstract             = {This paper provides a plausible explanation for why the optimum number of stocks in a portfolio is elusive, and suggests a way to determine this optimal number. Diversification has a lot to do with the number of stocks in a portfolio. Adding stocks to a portfolio increases the level of diversification, and consequently leads to risk reduction up to a certain number of stocks, beyond which additional stocks are of no benefit, in terms of risk reduction. To explain this phenomenon, this paper investigates the relationship between portfolio diversification and concentration using a genetic algorithm. To quantify diversification, we use the portfolio Diversification Index (PDI). In the case of concentration, we introduce a new quantification method. Concentration is quantified as complexity of the correlation matrix. The proposed method quantifies the level of dependency (or redundancy) between stocks in a portfolio. By contrasting the two methods it is shown that the optimal number of stocks that optimizes diversification depends on both number of stocks and average correlation. Our result shows that, for a given universe, there is a set of Pareto optimal portfolios containing a different number of stocks that simultaneously maximizes diversification and minimizes concentration. The choice portfolio among the Pareto set will depend on the preference of the investor. Our result also suggests that an ideal condition for the optimal number of stocks is when variance reduction benefit of diversification is off-set by the variance contribution of complexity.},
  citeulike-article-id = {14398652},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-016-9600-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-016-9600-5},
  groups               = {Diversification_Measure, Invest_Diversif},
  posted-at            = {2017-07-23 16:05:16},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 21:10},
}

@Article{Raffinot-2016,
  author               = {Raffinot, Thomas},
  date                 = {2016-09},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Hierarchical Clustering Based Asset Allocation},
  url                  = {https://ssrn.com/abstract=2840729},
  abstract             = {Building upon the fundamental notion of hierarchy, Lopez de Prado (2016a) introduces a new portfolio diversification technique called "Hierarchical Risk Parity", which uses graph theory and machine learning techniques. Exploiting the same basic idea, a hierarchical clustering based asset allocation method is proposed. Classical and more modern hierarchical clustering methods are tested, such as Simple Linkage or Directed Bubble Hierarchical Tree for example. A simple and efficient capital allocation within and across clusters of assets at multiple hierarchical levels is computed. The out-of-sample performances of hierarchical clustering based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets. To avoid data snooping, the comparison of profit measures is assessed using the bootstrap based model confidence set procedure (Hansen et al. (2011)). The empirical results indicate that hierarchical clustering based portfolios are robust, truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14146762},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2840729},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2840729code2270025.pdf?abstractid=2840729 and mirid=1},
  day                  = {20},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:30:25},
  timestamp            = {2020-02-25 21:14},
}

@Article{Ren-et-al-2016,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2016-08},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Dynamic portfolio strategy using clustering approach},
  eprint               = {1608.03058},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1608.03058},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. This paper proposes a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: selecting the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, i.e., degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion, then using the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index or the sum of the amplitudes of the trading days with rising index to the total number of trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that the peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all the possible optimal portfolio strategy based on different parameters to select portfolios and different criteria to identify market conditions, 65dollar; of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market and the proportion is 70dollar; for the Shenzhen A-Share market.},
  citeulike-article-id = {14148628},
  citeulike-linkout-0  = {http://arxiv.org/abs/1608.03058},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1608.03058},
  day                  = {10},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, Invest_Dynamic, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:56:42},
  timestamp            = {2020-02-25 21:14},
}

@InCollection{Wang-et-al-2017,
  author               = {Wang, Hao and Pappada, Roberta and Durante, Fabrizio and Foscolo, Enrico},
  booktitle            = {Soft Methods for Data Science},
  date                 = {2017},
  title                = {A Portfolio Diversification Strategy via Tail Dependence Clustering},
  doi                  = {10.1007/978-3-319-42972-4\_63},
  editor               = {Ferraro, Maria B. and Giordani, Paolo and Vantaggi, Barbara and Gagolewski, Marek and Angeles Gil, Mara and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  pages                = {511--518},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {456},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures, a cluster analysis is carried out for discerning between assets with the same performance in risky scenarios. Then, the portfolio composition is determined by fixing a number of assets and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected assets in trouble periods may improve the performance of risk-averse investors.},
  citeulike-article-id = {14150080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-42972-463},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-42972-463},
  groups               = {Networks and investment management, Clustering and network analysis, Diversification_Measure, Diversified_Invest, Network_Invest, Invest_Network, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:54},
  timestamp            = {2020-02-25 21:14},
}

@Article{Dose-Cincotti-2005,
  author               = {Dose, Christian and Cincotti, Silvano},
  date                 = {2005-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Clustering of financial time series with application to index and enhanced index tracking portfolio},
  doi                  = {10.1016/j.physa.2005.02.078},
  issn                 = {0378-4371},
  number               = {1},
  pages                = {145--151},
  volume               = {355},
  abstract             = {A stochastic-optimization technique based on time series cluster analysis is described for index tracking and enhanced index tracking problems. Our methodology solves the problem in two steps, i.e., by first selecting a subset of stocks and then setting the weight of each stock as a result of an optimization process (asset allocation). Present formulation takes into account constraints on the number of stocks and on the fraction of capital invested in each of them, whilst not including transaction costs. Computational results based on clustering selection are compared to those of random techniques and show the importance of clustering in noise reduction and robust forecasting applications, in particular for enhanced index tracking.},
  citeulike-article-id = {2251197},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2005.02.078},
  citeulike-linkout-1  = {http://www.sciencedirect.com/science/article/B6TVG-4G33NP0-4/2/33fd4c2a5caa0ce508e18151530c9250},
  day                  = {1},
  groups               = {Networks and investment management, Network_Invest, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-26 18:22:57},
  timestamp            = {2020-02-25 21:14},
}

@MastersThesis{Fucik-2017,
  author               = {Fucik, Vojtech},
  date                 = {2017},
  institution          = {Charles University},
  title                = {Portfolio Construction Using Hierarchical Clustering},
  url                  = {https://dspace.cuni.cz/handle/20.500.11956/91113?locale-attribute=en},
  abstract             = {The main objective of this thesis is to summarize and mainly interconnect the existing methodology on correlation matrix filtering, graph algorithms utilized in the minimum spanning trees, hierarchical clustering and principal components analysis in order to create quantitative investment strategies. Instead of traditional usage of stocks returns series, factor models residuals are utilized. Residuals are then an ultimate input for all the algorithms to arrive at probability of centrality (PoC) - an impure probability where values near 1 signalize high probability of a stock being central in the network. Several investment strategies are created based on PoC and tested on data from major US stock market indices. It cannot be imperatively argued that peripheralbased strategies are always better than central-based strategies. Both central and peripheral-based strategies share high upside profit potential at the cost of high volatility whereas traditional Markowitz's optimization process yields stable profits with moderate upside potential.},
  citeulike-article-id = {14461303},
  groups               = {Network_Invest, PortfOptim_Network, Invest_Network, Vol_Cluster},
  posted-at            = {2017-10-19 20:42:17},
  timestamp            = {2020-02-25 21:14},
}

@Article{Page-Panariello-2018,
  author         = {Page, Sebastien and Panariello, Robert A.},
  date           = {2018-08},
  journaltitle   = {Financial Analysts Journal},
  title          = {When Diversification Fails},
  doi            = {10.2469/faj.v74.n3.3},
  issn           = {0015-{198X}},
  number         = {3},
  pages          = {19--32},
  volume         = {74},
  abstract       = {One of the most vexing problems in investment management is that diversification seems to disappear when investors need it the most. We surmise that many investors still do not fully appreciate the impact of extreme correlations on portfolio efficiency particular, on exposure to loss. We take an in-depth look at what drives the stock-to-credit, stock-to-hedge fund, stock-to-private asset, stock-to-risk factors, and stock-to-bond correlations during tail events. We introduce a data-augmentation technique to improve the robustness of tail correlation estimates. Finally, we discuss implications for multi-asset investing.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Pappas-et-al-2014,
  author       = {Pappas, Scott N. and Bianchi, Robert J. and Drew, Michael E. and Gupta, Rakesh},
  date         = {2012},
  journaltitle = {SSRN Electronic Journal},
  title        = {Risk-Factor Diversification and Portfolio Selection},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2136827},
  abstract     = {Traditionally, investment portfolios have been constructed with a focus on what asset classes to invest in and how much to invest in each. Recent research, however, has shown that focusing on risk-factor allocations, rather than asset class allocations, can result in better risk-adjusted portfolio performance. The existing literature has focused on simple allocation strategies such as equal-weighted and equal-risk-weighted portfolios.

In addition to these simple allocation techniques, this paper compares the performance using mean-variance analysis, and presents evidence that the outperformance of risk-factor diversification may not be as conclusive as has been previously presented in the literature.

While confirming some of the prior findings on risk-factor diversification, the research shows that previous findings may be subject to strong caveats. Specifically, the evidence suggests that the selection of risk-factors, portfolio selection techniques and time-period have a large impact on performance outcomes.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2136827},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:15},
}

@MastersThesis{Parmentier-2018,
  author      = {Loic Parmentier},
  date        = {2018},
  institution = {Louvain School of Management},
  title       = {Measures of Portfolio Diversification},
  url         = {https://dial.uclouvain.be/memoire/ucl/en/object/thesis%3A14352/datastream/PDF_01/view},
  abstract    = {Diversification is one the main and most important concept in the financial world. It is often said that diversification is the only free lunch in finance. From a qualitative point of view, the concept of diversification is quite clear: a portfolio is well-diversified if shocks in the individual components do not heavily impact on the overall portfolio. Relatively simple to understand then but profoundly difficult to define. Indeed, there is no broadly accepted precise and quantitative definition of diversification.

Over the years, many different measures of diversification have been developed in the literature, each with its pros and cons. In the framework of this thesis, we have chosen to analyze six of them. Because we wanted to confront the weights concentration criterion with the risk minimization criterion, we decided to select measures that are based on the entropy of the weights and others that are based on the sources of risk. Those six different measures are the Shannon's Entropy, the Diversification Delta, the Diversification Ratio, the MarginalRisk Contributions, the Portfolio Diversification Index and the Effective Number of Bets.},
  groups      = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp   = {2020-02-25 21:15},
}

@Article{Platanakis-et-al-2017b,
  author               = {Platanakis, Emmanouil and Sakkas, Athanasios and Sutcliffe, Charles},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Harmful Diversification: Evidence from Alternative Investments},
  url                  = {https://ssrn.com/abstract=2911212},
  abstract             = {Alternative assets have become as important as equities and fixed income in the portfolios of major investors, and so their diversification properties are also important. However, adding five alternative assets (real estate, commodities, hedge funds, emerging markets and private equity) to equity and bond portfolios is shown to be harmful for US investors. We use 19 portfolio models, in conjunction with dummy variable regression, to demonstrate this harm over the 1997-2015 period. This finding is robust to different estimation periods, risk aversion levels, and the use of two regimes. Harmful diversification into alternatives is not primarily due to transactions costs or non-normality, but to estimation risk. This is larger for alternative assets, particularly during the credit crisis which accounts for the harmful diversification of real estate, private equity and emerging markets. Diversification into commodities, and to a lesser extent hedge funds, remains harmful even when the credit crisis is excluded.},
  citeulike-article-id = {14510389},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2911212},
  groups               = {Private_Equity, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-30 14:25:45},
  timestamp            = {2020-02-25 21:15},
}

@Article{Pola-2016,
  author               = {Pola, Gianni},
  date                 = {2016-04},
  journaltitle         = {Journal of Asset Management},
  title                = {On entropy and portfolio diversification},
  doi                  = {10.1057/jam.2016.10},
  issn                 = {1470-8272},
  abstract             = {Entropy, a term used in Physics to quantify the degree of randomness in a complex system, is shown to be relevant for portfolio diversification. The link between entropy and diversification lies in the notion of uncertainty. We introduce the concept of available diversification in an investment universe and of diversification curves. We build a framework for assembling a fully diversified risk parity-like portfolio with a fundamental-based high-conviction strategy, through a constrained entropy-maximisation process by which a portion of potential portfolio return is swapped for extra diversification. The main results of this study are: mean-variance optimised portfolios are highly concentrated and scarcely related to the asset return assumptions; few basis points of expected returns can be converted into a huge amount of extra diversification that making the portfolio allocation more robust to parameter uncertainty; on a more conceptual ground, we investigate the relationship between portfolio risk and diversification concluding that they should be managed distinctly. The empirical analysis presented in this work shows that entropy is a useful means to alleviate the lack of diversification of portfolios on the efficient frontier.},
  citeulike-article-id = {14030186},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2016.10},
  day                  = {07},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-05-08 22:55:09},
  timestamp            = {2020-02-25 21:15},
}

@Article{Polbennikov-et-al-2010,
  author               = {Polbennikov, Simon and Desclee, Albert and Hyman, Jay},
  date                 = {2010-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Horizon Diversification: Reducing Risk in a Portfolio of Active Strategies},
  doi                  = {10.3905/jpm.2010.36.2.026},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {26--38},
  volume               = {36},
  abstract             = {A primary mechanism for controlling portfolio risk is diversification. Diversification is typically addressed by distributing assets among investment sectors and issuers, preferably with low correlations among their returns, a process that can be called asset diversification. The risk reduction from this type of diversification can be less than expected in the midst of a crisis as correlations increase across market segments.

The authors of this article consider a new approach to managing the active risk profile of a portfolio, an approach that uses active strategies rather than asset allocations as its basic building blocks. The authors show that in this framework, risk reduction is achieved by a combination of two distinct mechanisms asset diversification and signal diversification. Combining alpha strategies based on independent signals can help reduce portfolio risk, even when the returns of the underlying assets are correlated.

One way to achieve signal diversification is by combining strategies with various investment horizons or trading frequencies a technique the authors call horizon diversification. Horizon diversification is an intuitive and robust way to decrease risk in a portfolio of active strategies.},
  citeulike-article-id = {13971833},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2010.36.2.026},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 04:23:53},
  timestamp            = {2020-02-25 21:15},
}

@Article{Qian-2011,
  author               = {Qian, Edward},
  date                 = {2011-02},
  journaltitle         = {The Journal of Investing},
  title                = {Risk Parity and Diversification},
  doi                  = {10.3905/joi.2011.20.1.119},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {119--127},
  volume               = {20},
  abstract             = {Traditional 60/40 asset allocation portfolios are not truly diversified because they have an unbalanced risk allocation to high-risk assets. As a result, their expected risk-adjusted returns are low. Risk parity is a new way to construct asset allocation portfolios based on the principle of risk diversification, achieving both higher risk-adjusted returns and higher total returns than traditional asset allocation approaches. The diversification benefits of risk parity portfolios also include balanced correlations to underlying asset classes and stronger downside protection against severe losses. Risk parity portfolios can also incorporate active views on risk-adjusted returns of different asset classes. All of these features make risk parity an attractive alternative to traditional asset allocation approaches.},
  citeulike-article-id = {13970982},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.1.119},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:18:12},
  timestamp            = {2020-02-25 21:15},
}

@InCollection{Richard-Roncalli-2015,
  author               = {Richard, Jean-Charles and Roncalli, Thierry},
  booktitle            = {Risk-Based and Factor Investing},
  date                 = {2015},
  title                = {Smart Beta: Managing Diversification of Minimum Variance Portfolios},
  doi                  = {10.1016/b978-1-78548-008-9.50002-2},
  isbn                 = {9781785480089},
  pages                = {31--63},
  publisher            = {Elsevier},
  abstract             = {In this chapter, we consider a new framework for understanding risk-based portfolios (global minimum variance (GMV), equally weighted (EW), equal risk contribution (ERC) and most diversified portfolio (MDP)). This framework is similar to the constrained minimum variance model of Jurczenko et al., but with another definition of the diversification constraint. The corresponding optimization problem can then be solved using the cyclical coordinate descent (CCD) algorithm. This allows us to extend the results of Cazalet et al. and to better understand the trade-off relationships between volatility reduction, tracking error and risk diversification. In particular, we show that the smart beta portfolios differ because they implicitly target different levels of volatility reduction.

We also develop new smart beta strategies by managing the level of volatility reduction and show that they present appealing properties compared to the traditional risk-based portfolios.},
  citeulike-article-id = {13978525},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-008-9.50002-2},
  groups               = {Invest_SmartBeta, Diversified_Invest},
  owner                = {cristi},
  posted-at            = {2016-03-12 19:04:23},
  timestamp            = {2020-02-25 21:15},
}

@TechReport{Romahi-Santiago-2012,
  author      = {Romahi, Y. and Santiago, K.},
  date        = {2012},
  institution = {JP Morgan Asset Management},
  title       = {Diversification - Is it still the only Free Lunch? Alternative building blocks for risk parity portfolios},
  url         = {https://am.jpmorgan.com/blobcontent/800/973/1383169203651_11_566.pdf},
  abstract    = {Risk parity has recently garnered significant attention, particularly owing to its strong performance to more traditional approaches of asset allocation in the last decade. This paper seeks to shed some light on this framework and outline the main advantages, while highlighting some of the concerns currently at the forefront of the minds of risk parity investors - namely leveraged positions in fixed income assets at this point in the interest rate cycle as well as the increasing correlation among asset classes.

The premise of risk parity as an approach to strategic asset allocation is based on maximal diversification of beta (or risk premia) as it emphasises the balanced contribution of various risk exposures to overall portfolio risk. One should essentially remain agnostic to return forecasts on the basis that volatility is a much more stable estimate than return.

Much has been made recently of the increasing correlation among asset classes and the increasing difficulty of achieving diversification - particularly at times of crisis arising from systemic risk. A number of recent studies have examined the benefits of factor diversification over asset class diversification.

The difference is subtle because when one refers to asset classes one is also referring to compensated risk premia. These themselves are therefore factors. One can think of equities as a growth factor, Treasuries as a deflation factor and commodities as an inflation factor. However, risk premia go much further than these traditional factors, as argued in a previous J.P. Morgan Asset Management white paper on alternative beta [15].

Indeed, when one focuses on the risk premia, there are a much broader and more orthogonal set of factors of which one can take advantage. In addition to those mentioned, for example, we can also include the equity value premium, the size premium, the forward rate bias and the merger arbitrage premium among others as further risk premia.

The literature is clear that factor diversification is generally more appealing to asset class diversification. Ilmanen and Kizer [8] go further and point out that factor diversification has been more effective, particularly during periods of crisis.

In this paper, extending risk parity in this direction can be seen to address the core concerns around traditional risk parity and can offer a very attractive approach to strategic asset allocation.

In order to demonstrate this, data is included from several periods going back to 1927 and shows that 'factor premium' risk parity consistently outperforms and is stronger to 'asset class' risk parity.},
  groups      = {Diversified_Invest, Invest_Diversif},
  owner       = {Anne},
  timestamp   = {2020-02-25 21:15},
}

@Article{Sharma-Vipul-2018,
  author         = {Sharma, Prateek and Vipul, A},
  date           = {2018},
  journaltitle   = {Managerial and Decision Economics},
  title          = {Improving portfolio diversification: Identifying the right baskets for putting your eggs},
  doi            = {10.1002/mde.2939},
  abstract       = {We measure the economic value of diversification for international multiasset investment strategies. This study implements five existing diversification measures and proposes a novel measure of diversification, the unsystematic risk ratio (URR). Only the URR and the effective number of bets measures predict the future risk-adjusted performance. These relations are robust to the choice of investment horizon and degree of relative risk aversion. The diversification benefits are larger for the frontier and emerging markets than for the developed markets, for multiasset strategies than for single asset class strategies, and for the pre-crisis and post-crisis periods than for the financial crisis period.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Shi-2015,
  author               = {Shi, Xiang},
  date                 = {2015-08},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Marginal Contribution to Risk and Generalized Effective Number of Bets},
  url                  = {https://ssrn.com/abstract=2642408},
  abstract             = {This paper extends Meucci's Effective Number of Bets to general risk measures with heavy-tailed distributions. By diagonalizing the Hessian matrix of a risk measure we are able to extract locally independent marginal contributions to the risk. The Minimal Torsion approach can still be applied to get the local coordinators of the marginal contributions.

We also calculated second derivatives of CVaR. Furthermore, the Hessian of CVaR can be computed efficiently when the underlying distribution belongs to a class of normal mixture distributions.},
  citeulike-article-id = {13926624},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2642408},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2642408code2345648.pdf?abstractid=2642408 and mirid=1},
  day                  = {18},
  groups               = {Diversified_Invest},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2642408},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 04:51:27},
  timestamp            = {2020-02-25 21:15},
}

@Article{Staines-et-al-2016,
  author               = {Staines, Joe and Li, Wei V. and Romahi, Yazann},
  date                 = {2016-08},
  journaltitle         = {The Journal of Index Investing},
  title                = {Dimensions of Diversification},
  doi                  = {10.3905/jii.2016.7.2.119},
  issn                 = {2154-7238},
  number               = {2},
  pages                = {119--127},
  volume               = {7},
  abstract             = {Within the investment industry, diversification now refers to not only the division of capital among a large number of securities but also the avoidance of risk concentration in any of a number of dimensions. Market-capitalization-weighted indexes often fail this requirement. The authors thus argue that although capitalization weighting makes a suitable benchmark, smart beta can provide a way to build indexes more suitable for investment. The authors present a methodology to measure and hence maximize diversification simultaneously across multiple dimensions. They show the practical value of this measure by using it to backtest equity portfolios. This provides an example of how the properties of assets, rather than historical returns, can be used to systematically construct well-diversified portfolios.},
  citeulike-article-id = {14150143},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jii.2016.7.2.119},
  groups               = {Diversified_Invest, Effective_Dim_Diversif, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-02 02:36:41},
  timestamp            = {2020-02-25 21:15},
}

@Article{Stein-2015a,
  author               = {Stein, Michael},
  date                 = {2015-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Limits to Diversification: Tail Risks in Real Estate Portfolios},
  url                  = {https://ssrn.com/abstract=2611905},
  abstract             = {This study addresses real estate's riskiness from a distributional viewpoint. Several studies have found real estate returns to be best modeled with stable paretian distributions. Using NCREIF individual property returns this is confirmed, but the first application of stable distributions to real estate portfolio returns provides evidence that diversification effects ultimately reduce the tailedness and surprisingly drive the tail parameter towards normality. The study further contributes to the literature by showing the importance of a complete view, beyond pure tail parameter considerations. Even in the presence of tail parameters being close to normal, the return risk may still be tremendous, and can only be reduced by diversification effects in property portfolios, and only to a certain time-dependent extent. The results have strong implications for risk managers, fund managers and holders of large commercial real estate portfolios alike.},
  citeulike-article-id = {13997240},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2611905},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2691024code1091028.pdf?abstractid=2611905 and mirid=1},
  day                  = {30},
  groups               = {Diversified_Invest, Invest_TailRisk, Invest_RealEstate, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-04 22:56:41},
  timestamp            = {2020-02-25 21:15},
}

@InCollection{Takada-Stern-2017,
  author               = {Takada, Hellinton H. and Stern, Julio M.},
  booktitle            = {Publisher Logo Conference Proceedings},
  date                 = {2017},
  title                = {On portfolio risk diversification},
  doi                  = {10.1063/1.4985363},
  location             = {Ghent, Belgium},
  pages                = {070002+},
  url                  = {https://aip.scitation.org/doi/10.1063/1.4985363},
  abstract             = {The first portfolio risk diversification strategy was put into practice by the All Weather fund in 1996. The idea of risk diversification is related to the risk contribution of each available asset class or investment factor to the total portfolio risk. The maximum diversification or the risk parity allocation is achieved when the set of risk contributions is given by a uniform distribution. Meucci (2009) introduced the maximization of the Renyi entropy as part of a leverage constrained optimization problem to achieve such diversified risk contributions when dealing with uncorrelated investment factors. A generalization of the risk parity is the risk budgeting when there is a prior for the distribution of the risk contributions. Our contribution is the generalization of the existent optimization frameworks to be able to solve the risk budgeting problem. In addition, our framework does not possess any leverage constraint.},
  citeulike-article-id = {14520884},
  citeulike-linkout-0  = {http://dx.doi.org/10.1063/1.4985363},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-22 01:13:17},
  timestamp            = {2020-02-25 21:15},
}

@Article{Theron-vanVuuren-2018,
  author         = {Theron, Ludan and van Vuuren, Gary},
  date           = {2018-01-18},
  journaltitle   = {Cogent Economics \& Finance},
  title          = {The maximum diversification investment strategy: A portfolio performance comparison},
  doi            = {10.1080/23322039.2018.1427533},
  issn           = {2332-2039},
  number         = {1},
  volume         = {6},
  abstract       = {The efficacy of four different portfolio allocation strategies is evaluated according to their absolute returns during different economic conditions over a period of 10 years. A comparison is drawn between the Most Diversified portfolio (MD) and three alternatives; a Minimum Variance portfolio, an Equally-Weighted portfolio and a Tangent (or Maximum Sharpe ratio) portfolio. The aim is to assess portfolio performance using cumulative returns, the Sharpe ratio and the daily volatilities of each portfolio. The four asset allocation methods are governed by multiple constraints. Although previous work has shown that MD portfolios exhibit greater diversification and a higher Sharpe ratio than other investment strategies, this was not found using developed market index data.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Vermorken-et-al-2012,
  author               = {Vermorken, Maximilian A. and Medda, Francesca R. and Schroder, Thomas},
  date                 = {2012-10},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Diversification Delta: A Higher-MomentMeasure for Portfolio Diversification},
  doi                  = {10.3905/jpm.2012.39.1.067},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {67--74},
  volume               = {39},
  abstract             = {The concept of diversification is central in finance and has become even more so since the 2008 financial crisis.

In this article, the authors introduce a new measure for diversification. The measure, referred to as diversification delta, is nonparametric, based on higher moments, easily interpretable due to its mathematical formulation, and incorporates the advantages of the present measures of diversification while extending them.

The measure is applied to infrastructure returns data in order to understand the benefits of diversifying across various infrastructure classes, gaining useful insights for infrastructure fund managers and investors.},
  citeulike-article-id = {13972052},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2012.39.1.067},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:20:54},
  timestamp            = {2020-02-25 21:15},
}

@Article{Viceira-et-al-2017,
  author               = {Viceira, Luis M. and Wang, Zixuan and Zhou, John},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Global Portfolio Diversification for Long-Horizon Investors},
  url                  = {https://ssrn.com/abstract=2941652},
  abstract             = {This paper conducts a theoretical and empirical investigation of the risks of globally diversified portfolios of stocks and bonds and of optimal intertemporal global portfolio choice for long horizon investors in the presence of permanent cash flow shocks and transitory discount rate shocks to asset values. We show that an upward shift in cross-country one-period return correlations resulting from correlated cash flow shocks increases the risk of global portfolios and reduces investors' willingness to hold risky assets at all horizons. However, a similar upward shift in cross-country one-period return correlations resulting from correlated discount rate shocks has a much more muted effect on long-run portfolio risk and on the willingness to long horizon investors to hold risky assets. Correlated cash flow shocks imply that markets tend to move together at all horizons, thus reducing the scope for global diversification for all investors regardless of their investment horizon. By contrast, correlated discount rate shocks imply that markets tend to move together only transitorily and long-horizon investors can still benefit from global portfolios to diversify long-term cash flow risk. We document a secular increase in the cross-country correlations of stock and government bond returns since the late 1990's. We show that for global equities this increase has been driven primarily by increased cross-country correlations of discount rate shocks, or global capital markets integration, while for bonds it has been driven by both global capital markets integration and increased cross-country correlations of inflation shocks that determine the real cash flows of nominal government bonds. Therefore, despite the significant increase in the short-run correlation of global equity markets, the benefits from global equity portfolio diversification have not declined nearly as much for long-horizon investors as they have for short-horizon investors. By contrast, increased correlation of inflation across markets implies that the benefits of global bond portfolio diversification have declined for long-only bond investors at all horizons. However, it also means that the scope for hedging liabilities using global bonds has increased, benefiting investors with long-dated liabilities. Finally, we show that the well documented negative stock-bond correlation in the U.S. since the late 1990's is a global phenomenon, suggesting that the benefits of stock-bond diversification have increased in all developed markets.},
  citeulike-article-id = {14327821},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2941652},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-03 19:26:42},
  timestamp            = {2020-02-25 21:15},
}

@Article{Pastor-et-al-2017,
  author               = {Pastor, Lubos and Stambaugh, Robert F. and Taylor, Lucian A.},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Portfolio Liquidity and Diversification: Theory and Evidence},
  url                  = {https://ssrn.com/abstract=3016781},
  abstract             = {A portfolio's liquidity depends not only on the liquidity of its holdings but also on its diversification. We propose simple, theoretically motivated measures of portfolio liquidity and diversification. We also develop an equilibrium model relating portfolio liquidity to fund size, expense ratio, and turnover. As the model predicts, mutual funds with less liquid portfolios have smaller size, higher expense ratios, and lower turnover. The model also yields additional predictions that we verify empirically: larger funds are cheaper, funds that trade less are larger and cheaper, and funds that are too big perform worse. We also find that mutual fund portfolios have become more liquid because both components of diversification, coverage and balance, have trended upward.},
  citeulike-article-id = {14428169},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3016781},
  groups               = {Characteristics and return prediction, Diversification_Measure, Invest_Liquidity, Invest_Diversif},
  posted-at            = {2017-09-09 17:49:42},
  timestamp            = {2020-02-25 21:16},
}

@Article{Rudin-Morgan-2006,
  author               = {Rudin, Alexander M. and Morgan, Jonathan S.},
  date                 = {2006-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {A Portfolio Diversification Index},
  doi                  = {10.3905/jpm.2006.611807},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {81--89},
  volume               = {32},
  abstract             = {Despite the importance of diversification in portfolio construction, our current methods of measuring it are inefficient. Construction of a Portfolio Diversification Index (PDI) presents a new way to understand the concept. PDI, which measures the number of unique investments in a portfolio, is useful to assess marginal and cumulative diversification benefits across asset classes and across time. Implementation in hedge fund strategies reveals that various hedge funds offer less diversification than may have been thought, and that there has been reduced diversification in the past several years},
  citeulike-article-id = {14337565},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2006.611807},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-15 04:50:36},
  timestamp            = {2020-02-25 21:16},
}

@Article{Yu-2014,
  author               = {Yu, Jing-Rung and Lee, Wen-Yi and Chiou, Wan-Jiun P.},
  date                 = {2014-08},
  journaltitle         = {Applied Mathematics and Computation},
  title                = {Diversified portfolios with different entropy measures},
  doi                  = {10.1016/j.amc.2014.04.006},
  issn                 = {0096-3003},
  pages                = {47--63},
  volume               = {241},
  abstract             = {One of the major issues for Markowitz mean-variance model is the errors in estimations cause "corner solutions"and low diversity in the portfolio. In this paper, we compare the mean-variance efficiency, realized portfolio values, and diversity of the models incorporating different entropy measures by applying multiple criteria method. Differing from previous studies, we evaluate twenty-three portfolio over-time rebalancing strategies with considering short-sales and various transaction costs in asset diversification. Using the data of the most liquid stocks in Taiwan's market, our finding shows that the models with Yager's entropy yield higher performance because they respond to the change in market by reallocating assets more effectively than those with Shannon's entropy and with the minimax disparity model. Furthermore, including entropy in models enhances diversity of the portfolios and makes asset allocation more feasible than the models without incorporating entropy.},
  citeulike-article-id = {14310470},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.amc.2014.04.006},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-14 02:52:48},
  timestamp            = {2020-02-25 21:16},
}

@Article{Carmichael-et-al-2017,
  author               = {Carmichael, Benoit and Koumou, Gilles B. and Moran, Kevin},
  date                 = {2017-11-24},
  journaltitle         = {Quantitative Finance},
  title                = {Rao's quadratic entropy and maximum diversification indexation},
  doi                  = {10.1080/14697688.2017.1383625},
  pages                = {1--15},
  abstract             = {This paper proposes a new formulation of the maximum diversification indexation strategy based on Rao's Quadratic Entropy. It clarifies the investment problem underlying this diversification strategy, identifies the source of its out-of-sample performance, and suggests new dimensions along which this performance can be improved. We show that these potential improvements are quantitatively important and are robust to portfolio turnover, portfolio risk, estimation window, and covariance matrix estimation.},
  citeulike-article-id = {14486031},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1383625},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2017.1383625},
  day                  = {24},
  groups               = {Invest_Diversif},
  posted-at            = {2017-11-29 23:50:51},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 21:25},
}

@Article{Amenc-et-al-2017,
  author               = {Amenc, Noel and Ducoulombier, Frederic and Esakia, Mikheil and Goltz, Felix and Sivasubramanian, Sivagaminathan},
  date                 = {2017-03},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Accounting for Cross-Factor Interactions in Multifactor Portfolios without Sacrificing Diversification and Risk Control},
  doi                  = {10.3905/jpm.2017.43.5.099},
  issn                 = {0095-4918},
  number               = {5},
  pages                = {99--114},
  volume               = {43},
  citeulike-article-id = {14324691},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.5.099},
  groups               = {Factor_Types, Multifactor_Invest, Invest_Diversif},
  posted-at            = {2017-03-30 14:36:32},
  timestamp            = {2020-02-25 21:26},
}

@Article{Choueifaty-et-al-2013,
  author       = {Yves Choueifaty and Tristan Froidure and Julien Reynier},
  date         = {2013},
  journaltitle = {The Journal of Investment Strategies},
  title        = {Properties of the most diversified portfolio},
  number       = {2},
  pages        = {119--131},
  url          = {https://www.risk.net/journal-of-investment-strategies/2255764/properties-of-the-most-diversified-portfolio},
  volume       = {1},
  abstract     = {This article expands upon Toward Maximum Diversification by Choueifaty and Coignard [2008]. We present new mathematical properties of the Diversification Ratio and Most Diversified Portfolio (MDP), and investigate the optimality of the MDP in a mean-variance framework. We also introduce a set of Portfolio Invariance Properties, providing the basic rules an unbiased portfolio construction process should respect.

The MDP is then compared in light of these rules to popular methodologies (equal weights, equal risk contribution, minimum variance), and their performance is investigated over the past decade, using the MSCI World as reference universe. We believe that the results obtained in this article show that the MDP is a strong candidate for being the un-diversifiable portfolio, and as such delivers investors with the full benefit of the equity premium.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:29},
}

@Article{Heckel-et-al-2019,
  author       = {Heckel, Thomas and Amghar, Zine and Haik, Isaac and Laplenie, Olivier and de Carvalho, Raul Leote},
  date         = {2019},
  journaltitle = {The Journal of Fixed Income},
  title        = {Factor Investing in Corporate Bond Markets: Enhancing Efficacy Through Diversification and Purification!},
  url          = {https://jfi.pm-research.com/content/early/2019/10/03/jfi.2019.1.074},
  abstract     = {We show that factors from value, quality, low risk, and momentum styles play an important role in explaining the cross-section of corporate bond expected returns for the US and Euro Investment Grade and US BB-B Nonfinancial High Yield universes. We demonstrate the importance of purifying factor data by neutralizing a number of risk biases that are present in the factors: controlling for sectors, option-adjusted spread (OAS), duration, and size biases significantly increase the predictive power of style factors. We propose a new simple approach for efficiently neutralizing the biases from multiple risk variables and demonstrate its superiority relative to stratified sampling and optimization as alternative control methods. We also measure the added value from diversifying the number of factors in each style. Finally, we show that the results are robust in relation to transaction costs and can be used to design strategies that aim at outperforming traditional benchmark indexes.},
  groups       = {Invest_Diversif},
  timestamp    = {2020-02-25 21:31},
}

@Article{Sorensen-et-al-2018,
  author         = {Sorensen, Eric and Barnes, Mark and Alonso, Nick and Qian, Edward},
  date           = {2018-03-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Not all factor exposures are created equal},
  doi            = {10.3905/jpm.2018.44.4.039},
  issn           = {0095-4918},
  number         = {4},
  pages          = {39--45},
  volume         = {44},
  abstract       = {Approaches to quantitative equity investing have evolved markedly. Thirty years ago, the focus was on alpha generation, but with the recent decade acceptance of smart (alternative) beta, the focus is turning to transparent methods of construction for factor investing. In this article, the authors present an approach for evaluating various methods of portfolio construction that lead to the same factor exposures. Four portfolios are of interest: factor weighted, cap weighted, equal weighted, and risk parity weighted. The authors compare these portfolios based on standard performance statistics as well as new metrics of value-added, such as performance participation rates and portfolio sector concentrations. The results indicate that once the desired factor exposure is achieved, it is beneficial to build the portfolio with the most desirable characteristics in terms of diversification.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_SmartBeta},
  timestamp      = {2020-02-25 21:32},
}

@Article{Bardoscia-et-al-2019,
  author         = {Bardoscia, Marco and {d'Arienzo}, Daniele and Marsili, Matteo and Volpati, Valerio},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Lost in Diversification},
  doi            = {10.2139/ssrn.3323440},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3323440},
  abstract       = {As financial instruments grow in complexity more and more information is neglected by risk optimization practices. This brings down a curtain of opacity on the origination of risk, that has been one of the main culprits in the 2007-2008 global financial crisis. We discuss how the loss of transparency may be quantified in bits, using information theoretic concepts. We find that i) financial transformations imply large information losses, ii) portfolios are more information sensitive than individual stocks only if fundamental analysis is sufficiently informative on the co-movement of assets, that iii) securitisation, in the relevant range of parameters, yields assets that are less information sensitive than the original stocks and that iv) when diversification (or securitisation) is at its best (i.e. when assets are uncorrelated) information losses are maximal. We also address the issue of whether pricing schemes can be introduced to deal with information losses. This is relevant for the transmission of incentives to gather information on the risk origination side. Within a simple mean variance scheme, we find that market incentives are not generally sufficient to make information harvesting sustainable.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:34},
}

@Article{Bennyhoff-2009,
  author               = {Bennyhoff, Donald G.},
  date                 = {2009-02},
  journaltitle         = {The Journal of Investing},
  title                = {Time Diversification and Horizon-Based Asset Allocations},
  doi                  = {10.3905/joi.2009.18.1.045},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {45--52},
  volume               = {18},
  abstract             = {Time diversification, the concept that investments in stocks are less risky over longer periods than shorter ones, has been the subject of spirited debate for decades. Over the last few years the growing acceptance of life cycle investment products, such as target retirement mutual funds, has renewed interest in this topic. The objective of this article is not to prove or disprove time diversification, but to evaluate whether the concept must be valid for a horizon-based asset allocation framework to be viable and appropriate. Our findings suggest that there is little evidence to support the notion that time moderates the perceived volatility inherent in risky assets. However, we would expect the risk/reward relationships of the past to prevail in the future, and if that is the case, a longer investment horizon may support a willingness and ability to assume the greater uncertainty of equity-centric asset allocations. This may be true particularly for younger investors for whom the allocation to human capital and the risk posed by the erosion of purchasing power by inflation can reasonably be assumed to be greatest.},
  citeulike-article-id = {14322263},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2009.18.1.045},
  groups               = {Human_Capital, Invest_Diversif},
  posted-at            = {2017-03-29 07:02:30},
  timestamp            = {2020-02-25 21:35},
}

@Article{Dees-et-al-2019,
  author         = {Dees, Bruno Scalzo and Stankovic, Ljubisa and Constantinides, Anthony G. and Mandic, Danilo P.},
  date           = {2019-10-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Portfolio Cuts: A Graph-Theoretic Framework to Diversification},
  url            = {https://arxiv.org/abs/1910.05561},
  urldate        = {2019-10-24},
  abstract       = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 21:35},
}

@Article{Sebastian-Gebbie-2019,
  author         = {Sebastian, Ann and Gebbie, Tim},
  date           = {2019-10-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Systematic Asset Allocation using Flexible Views for South African Markets},
  url            = {https://arxiv.org/abs/1910.05555},
  urldate        = {2019-10-24},
  abstract       = {We implement a systematic asset allocation model using the Historical Simulation with Flexible Probabilities (HS-FP) framework developed by Meucci. The HS-FP framework is a flexible non-parametric estimation approach that considers future asset class behavior to be conditional on time and market environments, and derives a forward looking distribution that is consistent with this view while remaining close as possible to the prior distribution. The framework derives the forward looking distribution by applying unequal time and state conditioned probabilities to historical observations of asset class returns. This is achieved using relative entropy to find estimates with the least distortion to the prior distribution. Here, we use the HS-FP framework on South African financial market data for asset allocation purposes; by estimating expected returns, correlations and volatilities that are better represented through the measured market cycle. We demonstrated a range of state variables that can be useful towards understanding market environments. Concretely, we compare the out-of-sample performance for a specific configuration of the HS-FP model relative to classic Mean Variance Optimization(MVO) and Equally Weighted (EW) benchmark models. The framework displays low probability of backtest overfitting and the out-of-sample net returns and Sharpe ratio point estimates of the HS-FP model outperforms the benchmark models. However, the results are inconsistent when training windows are varied, the Sharpe ratio is seen to be inflated, and the method does not demonstrate statistically significant out-performance on a gross and net basis.},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 21:36},
}

@Article{Barkhagen-et-al-2019,
  author         = {Barkhagen, Mathias and Fleming, Brian and Quiles, Sergio Garcia and Gondzio, Jacek and Kalcsics, Joerg and Kroeske, Jens and Sabanis, Sotirios and Staal, Arne},
  date           = {2019-06-03},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Optimising portfolio diversification and dimensionality},
  url            = {https://arxiv.org/abs/1906.00920},
  urldate        = {2019-10-02},
  abstract       = {A new framework for portfolio diversification is introduced which goes beyond the classical mean-variance approach and portfolio allocation strategies such as risk parity. It is based on a novel concept called portfolio dimensionality that connects diversification to the non-Gaussianity of portfolio returns and can typically be defined in terms of the ratio of risk measures which are homogenous functions of equal degree. The latter arises naturally due to our requirement that diversification measures should be leverage invariant. We introduce this new framework and argue the benefits relative to existing measures of diversification in the literature, before addressing the question of optimizing diversification or, equivalently, dimensionality. Maximising portfolio dimensionality leads to highly non-trivial optimization problems with objective functions which are typically non-convex and potentially have multiple local optima. Two complementary global optimization algorithms are thus presented. For problems of moderate size and more akin to asset allocation problems, a deterministic Branch and Bound algorithm is developed, whereas for problems of larger size a stochastic global optimization algorithm based on Gradient Langevin Dynamics is given. We demonstrate analytically and through numerical experiments that the framework reflects the desired properties often discussed in the literature.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:41},
}

@Article{Louton-Saraoglu-2008,
  author         = {Louton, David and Saraoglu, Hakan},
  date           = {2008-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {How Many Mutual Funds Are Needed to Form a Well- Diversified Asset Allocated Portfolio?},
  doi            = {10.3905/joi.2008.710919},
  issn           = {1068-0896},
  number         = {3},
  pages          = {47--63},
  urldate        = {2019-07-15},
  volume         = {17},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:43},
}

@Article{Meucci-et-al-2015,
  author       = {Attilio Meucci and Alberto Santangelo and Romain Deguest},
  date         = {2015},
  journaltitle = {Risk Magazine},
  title        = {Risk budgeting and diversification based on optimised uncorrelated factors},
  url          = {https://www.risk.net/risk-management/2433224/risk-budgeting-and-diversification-based-on-optimised-uncorrelated-factors},
  abstract     = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies.

The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification.

We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the S\&P 500, and a factor-based investment in the five Fama-French factors.},
  groups       = {Risk_Budgeting, Invest_Risk, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:43},
}

@Article{Jennings-Payne-2016,
  author               = {Jennings, William W. and Payne, Brian C.},
  date                 = {2016-03},
  journaltitle         = {Financial Analysts Journal},
  title                = {Fees Eat Diversification's Lunch},
  doi                  = {10.2469/faj.v72.n2.1},
  issn                 = {0015-198X},
  number               = {2},
  pages                = {31--40},
  volume               = {72},
  abstract             = {Although diversification is often spoken of as the only free lunch in investing, the authors show that it is not free and that it must be considered in light of its costs. They also show that fees on diversifying asset classes are high relative to their risk-adjusted diversification benefit, with the more exotic asset classes carrying higher price tags. Because there is meaningful cross-sectional variation, fees need to be considered when making strategic asset allocation decisions.},
  citeulike-article-id = {14150315},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v72.n2.1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-02 16:52:21},
  timestamp            = {2020-02-25 21:45},
}

@Article{Kritzman-2015,
  author               = {Kritzman, Mark},
  date                 = {2015-01},
  journaltitle         = {Financial Analysts Journal},
  title                = {What Practitioners Need to Know ... About Time Diversification (corrected)},
  doi                  = {10.2469/faj.v71.n1.4},
  issn                 = {0015-198X},
  number               = {1},
  pages                = {29--34},
  volume               = {71},
  abstract             = {Although an investor may be less likely to lose money over a long horizon than over a short horizon, the magnitude of a potential loss increases with the length of the investment horizon.},
  citeulike-article-id = {14514123},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v71.n1.4},
  groups               = {Invest_Diversif},
  posted-at            = {2018-01-09 16:47:05},
  timestamp            = {2020-02-25 21:46},
}

@Article{Chollete-et-al-2011,
  author       = {Chollete, L. and de la Pena, V. and Lu, C.},
  date         = {2011},
  journaltitle = {Journal of Banking and Finance},
  title        = {International Diversification: A Copula Approach},
  number       = {2},
  pages        = {403--417},
  url          = {https://www.sciencedirect.com/science/article/pii/S0378426610003298},
  volume       = {35},
  abstract     = {The viability of international diversification involves balancing benefits and costs. This balance hinges on the degree of asset dependence. In light of theoretical research linking diversification and dependence, we examine international diversification using two measures of dependence: correlations and copulas.

We document several findings.

First, dependence has increased over time.

Second, we find evidence of asymmetric dependence or downside risk in Latin America, but less in the G5. The results indicate very little downside risk in East Asia.

Third, East Asian and Latin American returns exhibit some correlation complexity. Interestingly, the regions with maximal dependence or worst diversification do not command large returns. Our results suggest international limits to diversification. They are also consistent with a possible tradeoff between international diversification and systemic risk.},
  groups       = {Diversification_Measure, Asymm_Dependence},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:47},
}

@Article{Carmichael-et-al-2015a,
  author         = {Carmichael, Benoit and Koumou, Gilles and Moran, Kevin},
  date           = {2015-09},
  journaltitle   = {SSRN Electronic Journal},
  title          = {A New Formulation of Maximum Diversification Indexation Using Rao's Quadratic Entropy},
  url            = {https://ssrn.com/abstract=2923220},
  abstract       = {This paper proposes a new formulation of the Maximum Diversification indexation strategy based on Rao Quadratic Entropy (RQE). It clarifies the investment problem underlying the Most Diversified Portfolio (MDP) formed with this strategy, identifies the source of the MDP out-of-sample performance, and suggests dimensions along which this performance can be improved. We show that these potential improvements are quantitatively important and are robust to portfolio turnover, portfolio risk, estimation window, and covariance matrix estimation.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:52},
}

@Conference{Lee-2013,
  author    = {Wai Lee},
  booktitle = {Second Annual Inside Indexing Conference},
  date      = {2013},
  title     = {Risk Based Asset Allocation},
  url       = {https://pdfs.semanticscholar.org/6101/ee13a5ac3a2387441351be7ffd03b6a8a9d1.pdf},
  abstract  = {In recent years, we have witnessed an alarmingly large and growing amount of literature on portfolio construction approaches focused on risks and diversification rather than estimating expected returns. Numerous simulations, applied to different universes, have been documented in support of these approaches based on their apparent outperformance versus passive market-capitalization weighting or static, fixed weight portfolios. Many studies attribute the better performance of these risk-based asset allocation approaches to superior diversification.

Given the absence of clearly defined investment objective functions behind these approaches as well as the metrics used by these studies to evaluate ex-post performance, we put these approaches into the same context of mean-variance efficiency in an attempt to understand their theoretical underpinnings. In doing so, we hope to shed some light on what these approaches attempt to achieve and on the characteristics of the investment universe, if indeed these approaches are meant to approximate meanvariance efficiency.

Rather than adding to the already large collection of simulation results, we use some simple examples to compare and contrast the portfolio and risk characteristics of these approaches. We also reiterate that any portfolio that deviates from the market capitalization-weighted portfolio is an active portfolio.

Finally, we conclude there is no theory to predict, ex-ante, that any of these riskbased approaches should outperform.},
  groups    = {Diversification_Measure},
  owner     = {zkgst0c},
  timestamp = {2020-02-25 21:52},
}

@Article{Cesarone-et-al-2019,
  author         = {Cesarone, Francesco and Scozzari, Andrea and Tardella, Fabio},
  date           = {2019-07-25},
  journaltitle   = {Journal of Global Optimization},
  title          = {An optimization-diversification approach to portfolio selection},
  doi            = {10.1007/s10898-019-00809-7},
  issn           = {0925-5001},
  urldate        = {2019-09-10},
  abstract       = {The classical approaches to optimal portfolio selection call for finding a feasible portfolio that optimizes a risk measure, or a gain measure, or a combination thereof by means of a utility function or of a performance measure. However, the optimization approach tends to amplify the estimation errors on the parameters required by the model, such as expected returns and covariances. For this reason, the Risk Parity model, a novel risk diversification approach to portfolio selection, has been recently theoretically developed and used in practice, mainly for the case of the volatility risk measure. Here we first provide new theoretical results for the Risk Parity approach for general risk measures. Then we propose a novel framework for portfolio selection that combines the diversification and the optimization approaches through the global solution of a hard nonlinear mixed integer or pseudo Boolean problem. For the latter problem we propose an efficient and accurate Multi-Greedy heuristic that extends the classical single-threaded greedy approach to a multiple-threaded setting. Finally, we provide empirical results on real-world data showing that the diversified optimal portfolios are only slightly suboptimal in-sample with respect to optimal portfolios, and generally show improved out-of-sample performance with respect to their purely diversified or purely optimized counterparts.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:53},
}

@Article{Darnell-2009,
  author       = {Max Darnell},
  date         = {2009},
  journaltitle = {SSRN Electronic Journal},
  title        = {Did Diversification Fail?},
  url          = {https://www.firstquadrant.com/system/files/2009_10_Did_Diversification_Fail_0.pdf},
  abstract     = {Many investors had felt that by spreading their investments across many asset classes - by investing in a wide array of betas - that they would avoid the risk of an across-theboard decline in their investments. They thought that they had avoided the problem associated with putting all their eggs in one basket as the adage advises. When most assets did fall together in largely simultaneous fashion in the midst of the recent credit crisis, investors rated diversification a failure, and cried out in frustration that correlations had all converged on one. Diversification failed this year, 1 was the title of a New York Times article in the business section in November last year. In another, more recent article,2 one of the large university endowments explained that diversification had failed to protect its asset values. This sentiment was, and is, entirely common. If their eggs were all in different baskets, then it would appear that they were somehow all tied together sharing a common fate when their fates were assumed to have been independent of one another instead. There are several aspects of this that are wrong. Diversification didn't fail; the metaphor of eggs in different baskets doesn't accurately capture the purpose of diversification; and those weren't betas that they diversified across},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:54},
}

@InCollection{deCarvalho-et-al-2017a,
  author               = {{de Carvalho}, Raul L. and Lu, Xiao and Soupe, Francois and Dugnolle, Patrick},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversify and Purify Factor Premiums in Equity Markets},
  doi                  = {10.1016/b978-1-78548-201-4.50004-0},
  isbn                 = {9781785482014},
  pages                = {73--97},
  publisher            = {Elsevier},
  abstract             = {In this chapter, we consider the question of how to improve the efficacy of strategies designed to capture factor premiums in equity markets and, in particular, from the value, quality, low-risk and momentum factors. We consider a number of portfolio construction approaches designed to capture factor premiums with the appropriate levels of risk controls aiming at increasing information ratios. We show that information ratios can be increased by targeting constant volatility (CV) over time, hedging market beta (HB) and hedging exposures to the size factor, i.e. neutralizing biases in the market capitalization of stocks used in factor strategies. With regard to the neutralization of sector exposures, we find this to be of particular importance for the value and low-risk factors. Finally, we look at the added value of shorting stocks in factor strategies. We find that with few exceptions the contributions to performance from the short leg are inferior to those from the long leg. Thus, long-only strategies can be efficient alternatives to capture these factor premiums. Finally, we find that factor premiums tend to have fatter tails than what could be expected from a Gaussian distribution of returns, but that skewness is not significantly negative in most cases.},
  citeulike-article-id = {14499092},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50004-0},
  groups               = {Invest_Factor, Invest_Risk, Factor_Types, Invest_Diversif},
  posted-at            = {2017-12-08 00:48:48},
  timestamp            = {2020-02-25 21:55},
}

@Article{DeMiguel-et-al-2009a,
  author       = {DeMiguel, V. and Garlappi, L. and Uppal, R.},
  date         = {2009},
  journaltitle = {Review of Financial Studies},
  title        = {Optimal versus Naive Diversification: how Inefficient is the 1/N Portfolio Strategy},
  doi          = {10.1093/rfs/hhm075},
  pages        = {1915--1953},
  url          = {Optimal versus Naive Diversification: how Inefficient is the 1/N
Portfolio Strategy},
  volume       = {22},
  abstract     = {We evaluate the out-of-sample performance of the sample-based mean-variance model, and its extensions designed to reduce estimation error, relative to the naive 1/N portfolio. Of the 14 models we evaluate across seven empirical datasets, none is consistently better than the 1/N rule in terms of Sharpe ratio, certainty-equivalent return, or turnover, which indicates that, out of sample, the gain from optimal diversification is more than offset by estimation error. Based on parameters calibrated to the US equity market, our analytical results and simulations show that the estimation window needed for the sample-based mean-variance strategy and its extensions to outperform the 1/N benchmark is around 3000 months for a portfolio with 25 assets and about 6000 months for a portfolio with 50 assets. This suggests that there are still many miles to go before the gains promised by optimal portfolio choice can actually be realized out of sample.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:55},
}

@Article{Domian-et-al-2007,
  author               = {Domian, Dale L. and Louton, David A. and Racine, Marie D.},
  date                 = {2007-11},
  journaltitle         = {Financial Review},
  title                = {Diversification in Portfolios of Individual Stocks: 100 Stocks Are Not Enough},
  doi                  = {10.1111/j.1540-6288.2007.00183.x},
  issn                 = {0732-8516},
  number               = {4},
  pages                = {557--570},
  volume               = {42},
  abstract             = {We examine returns and ending wealth in portfolios selected from 1,000 large U.S. stocks over a 20-year holding period. Shortfall risk, the possibility of ending wealth being below a target, is a useful metric for long horizon investors and is consistent with the Safety First criterion. Density functions obtained from simulations illustrate that shortfall risk reduction continues as portfolio size is increased, even above 100 stocks. A slightly lower risk can be achieved in small portfolios by diversifying across industries, but a greater reduction is obtained by simply increasing the number of stocks.},
  citeulike-article-id = {1948681},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1540-6288.2007.00183.x},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/bpl/fire/2007/00000042/00000004/art00004},
  day                  = {1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-07 04:12:43},
  publisher            = {Blackwell Publishing Inc},
  timestamp            = {2020-02-25 21:56},
}

@Article{Fouquau-et-al-2018,
  author         = {Fouquau, Julien and Kharoubi, Cecile and Spieser, Philippe},
  date           = {2018},
  journaltitle   = {The Journal of Risk},
  title          = {International and temporal diversifications: the best of both worlds?},
  doi            = {10.21314/{JOR}.2018.382},
  issn           = {1465-1211},
  url            = {https://www.risk.net/journal-of-risk/5472731/international-and-temporal-diversifications-the-best-of-both-worlds},
  urldate        = {2019-05-30},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:57},
}

@Article{Hallerbach-2017,
  author               = {Hallerbach, Winfried G.},
  date                 = {2017-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {If You Have Said A, You Must Also Say B: Calculating Diversified Asset Returns},
  doi                  = {10.3905/jwm.2017.20.2.076},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {76--81},
  volume               = {20},
  abstract             = {The bottom-up route to portfolio diversification is clear: Combining individual assets into a portfolio will lower portfolio risk (especially when correlations are low). The top-down route of evaluating individual assets from the perspective of the diversified portfolio is widely applied in risk budgeting but is neglected in return attributions. Consequently, many investors evaluate individual assets on the basis of their undiversified returns instead of including the diversification benefits they offer. This perspective biases the evaluation of high-volatility/low-correlation assets in the portfolio. In this note, we highlight the importance of evaluating diversified returns and show how we can calculate these returns. We illustrate the method for a U.S. asset portfolio.},
  citeulike-article-id = {14402574},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.20.2.076},
  groups               = {Invest_Diversif},
  posted-at            = {2017-07-30 07:46:31},
  timestamp            = {2020-02-25 21:58},
}

@Article{Humphrey-et-al-2015,
  author               = {Humphrey, Jacquelyn E. and Benson, Karen L. and Low, Rand K. Y. and Lee, Wei-Lun},
  date                 = {2015-11},
  journaltitle         = {Pacific-Basin Finance Journal},
  title                = {Is diversification always optimal?},
  doi                  = {10.1016/j.pacfin.2015.09.003},
  issn                 = {0927-538X},
  pages                = {521--532},
  volume               = {35},
  abstract             = {Should retirement savers diversify across many funds or consolidate into one fund? We examine Australian retirement savings. Theoretically, diversification across funds is the optimal strategy. With real-world short-selling constraints, investment in a single fund is optimal. Finance theory and recent literature suggest that investors should diversify their retirement savings across a number of funds. However, the Australian government encourages investors to consolidate retirement savings into just one fund. Using a number of optimization techniques, we investigate which of these two actions would result in the best outcome for investors in terms of risk and return. We find that in the majority of cases investors would be better off not diversifying their holdings; mainly because superannuation funds cannot be short sold. Consolidation therefore does appear to be the optimal strategy for the average superannuation investor.},
  citeulike-article-id = {14160270},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.pacfin.2015.09.003},
  groups               = {Invest_Diversif},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 20:57:58},
  timestamp            = {2020-02-25 21:59},
}

@Article{Jacobs-et-al-2014,
  author               = {Jacobs, Heiko and M uller, Sebastian and Weber, Martin},
  date                 = {2014-06},
  journaltitle         = {Journal of Financial Markets},
  title                = {How should individual investors diversify? An empirical evaluation of alternative asset allocation policies},
  doi                  = {10.1016/j.finmar.2013.07.004},
  issn                 = {1386-4181},
  pages                = {62--85},
  volume               = {19},
  abstract             = {For global equity diversification, prominent Markowitz extensions do not outperform several heuristic weighting schemes (1/N heuristic, market value-weighting and GDP-weighting). Comparing the different heuristic stock weighting schemes, the value-weighted heuristic performs worse than the GDP-weighted global stock portfolio. Diversification gains in the asset allocation context are mainly driven by a well-balanced allocation over different asset classes. Consistent with global equity diversification, Markowitz-based optimization methods do not add significant value when allocating across different asset classes. This paper evaluates numerous diversification strategies as a possible remedy against widespread costly investment mistakes of individual investors. Our results reveal that a very broad range of simple heuristic allocation schemes offers similar diversification gains as well-established or recently developed portfolio optimization approaches. This holds true for both international diversification in the stock market and diversification over different asset classes. We thus suggest easy-to-implement allocation guidelines for individual investors.},
  citeulike-article-id = {13987903},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.finmar.2013.07.004},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-25 08:36:39},
  timestamp            = {2020-02-25 21:59},
}

@Article{McKay-et-al-2017,
  author               = {McKay, Shawn and Shapiro, Robert and Thomas, Ric},
  date                 = {2017-11},
  journaltitle         = {Financial Analysts Journal},
  title                = {What Free Lunch? The Costs of Overdiversification},
  doi                  = {10.2469/faj.v74.n1.2},
  issn                 = {0015-198X},
  pages                = {1--15},
  abstract             = {Institutional investors, charged with outperforming a policy benchmark, often allocate to external active managers in order to hit their return objective. The challenge is to do so without overdiversifying the plan. Hiring too many managers can significantly reduce active risk, leaving the plan with high fees and limited ability to outperform a policy benchmark. We review the number of external investment strategies held by the largest US public and corporate pension funds. Our analysis shows that most large pension funds are overdiversified, allowing us to suggest a simpler framework for moving forward.},
  citeulike-article-id = {14485296},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v74.n1.2},
  day                  = {20},
  groups               = {BenchmarkInvest, Invest_Diversif},
  posted-at            = {2017-11-28 18:26:03},
  timestamp            = {2020-02-25 22:00},
}

@Article{Page-Taborsky-2011,
  author       = {Sebastien Page and Mark A. Taborsky},
  date         = {2011},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The myth of diversification: risk factors versus asset classes},
  number       = {4},
  pages        = {1--2},
  url          = {https://jpm.pm-research.com/content/36/1/26},
  volume       = {37},
  abstract     = {In our New Normal world, regime shifts in economic conditions will continue to cause significant challenges for risk management and portfolio construction. On average, correlations across risk factors are lower than correlations across asset classes, and risk factor correlations tend to be more robust to regime shifts. Risk factors provide a flexible language with which investors may express their forward-looking economic views, adapt to regime shifts and diversify their portfolios accordingly.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 22:01},
}

@Article{Pittman-et-al-2019,
  author         = {Pittman, Sam and Singh, Amneet and Srinivasan, Sangeetha},
  date           = {2019-09-07},
  journaltitle   = {The Journal of Wealth Management},
  title          = {Diversification benefits, where art thou?},
  doi            = {10.3905/jwm.2019.1.081},
  issn           = {1534-7524},
  pages          = {jwm.2019.1.081},
  urldate        = {2019-09-28},
  abstract       = {Following the global financial crisis, a portfolio concentrated in US large cap equity and aggregate fixed income has provided higher returns than diversified portfolios through 2019. Such a prolonged experience causes investors to question the benefits of diversification. This leads us to use a longer history of data across 15 asset classes to understand the historical benefits of diversifying a portfolio with international equity, real assets, and below investment grade fixed income. Our results portray the frequency and magnitude of risk-adjusted return improvement coming from different diversifying asset classes over five-year holding periods. We find that certain asset classes, such as below investment grade fixed income, regularly improve risk-adjusted return of the portfolio, while other asset classes like commodities improve risk-adjusted returns less frequently. Further, we observe that some asset classes do not deliver meaningful risk-adjusted return improvements in the presence of other asset classes. Our conclusion is that investors should continue to build diversified portfolios, but in doing so they should consider that some asset classes more consistently improved risk-adjusted returns than others.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 22:01},
}

@Article{Pourbabaee-et-al-2016,
  author               = {Pourbabaee, Farzad and Kwak, Minsuk and Pirvu, Traian A.},
  date                 = {2016-09},
  journaltitle         = {Quantitative Finance},
  title                = {Risk minimization and portfolio diversification},
  doi                  = {10.1080/14697688.2015.1115891},
  number               = {9},
  pages                = {1325--1332},
  volume               = {16},
  citeulike-article-id = {14150627},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1115891},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1115891},
  day                  = {1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-03 01:18:37},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 22:02},
}

@Article{Vandenbroucke-2019,
  author         = {Vandenbroucke, Jurgen},
  date           = {2019-05-09},
  journaltitle   = {The Journal of Investing},
  title          = {Adaptive Portfolios and the Power of Diversification},
  url            = {https://joi.iijournals.com/content/early/2019/05/09/joi.2019.1.089},
  urldate        = {2019-05-09},
  abstract       = {The article gives a qualitative description of an advisory or discretionary investment process that manages the emotional aspect of investing. Portfolios are adaptive, meaning they automatically adjust their allocation in response to changing market conditions. The adjustments are model-based and transparent, and align in terms of frequency and magnitude with the investor’s emotionality. The process looks beyond the risk-focused paradigm in relation to investor profiling, product positioning, and portfolio construction. First, investor profiles distinguish between the attitude toward risk and the attitude toward loss. Second, products differentiate in terms of variance and in terms of skewness. Finally, adaptive portfolios represent a client centric combination of products that lifts the power of diversification to a higher level and ultimately contributes to long term buy-and-hold investor behavior.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 22:03},
}

@Article{Adachi-Trendafilov-2017,
  author               = {Adachi, Kohei and Trendafilov, NickolayT},
  date                 = {2017},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Sparsest factor analysis for clustering variables: a matrix decomposition approach},
  doi                  = {10.1007/s11634-017-0284-z},
  pages                = {1--27},
  abstract             = {We propose a new procedure for sparse factor analysis (FA) such that each variable loads only one common factor. Thus, the loading matrix has a single nonzero element in each row and zeros elsewhere. Such a loading matrix is the sparsest possible for certain number of variables and common factors. For this reason, the proposed method is named sparsest FA (SSFA). It may also be called FA-based variable clustering, since the variables loading the same common factor can be classified into a cluster. In SSFA, all model parts of FA (common factors, their correlations, loadings, unique factors, and unique variances) are treated as fixed unknown parameter matrices and their least squares function is minimized through specific data matrix decomposition. A useful feature of the algorithm is that the matrix of common factor scores is re-parameterized using QR decomposition in order to efficiently estimate factor correlations. A simulation study shows that the proposed procedure can exactly identify the true sparsest models. Real data examples demonstrate the usefulness of the variable clustering performed by SSFA.},
  citeulike-article-id = {14433260},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-017-0284-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-017-0284-z},
  groups               = {Sparse factor analysis, Clustering and network analysis},
  posted-at            = {2017-09-17 20:19:21},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 03:51},
}

@Article{Aghabozorgi-et-al-2015,
  author               = {Aghabozorgi, Saeed and Seyed Shirkhorshidi, Ali and Ying Wah, Teh},
  date                 = {2015-10},
  journaltitle         = {Information Systems},
  title                = {Time-series clustering - A decade review},
  doi                  = {10.1016/j.is.2015.04.007},
  issn                 = {0306-4379},
  pages                = {16--38},
  volume               = {53},
  abstract             = {Anatomy of time-series clustering is revealed by introducing its 4 main component. Research works in each of the four main components are reviewed in detail and compared. Analysis of research works published in the last decade. Enlighten new paths for future works for time-series clustering and its components. Clustering is a solution for classifying enormous data when there is not any early knowledge about classes. With emerging new concepts like cloud computing and big data and their vast applications in recent years, research works have been increased on unsupervised solutions like clustering algorithms to extract knowledge from this avalanche of data. Clustering time-series data has been used in diverse scientific areas to discover patterns which empower data analysts to extract valuable information from complex and massive datasets. In case of huge datasets, using supervised classification solutions is almost impossible, while clustering can solve this problem using un-supervised approaches. In this research work, the focus is on time-series data, which is one of the popular data types in clustering problems and is broadly used from gene expression data in biology to stock market analysis in finance. This review will expose four main components of time-series clustering and is aimed to represent an updated investigation on the trend of improvements in efficiency, quality and complexity of clustering time-series approaches during the last decade and enlighten new paths for future works.},
  citeulike-article-id = {14168675},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.is.2015.04.007},
  keywords             = {pdf},
  posted-at            = {2016-10-19 19:24:13},
  timestamp            = {2020-02-27 03:51},
}

@InCollection{Alaiz-et-al-2014,
  author               = {Alaz, Carlos M and Fernandez, Angela and Gala, Yvonne and Dorronsoro, Jose R},
  booktitle            = {Intelligent Data Engineering and Automated Learning - IDEAL 2014},
  date                 = {2014},
  title                = {Kernel K-Means Low Rank Approximation for Spectral Clustering and Diffusion Maps},
  doi                  = {10.1007/978-3-319-10840-7\_30},
  editor               = {Corchado, Emilio and Lozano, JoseA and Quintian, Hector and Yin, Hujun},
  pages                = {239--246},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {8669},
  abstract             = {Spectral Clustering and Diffusion Maps are currently the leading methods for advanced clustering or dimensionality reduction. However, they require the eigenanalysis of a sample's graph Laplacian L, something very costly for moderately sized samples and prohibitive for very large ones. We propose to build a low rank approximation to L using essentially the centroids obtained applying kernel K-means over the similarity matrix. We call this approach kernel KASP (kKASP) as it follows the KASP procedure of Yan et al. but coupling centroid selection with the local geometry defined by the similarity matrix. As we shall see, kKASP's reconstructions are competitive with KASP's ones, particularly in the low rank range.},
  citeulike-article-id = {14212409},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-10840-730},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-10840-730},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:14:56},
  timestamp            = {2020-02-27 03:51},
}

@Article{Albatineh-NiewiadomskaBugaj-2011,
  author         = {Albatineh, Ahmed N. and Niewiadomska-Bugaj, Magdalena},
  date           = {2011-07},
  journaltitle   = {Journal of Classification},
  title          = {MCS: A method for finding the number of clusters},
  doi            = {10.1007/s00357-010-9069-1},
  issn           = {0176-4268},
  number         = {2},
  pages          = {184--209},
  volume         = {28},
  abstract       = {This paper proposes a maximum clustering similarity (MCS) method for determining the number of clusters in a data set by studying the behavior of similarity indices comparing two (of several) clustering methods. The similarity between the two clusterings is calculated at the same number of clusters, using the indices of Rand (R), Fowlkes and Mallows (FM), and Kulczynski (K) each corrected for chance agreement. The number of clusters at which the index attains its maximum is a candidate for the optimal number of clusters. The proposed method is applied to simulated bivariate normal data, and further extended for use in circular data. Its performance is compared to the criteria discussed in Tibshirani, Walther, and Hastie (2001). The proposed method is not based on any distributional or data assumption which makes it widely applicable to any type of data that can be clustered using at least two clustering algorithms.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Ando-Bai-2017,
  author               = {Ando, Tomohiro and Bai, Jushan},
  date                 = {2017-06},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Clustering Huge Number of Financial Time Series: A Panel Data Approach With High-Dimensional Predictors and Factor Structures},
  doi                  = {10.1080/01621459.2016.1195743},
  number               = {519},
  pages                = {1182--1198},
  volume               = {112},
  abstract             = {AbstractThis article introduces a new procedure for clustering a large number of financial time series based on high-dimensional panel data with grouped factor structures. The proposed method attempts to capture the level of similarity of each of the time series based on sensitivity to observable factors as well as to the unobservable factor structure. The proposed method allows for correlations between observable and unobservable factors and also allows for cross-sectional and serial dependence and heteroscedasticities in the error structure, which are common in financial markets. In addition, theoretical properties are established for the procedure. We apply the method to analyze the returns for over 6000 international stocks from over 100 financial markets. The empirical analysis quantifies the extent to which the U.S. subprime crisis spilled over to the global financial markets. Furthermore, we find that nominal classifications based on either listed market, industry, country or region are insufficient to characterize the heterogeneity of the global financial markets. Supplementary materials for this article are available online.},
  citeulike-article-id = {14431349},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/01621459.2016.1195743},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2016.1195743},
  day                  = {10},
  groups               = {Clustering and network analysis, Scenario_TimeSeries},
  posted-at            = {2017-09-16 16:57:06},
  publisher            = {Taylor \& Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Arakelian-Karlis-2014,
  author               = {Arakelian, Veni and Karlis, Dimitris},
  date                 = {2014-01},
  journaltitle         = {Communications in Statistics - Simulation and Computation},
  title                = {Clustering Dependencies Via Mixtures of Copulas},
  doi                  = {10.1080/03610918.2012.752832},
  number               = {7},
  pages                = {1644--1661},
  volume               = {43},
  abstract             = {The use of mixture models for clustering purposes has been considerably increased the last years primarily due to the existence of efficient computational methods that facilitate estimation. Nowadays, there are several clustering procedures based on mixtures for certain types of data. On the other hand, copulas are becoming very popular models to model dependencies as one of their appealing properties is the separation of the marginal properties of the data from the dependence properties. The purpose of this article is to put together the two distinct ideas, namely mixtures and copulas, so as to use mixtures of copulas aiming at using them for clustering with respect to the dependence properties of the data. This is accomplished by considering finite mixture of different copulas to represent different dependence structures. We provide properties of the derived models along with the description of an estimation method using an EM algorithm based on the standard approach for mixture models. Using daily returns from major stock markets, we illustrate the potential of our method.},
  citeulike-article-id = {14151134},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/03610918.2012.752832},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/03610918.2012.752832},
  day                  = {1},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-03 20:23:53},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Arbelaitz-et-al-2013,
  author               = {Arbelaitz, Olatz and Gurrutxaga, Ibai and Muguerza, Javier and Perez, Jesus M. and Perona, Inigo},
  date                 = {2013-01},
  journaltitle         = {Pattern Recognition},
  title                = {An extensive comparative study of cluster validity indices},
  doi                  = {10.1016/j.patcog.2012.07.021},
  issn                 = {0031-3203},
  number               = {1},
  pages                = {243--256},
  volume               = {46},
  abstract             = {The validation of the results obtained by clustering algorithms is a fundamental part of the clustering process. The most used approaches for cluster validation are based on internal cluster validity indices. Although many indices have been proposed, there is no recent extensive comparative study of their performance. In this paper we show the results of an experimental work that compares 30 cluster validity indices in many different environments with different characteristics. These results can serve as a guideline for selecting the most suitable index for each possible application and provide a deep insight into the performance differences between the currently available indices. We compare 30 cluster validity indices (CVIs) in 720 synthetic and 20 real datasets. We use a new comparison methodology and three clustering algorithms: k-means, Ward and Average-linkage. The CVI performance drops dramatically when noise is present or clusters overlap. Statistical tests suggest a division of three groups of CVIs.},
  citeulike-article-id = {11208386},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2012.07.021},
  posted-at            = {2017-11-22 22:50:31},
  timestamp            = {2020-02-27 03:51},
}

@Article{Augustynski-LaskosGrabowski-2018,
  author         = {Augustynski, Iwo and Laskos-Grabowski, Pawel},
  date           = {2018},
  journaltitle   = {Econometrics},
  title          = {Clustering Macroeconomic Time Series},
  doi            = {10.15611/eada.2018.2.06},
  issn           = {1507-3866},
  number         = {2},
  pages          = {74--88},
  volume         = {22},
  abstract       = {The data mining technique of time series clustering is well established in many fields. However, as an unsupervised learning method, it requires making choices that are nontrivially influenced by the nature of the data involved. The aim of this paper is to verify usefulness of the time series clustering method for macroeconomics research, and to develop the most suitable methodology. By extensively testing various possibilities, we arrive at a choice of a dissimilarity measure (compression-based dissimilarity measure, or CDM) which is particularly suitable for clustering macroeconomic variables. We check that the results are stable in time and reflect large-scale phenomena such as crises. We also successfully apply our findings to analysis of national economies, specifically to identifying their structural relations.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network, ML_ClustTimeSrs},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bacidore-et-al-2018,
  author         = {Bacidore, Jeff and Berkow, Kathryn and Polidore, Ben and Saraiya, Nigam},
  date           = {2018-10-31},
  journaltitle   = {The Journal of Trading},
  title          = {Cluster Analysis for Evaluating Trading Strategies},
  doi            = {10.3905/jot.2012.7.3.006},
  url            = {https://jot.pm-research.com/content/7/3/6},
  abstract       = {In this article, we introduce a new methodology to empirically identify the primary strategies used by a trader using only post-trade fill data. To do this, we apply a well-established statistical clustering technique called k-means to a sample of progress charts, representing the portion of the order completed by each point in the day as a measure of a trade aggressiveness. Our methodology identifies the primary strategies used by a trader and determines which strategy the trader used for each order in the sample. Having identified the strategy used for each order, trading cost analysis can be performed by strategy. We also discuss ways to exploit this technique to characterize trader behavior, assess trader performance, and suggest the appropriate benchmarks for each distinct trading strategy.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-27 03:51},
}

@Article{Balzer-et-al-2018,
  author         = {Balzer, Laura B. and Zheng, Wenjing and van der Laan, Mark J. and Petersen, Maya L.},
  date           = {2018-01-01},
  journaltitle   = {Statistical Methods in Medical Research},
  title          = {A new approach to hierarchical data analysis: Targeted maximum likelihood estimation for the causal effect of a cluster-level exposure.},
  doi            = {10.1177/0962280218774936},
  pages          = {962280218774936},
  abstract       = {We often seek to estimate the impact of an exposure naturally occurring or randomly assigned at the cluster-level. For example, the literature on neighborhood determinants of health continues to grow. Likewise, community randomized trials are applied to learn about real-world implementation, sustainability, and population effects of interventions with proven individual-level efficacy. In these settings, individual-level outcomes are correlated due to shared cluster-level factors, including the exposure, as well as social or biological interactions between individuals. To flexibly and efficiently estimate the effect of a cluster-level exposure, we present two targeted maximum likelihood estimators (TMLEs). The first TMLE is developed under a non-parametric causal model, which allows for arbitrary interactions between individuals within a cluster. These interactions include direct transmission of the outcome (i.e. contagion) and influence of one individual's covariates on another's outcome (i.e. covariate interference). The second TMLE is developed under a causal sub-model assuming the cluster-level and individual-specific covariates are sufficient to control for confounding. Simulations compare the alternative estimators and illustrate the potential gains from pairing individual-level risk factors and outcomes during estimation, while avoiding unwarranted assumptions. Our results suggest that estimation under the sub-model can result in bias and misleading inference in an observational setting. Incorporating working assumptions during estimation is more robust than assuming they hold in the underlying causal model. We illustrate our approach with an application to HIV prevention and treatment.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood, Data_Explo_Analysis},
  pmid           = {29921160},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bandara-et-al-2019,
  author         = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
  date           = {2019-10-09},
  journaltitle   = {Expert Systems with Applications},
  title          = {Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach},
  url            = {https://www.sciencedirect.com/science/article/pii/S0957417419306128},
  urldate        = {2019-03-07},
  abstract       = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context when trained across all available time series. However, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model that can be used with different types of RNN models on subgroups of similar time series, which are identified by time series clustering techniques. We assess our proposed methodology using LSTM networks, a widely popular RNN variant. Our method achieves competitive results on benchmarking datasets under competition evaluation procedures. In particular, in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM model and outperforms all other methods on the CIF2016 forecasting competition dataset.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Basalto-et-al-2007,
  author               = {Basalto, Nicolas and Bellotti, Roberto and De Carlo, Francesco and Facchi, Paolo and Pantaleo, Ester and Pascazio, Saverio},
  date                 = {2007-06},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Hausdorff clustering of financial time series},
  doi                  = {10.1016/j.physa.2007.01.011},
  issn                 = {0378-4371},
  number               = {2},
  pages                = {635--644},
  volume               = {379},
  abstract             = {A clustering procedure is introduced based on the Hausdorff distance as a similarity measure between clusters of elements. The method is applied to the financial time series of the Dow Jones industrial average (DJIA) index to find companies that share a similar behavior. Comparisons are made with other linkage algorithms.},
  citeulike-article-id = {14148581},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2007.01.011},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:44:58},
  timestamp            = {2020-02-27 03:51},
}

@Article{Bastos-Caiado-2014,
  author               = {Bastos, Joao A. and Caiado, Jorge},
  date                 = {2014-12},
  journaltitle         = {Quantitative Finance},
  title                = {Clustering financial time series with variance ratio statistics},
  doi                  = {10.1080/14697688.2012.726736},
  number               = {12},
  pages                = {2121--2133},
  volume               = {14},
  abstract             = {This study introduces a new distance measure for clustering financial time series based on variance ratio test statistics. The proposed metric attempts to assess the level of interdependence of time series from the point of view of return predictability. Simulation results show that this metric aggregates time series according to their serial dependence structure better than a metric based on the sample autocorrelations. An empirical application of this approach to international stock market returns is presented. The results suggest that this metric discriminates stock markets reasonably well according to size and the level of development. Furthermore, despite the substantial evolution of individual variance ratio statistics, the clustering pattern remains fairly stable across different time periods.},
  citeulike-article-id = {14316690},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.726736},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.726736},
  day                  = {2},
  groups               = {Networks and investment management},
  posted-at            = {2017-03-23 08:25:58},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 03:51},
}

@InProceedings{Begusic-Kostanjcar-2019,
  author         = {Begusic, Stjepan and Kostanjcar, Zvonko},
  booktitle      = {11th International Symposium on Image and Signal Processing and Analysis (ISPA)},
  date           = {2019-09-23},
  title          = {Cluster-Based Shrinkage of Correlation Matrices for Portfolio Optimization},
  doi            = {10.1109/{ISPA}.2019.8868482},
  isbn           = {978-1-7281-3140-5},
  pages          = {301--305},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8868482/},
  urldate        = {2020-01-13},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Berthold-Hoppner-2016,
  author               = {Berthold, Michael R. and Hoppner, Frank},
  date                 = {2016-01-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {On Clustering Time Series Using Euclidean Distance and Pearson Correlation},
  eprint               = {1601.02213},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1601.02213},
  abstract             = {For time series comparisons, it has often been observed that z-score normalized Euclidean distances far outperform the unnormalized variant. In this paper we show that a z-score normalized, squared Euclidean Distance is, in fact, equal to a distance based on Pearson Correlation. This has profound impact on many distance-based classification or clustering methods. In addition to this theoretically sound result we also show that the often used k-Means algorithm formally needs a mod ification to keep the interpretation as Pearson correlation strictly valid. Experimental results demonstrate that in many cases the standard k-Means algorithm generally produces the same results.},
  citeulike-article-id = {13904331},
  citeulike-linkout-0  = {http://arxiv.org/abs/1601.02213},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1601.02213},
  day                  = {10},
  posted-at            = {2017-11-23 21:43:08},
  timestamp            = {2020-02-27 03:51},
}

@Article{Bien-Tibshirani-2011,
  author               = {Bien, Jacob and Tibshirani, Robert},
  date                 = {2011-09},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Hierarchical Clustering With Prototypes via Minimax Linkage},
  doi                  = {10.1198/jasa.2011.tm10183},
  number               = {495},
  pages                = {1075--1084},
  volume               = {106},
  abstract             = {Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage, that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces.},
  citeulike-article-id = {14148576},
  citeulike-linkout-0  = {http://dx.doi.org/10.1198/jasa.2011.tm10183},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10183},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:30:51},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Biswas-Biswas-2017,
  author               = {Biswas, Anupam and Biswas, Bhaskar},
  date                 = {2017-04},
  journaltitle         = {Expert Systems with Applications},
  title                = {Defining quality metrics for graph clustering evaluation},
  doi                  = {10.1016/j.eswa.2016.11.011},
  issn                 = {0957-4174},
  pages                = {1--17},
  volume               = {71},
  abstract             = {Evaluation of clustering has significant importance in various applications of expert and intelligent systems. Clusters are evaluated in terms of quality and accuracy. Measuring quality is a unsupervised approach that completely depends on edges, whereas measuring accuracy is a supervised approach that measures similarity between the real clustering and the predicted clustering. Accuracy cannot be measured for most of the real-world networks since real clustering is unavailable. Thus, it will be advantageous from the viewpoint of expert systems to develop a quality metric that can assure certain level of accuracy along with the quality of clustering. In this paper we have proposed a set of three quality metrics for graph clustering that have the ability to ensure accuracy along with the quality. The effectiveness of the metrics has been evaluated on benchmark graphs as well as on real-world networks and compared with existing metrics. Results indicate competency of the suggested metrics while dealing with accuracy, which will definitely improve the decision-making in expert and intelligent systems. We have also shown that our metrics satisfy all of the six quality-related properties.},
  citeulike-article-id = {14447477},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2016.11.011},
  posted-at            = {2017-10-08 14:20:33},
  timestamp            = {2020-02-27 03:51},
}

@Article{Blau-2019,
  author         = {Blau, Benjamin M.},
  date           = {2019-01-02},
  journaltitle   = {Journal of Behavioral Finance},
  title          = {Price clustering and investor sentiment},
  doi            = {10.1080/15427560.2018.1431887},
  issn           = {1542-7560},
  number         = {1},
  pages          = {19--30},
  urldate        = {2019-09-01},
  volume         = {20},
  abstract       = {Among the anomalous findings in the finance literature, perhaps the most persistent is the finding that security prices tend to cluster on round pricing increments. The author examines how investor sentiment influences the degree of price clustering. Both univariate and multivariate tests show a contemporaneous correlation between price clustering and investor sentiment. Recognizing the need to make stronger causal inferences, the author conducts 2 additional sets of tests. First, the author uses the technology bubble period as natural experiment and examine the price clustering of technology vis-a-vis nontechnology stocks. Results show that price clustering is markedly higher in tech stocks than in nontech stocks during this period of rising, sector-specific, investor sentiment. Second, the author estimates a vector autoregression process and examines the impulse responses of price clustering to exogenous shocks in investor sentiment. The results from these tests indicate that causation flows from sentiment to clustering instead of the other way around.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bnouachir-Mkhadri-2019,
  author         = {Bnouachir, Najla and Mkhadri, Abdallah},
  date           = {2019-06-03},
  journaltitle   = {Communications in Statistics - Simulation and Computation},
  title          = {Efficient cluster-based portfolio optimization},
  doi            = {10.1080/03610918.2019.1621341},
  issn           = {0361-0918},
  pages          = {1--15},
  urldate        = {2020-01-13},
  abstract       = {The sample mean and covariance matrix of historical data provide a disappointing out-of-sample performance in mean-variance portfolio rules. This poor performance is certainly due to the high estimation error incurred in the optimization model. Our purpose in this article is to find a method that enhances the out-of-sample performance of the portfolio weights. Using hierarchical clustering, we propose an alternative cluster-based portfolio to obtain a sequence of cluster assets. On the basis of Gram-Schmidt orthogonalization, the estimation risk of the data set becomes the sum of the estimations of the clusters in the sequence. The performance of our method and its competitors is compared empirically and via some simulations in high dimension.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bulo-Pelillo-2017,
  author               = {Bulo, Samuel Rota and Pelillo, Marcello},
  date                 = {2017-10},
  journaltitle         = {European Journal of Operational Research},
  title                = {Dominant-set clustering: A review},
  doi                  = {10.1016/j.ejor.2017.03.056},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {1--13},
  volume               = {262},
  abstract             = {Clustering refers to the process of extracting maximally coherent groups from a set of objects using pairwise, or high-order, similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a predetermined number of classes, thereby obtaining the clusters as a by-product of the partitioning process. A radically different perspective of the problem consists in providing a formalization of the very notion of a cluster and considering the clustering process as a sequential search of structures in the data adhering to this cluster notion. In this manuscript we review one of the pioneering approaches falling in the latter class of algorithms, which has been proposed in the early 2000s and has been found since then a number of applications in different domains. It is known as dominant set clustering and provides a notion of a cluster (a.k.a. dominant set) that has intriguing links to game-theory, graph-theory and optimization theory. From the game-theoretic perspective, clusters are regarded as equilibria of non-cooperative "clustering" games; in the graph-theoretic context, it can be shown that they generalize the notion of maximal clique to edge-weighted graphs; finally, from an optimization point of view, they can be characterized in terms of solutions to a simplex-constrained, quadratic optimization problem, as well as in terms of an exquisitely combinatorial entity. Besides introducing dominant sets from a theoretical perspective, we will also focus on the related algorithmic issues by reviewing two state-of-the-art methods that are used in the literature to find dominant sets clusters, namely the Replicator Dynamics and the Infection and Immunization Dynamics. Finally, we conclude with an overview of different extensions of the dominant set framework and of applications where dominant sets have been successfully employed.},
  citeulike-article-id = {14500761},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.03.056},
  posted-at            = {2017-12-11 09:13:21},
  timestamp            = {2020-02-27 03:51},
}

@Article{Cai-et-al-2016,
  author               = {Cai, Fan and Le-Khac, Nhien-An and Kechadi, Tahar},
  date                 = {2016-09},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Clustering Approaches for Financial Data Analysis: a Survey},
  eprint               = {1609.08520},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1609.08520},
  abstract             = {Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, confidence of expected return, etc. Banking and financial institutes have applied different data mining techniques to enhance their business performance. Among these techniques, clustering has been considered as a significant method to capture the natural structure of data. However, there are not many studies on clustering approaches for financial data analysis. In this paper, we evaluate different clustering algorithms for analysing different financial datasets varied from time series to transactions. We also discuss the advantages and disadvantages of each method to enhance the understanding of inner structure of financial datasets as well as the capability of each clustering method in this context.},
  citeulike-article-id = {14148629},
  citeulike-linkout-0  = {http://arxiv.org/abs/1609.08520},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1609.08520},
  day                  = {4},
  groups               = {Networks and investment management, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 21:25:38},
  timestamp            = {2020-02-27 03:51},
}

@Article{Cai-et-al-2017,
  author               = {Cai, Yumei and Cui, Xiaomei and Huang, Qianyun and Sun, Jianqiang},
  date                 = {2017-09},
  journaltitle         = {International Review of Economics \& Finance},
  title                = {Hierarchy, cluster, and time-stable information structure of correlations between international financial markets},
  doi                  = {10.1016/j.iref.2017.07.024},
  issn                 = {1059-0560},
  pages                = {562--573},
  volume               = {51},
  abstract             = {This paper investigates the correlations between 52 financial markets located in different countries or regions from July 2004 through June 2011. By using a correlation matrix time series and a participation frequency method based on the random matrix theory, we show that a time-stable information structure is contained in the correlations between global financial markets. We further find that the information structure is closely associated with global market and global geographical factors, and that each financial index's participation in the global market factor varies over time and presents dynamics. Two patterns, hierarchy and cluster effects, are found to be in the dynamics of the indices' participation in the global market factor. The cluster effect implies a more concentrated participation during the 2008 financial crisis.},
  citeulike-article-id = {14429814},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.iref.2017.07.024},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-09-13 09:33:44},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2016,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2016-07},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Excisive Hierarchical Clustering Methods for Network Data},
  eprint               = {1607.06339},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1607.06339},
  abstract             = {We introduce two practical properties of hierarchical clustering methods for (possibly asymmetric) network data: excisiveness and linear scale preservation. The latter enforces imperviousness to change in units of measure whereas the former ensures local consistency of the clustering outcome. Algorithmically, excisiveness implies that we can reduce computational complexity by only clustering a data subset of interest while theoretically guaranteeing that the same hierarchical outcome would be observed when clustering the whole dataset. Moreover, we introduce the concept of representability, i.e. a generative model for describing clustering methods through the specification of their action on a collection of networks. We further show that, within a rich set of admissible methods, requiring representability is equivalent to requiring both excisiveness and linear scale preservation. Leveraging this equivalence, we show that all excisive and linear scale preserving methods can be factored into two steps: a transformation of the weights in the input network followed by the application of a canonical clustering method. Furthermore, their factorization can be used to show stability of excisive and linear scale preserving methods in the sense that a bounded perturbation in the input network entails a bounded perturbation in the clustering output.},
  citeulike-article-id = {14357923},
  citeulike-linkout-0  = {http://arxiv.org/abs/1607.06339},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1607.06339},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:04:03},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2016a,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2016-07},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Hierarchical Clustering of Asymmetric Networks},
  eprint               = {1607.06294},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1607.06294},
  abstract             = {This paper considers networks where relationships between nodes are represented by directed dissimilarities. The goal is to study methods that, based on the dissimilarity structure, output hierarchical clusters, i.e., a family of nested partitions indexed by a connectivity parameter. Our construction of hierarchical clustering methods is built around the concept of admissible methods, which are those that abide by the axioms of value - nodes in a network with two nodes are clustered together at the maximum of the two dissimilarities between them - and transformation - when dissimilarities are reduced, the network may become more clustered but not less. Two particular methods, termed reciprocal and nonreciprocal clustering, are shown to provide upper and lower bounds in the space of admissible methods. Furthermore, alternative clustering methodologies and axioms are considered. In particular, modifying the axiom of value such that clustering in two-node networks occurs at the minimum of the two dissimilarities entails the existence of a unique admissible clustering method.},
  citeulike-article-id = {14357925},
  citeulike-linkout-0  = {http://arxiv.org/abs/1607.06294},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1607.06294},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:05:05},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2017,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2017},
  journaltitle         = {IEEE Transactions on Signal and Information Processing over Networks},
  title                = {Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric Networks},
  doi                  = {10.1109/tsipn.2017.2662622},
  issn                 = {2373-776X},
  pages                = {1},
  abstract             = {This paper characterizes hierarchical clustering methods that abide by two previously introduced axioms - thus, denominated admissible methods - and proposes tractable algorithms for their implementation. We leverage the fact that, for asymmetric networks, every admissible method must be contained between reciprocal and nonreciprocal clustering, and describe three families of intermediate methods. Grafting methods exchange branches between dendrograms generated by different admissible methods. The convex combination family combines admissible methods through a convex operation in the space of dendrograms, and thirdly, the semi-reciprocal family clusters nodes that are related by strong cyclic influences in the network. An algorithmic framework for the computation of hierarchical clusters generated by reciprocal and nonreciprocal clustering as well as the grafting, convex combination, and semi-reciprocal families is presented via matrix operations in a dioid algebra. Finally, the introduced clustering methods and algorithms are exemplified through their application to a network describing the interrelation between sectors of the United States (U.S.) economy.},
  citeulike-article-id = {14357921},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tsipn.2017.2662622},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:02:52},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chamroukhi-et-al-2013,
  author               = {Chamroukhi, Faicel and Same, Allou and Aknin, Patrice and Govaert, Gerard},
  date                 = {2013-12-25},
  journaltitle         = {Proceedings of the 2011 International Joint Conference on Neural Networks (IJCNN)},
  title                = {Model-based clustering with Hidden Markov Model regression for time series with regime changes},
  doi                  = {10.1109/ijcnn.2011.6033590},
  pages                = {2814--2821},
  abstract             = {This paper introduces a novel model-based clustering approach for clustering time series which present changes in regime. It consists of a mixture of polynomial regressions governed by hidden Markov chains. The underlying hidden process for each cluster activates successively several polynomial regimes during time. The parameter estimation is performed by the maximum likelihood method through a dedicated Expectation-Maximization (EM) algorithm. The proposed approach is evaluated using simulated time series and real-world time series issued from a railway diagnosis application. Comparisons with existing approaches for time series clustering, including the stand EM for Gaussian mixtures, K-means clustering, the standard mixture of regression models and mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed approach.},
  booktitle            = {The 2011 International Joint Conference on Neural Networks},
  citeulike-article-id = {14510865},
  citeulike-linkout-0  = {http://arxiv.org/abs/1312.7024},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1312.7024},
  citeulike-linkout-2  = {http://dx.doi.org/10.1109/ijcnn.2011.6033590},
  day                  = {25},
  isbn                 = {978-1-4244-9635-8},
  location             = {San Jose, CA, USA},
  posted-at            = {2018-01-02 02:30:30},
  publisher            = {IEEE},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chamroukhi-Nguyen-2019,
  author         = {Chamroukhi, Faicel and Nguyen, Hien D.},
  date           = {2019-01-18},
  journaltitle   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  title          = {Model-based clustering and classification of functional data},
  doi            = {10.1002/widm.1298},
  issn           = {1942-4787},
  pages          = {e1298},
  urldate        = {2019-10-09},
  abstract       = {The problem of complex data analysis is a central topic of modern statistical science and learning systems and is becoming of broader interest with the increasing prevalence of high-dimensional data. The challenge is to develop statistical models and autonomous algorithms that are able to acquire knowledge from raw data for exploratory analysis, which can be achieved through clustering techniques or to make predictions of future data via classification (i.e., discriminant analysis) techniques. Latent data models, including mixture model-based approaches are one of the most popular and successful approaches in both the unsupervised context (i.e., clustering) and the supervised one (i.e, classification or discrimination). Although traditionally tools of multivariate analysis, they are growing in popularity when considered in the framework of functional data analysis (FDA). FDA is the data analysis paradigm in which the individual data units are functions (e.g., curves, surfaces), rather than simple vectors. In many areas of application, the analyzed data are indeed often available in the form of discretized values of functions or curves (e.g., time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data). This functional aspect of the data adds additional difficulties compared to the case of a classical multivariate (non-functional) data analysis. We review and present approaches for model-based clustering and classification of functional data. We derive well-established statistical models along with efficient algorithmic tools to address problems regarding the clustering and the classification of these high-dimensional data, including their heterogeneity, missing information, and dynamical hidden structure. The presented models and algorithms are illustrated on real-world functional data analysis problems from several application area.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs},
  timestamp      = {2020-02-27 03:51},
}

@Article{Charrad-et-al-2014,
  author               = {Charrad, Malika and Ghazzali, Nadia and Boiteau, Veronique and Niknafs, Azam},
  date                 = {2014},
  journaltitle         = {Journal of Statistical Software},
  title                = {NbClust: An R Package for Determining theRelevant Number of Clusters in a Data Set},
  abstract             = {Clustering is the partitioning of a set of objects into groups (clusters) so that objects within a group are more similar to each others than objects in dierent groups. Most of the clustering algorithms depend on some assumptions in order to dene the subgroups present in a data set. As a consequence, the resulting clustering scheme requires some sort of evaluation as regards its validity. The evaluation procedure has to tackle dicult problems such as the quality of clusters, the degree with which a clustering scheme ts a specic data set and the optimal number of clusters in a partitioning. In the literature, a wide variety of indices have been proposed to nd the optimal number of clusters in a partitioning of a data set during the clustering process. However, for most of indices proposed in the literature, programs are unavailable to test these indices and compare them. The R package NbClust has been developed for that purpose. It provides 30 indices which determine the number of clusters in a data set and it oers also the best clus- tering scheme from dierent results to the user. In addition, it provides a function to perform k-means and hierarchical clustering with dierent distance measures and aggre- gation methods. Any combination of validation indices and clustering methods can be requested in a single function call. This enables the user to simultaneously evaluate sev- eral clustering schemes while varying the number of clusters, to help determining the most appropriate number of clusters for the data set of interest.},
  citeulike-article-id = {14468583},
  posted-at            = {2017-10-29 20:23:51},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chatziafratis-et-al-2018,
  author         = {Chatziafratis, Vaggos and Niazadeh, Rad and Charikar, Moses},
  date           = {2018-07-03},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Hierarchical Clustering with Structural Constraints},
  url            = {http://proceedings.mlr.press/v80/chatziafratis18a.html},
  urldate        = {2019-09-15},
  abstract       = {Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraints. Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output. In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information (Dasgupta, 2016). We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective. Furthemore, we explore a variation of this objective for dissimilarity information (Cohen-Addad et al., 2018) and improve upon current techniques. Finally, we demonstrate our approach on a real dataset for the taxonomy application.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Clemente-Grassi-2018,
  author               = {Clemente, Gian P. and Grassi, Rosanna},
  date                 = {2018-06-19},
  journaltitle         = {Chaos, Solitons \& Fractals},
  title                = {Directed clustering in weighted networks: a new perspective},
  doi                  = {10.1016/j.chaos.2017.12.007},
  eprint               = {1706.07322},
  eprinttype           = {arXiv},
  issn                 = {0960-0779},
  pages                = {26--38},
  volume               = {107},
  abstract             = {In this paper, we consider the problem of assessing local clustering in complex networks. Various definitions for this measure have been proposed for the cases of networks having weighted edges, but less attention has been paid to both weighted and directed networks. We provide a new local clustering coefficient for this kind of networks, starting from those existing in the literature for the weighted and undirected case. Furthermore, we extract from our coefficient four specific components, in order to separately consider different link patterns of triangles. Empirical applications on several real networks from different frameworks and with different order are provided. The performance of our coefficient is also compared with that of existing coefficients in the literature.},
  citeulike-article-id = {14515534},
  citeulike-linkout-0  = {http://arxiv.org/abs/1706.07322},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1706.07322},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.chaos.2017.12.007},
  day                  = {19},
  posted-at            = {2018-01-11 22:01:47},
  timestamp            = {2020-02-27 03:51},
}

@Article{Crawford-Milenkovic-2018,
  author         = {Crawford, Joseph and Milenkovic, Tijana},
  date           = {2018-05-08},
  journaltitle   = {PLOS ONE},
  title          = {ClueNet: Clustering a temporal network based on topological similarity rather than denseness.},
  doi            = {10.1371/journal.pone.0195993},
  number         = {5},
  pages          = {e0195993},
  volume         = {13},
  abstract       = {Network clustering is a very popular topic in the network science field. Its goal is to divide (partition) the network into groups (clusters or communities) of "topologically related" nodes, where the resulting topology-based clusters are expected to "correlate" well with node label information, i.e., metadata, such as cellular functions of genes/proteins in biological networks, or age or gender of people in social networks. Even for static data, the problem of network clustering is complex. For dynamic data, the problem is even more complex, due to an additional dimension of the data-their temporal (evolving) nature. Since the problem is computationally intractable, heuristic approaches need to be sought. Existing approaches for dynamic network clustering (DNC) have drawbacks. First, they assume that nodes should be in the same cluster if they are densely interconnected within the network. We hypothesize that in some applications, it might be of interest to cluster nodes that are topologically similar to each other instead of or in addition to requiring the nodes to be densely interconnected. Second, they ignore temporal information in their early steps, and when they do consider this information later on, they do so implicitly. We hypothesize that capturing temporal information earlier in the clustering process and doing so explicitly will improve results. We test these two hypotheses via our new approach called ClueNet. We evaluate ClueNet against six existing DNC methods on both social networks capturing evolving interactions between individuals (such as interactions between students in a high school) and biological networks capturing interactions between biomolecules in the cell at different ages. We find that ClueNet is superior in over 83\% of all evaluation tests. As more real-world dynamic data are becoming available, DNC and thus ClueNet will only continue to gain importance.},
  day            = {8},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5940177},
  pmid           = {29738568},
  timestamp      = {2020-02-27 03:51},
}

@Article{Dantas-Oliveira-2018,
  author         = {Dantas, Tiago Mendes and Oliveira, Fernando Luiz Cyrino},
  date           = {2018-10},
  journaltitle   = {International Journal of Forecasting},
  title          = {Improving time series forecasting: An approach combining bootstrap aggregation, clusters and exponential smoothing},
  doi            = {10.1016/j.ijforecast.2018.05.006},
  issn           = {0169-2070},
  number         = {4},
  pages          = {748--761},
  volume         = {34},
  abstract       = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged Cluster ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_TimeSeries},
  timestamp      = {2020-02-27 03:56},
}

@Article{DeLuca-Zuccolotto-2016,
  author               = {{De Luca}, Giovanni and Zuccolotto, Paola},
  date                 = {2016-01},
  journaltitle         = {Statistics and Risk Modeling},
  title                = {A double clustering algorithm for financial time series based on extreme events},
  doi                  = {10.1515/strm-2015-0026},
  issn                 = {2193-1402},
  number               = {0},
  volume               = {0},
  abstract             = {This paper is concerned with a procedure for financial time series clustering, aimed at creating groups of time series characterized by similar behavior with regard to extreme events. The core of our proposal is a double clustering procedure: the former is based on the lower tail dependence of all the possible pairs of time series, the latter on the upper tail dependence. Tail dependence coefficients are estimated with copula functions. The final goal is to exploit the two clustering solutions in an algorithm designed to create a portfolio that maximizes the probability of joint positive extreme returns while minimizing the risk of joint negative extreme returns. In financial crisis scenarios, such a portfolio is expected to outperform portfolios generated by the traditional methods. We describe the results of a simulation study and, finally, we apply the procedure to a dataset composed of the 50 assets included in the EUROSTOXX index.},
  citeulike-article-id = {14150079},
  citeulike-linkout-0  = {http://dx.doi.org/10.1515/strm-2015-0026},
  day                  = {20},
  groups               = {Networks and investment management, [nbkcbu3:]},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:20},
  timestamp            = {2020-02-27 03:56},
}

@Article{dePrado-2020a,
  author         = {{de Prado}, Marcos Lopez},
  date           = {2020},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Clustering},
  doi            = {10.2139/ssrn.3512998},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3512998},
  urldate        = {2020-01-19},
  abstract       = {Many problems in finance require the clustering of variables or observations. Despite its usefulness, clustering is almost never taught in Econometrics courses. In this seminar we review two general clustering approaches: partitional and hierarchical.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Dias-et-al-2015,
  author       = {Dias, Jose G. and Vermunt, Jeroen K. and Ramos, Sofia},
  date         = {2015},
  journaltitle = {European Journal of Operational Research},
  title        = {Clustering financial time series: New insights from an extended hidden Markov model},
  doi          = {10.1016/j.ejor.2014.12.041},
  number       = {3},
  pages        = {852--864},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S0377221714010595},
  volume       = {243},
  abstract     = {In recent years, large amounts of financial data have become available for analysis. We propose exploring returns from 21 European stock markets by model-based clustering of regime switching models. These econometric models identify clusters of time series with similar dynamic patterns and moreover allow relaxing assumptions of existing approaches, such as the assumption of conditional Gaussian returns.

The proposed model handles simultaneously the heterogeneity across stock markets and over time, i.e., time-constant and time-varying discrete latent variables capture unobserved heterogeneity between and within stock markets, respectively. The results show a clear distinction between two groups of stock markets, each one characterized by different regime switching dynamics that correspond to different expected return-risk patterns.

We identify three regimes: the so-called bull and bear regimes, as well as a stable regime with returns close to 0, which turns out to be the most frequently occurring regime. This is consistent with stylized facts in financial econometrics.},
  groups       = {Networks and investment management, Clustering and network analysis, Scenario_TimeSeries},
  keywords     = {Data mining; Hidden Markov model; Stock indexes; Latent class model; Regime-switching model;},
  owner        = {zkgst0c},
  timestamp    = {2020-02-27 03:56},
}

@Article{DiLascio-et-al-2018,
  author         = {Di Lascio, F. Marta L. and Giammusso, Davide and Puccetti, Giovanni},
  date           = {2018-11},
  journaltitle   = {Journal of banking \& finance},
  title          = {A clustering approach and a rule of thumb for risk aggregation},
  doi            = {10.1016/j.jbankfin.2018.07.002},
  issn           = {0378-4266},
  pages          = {236--248},
  volume         = {96},
  abstract       = {Abstract The problem of establishing reliable estimates or bounds for the (T)VaR of a joint risk portfolio is a relevant subject in connection with the computation of total economic capital in the Basel regulatory framework for the finance sector as well as with the Solvency regulations for the insurance sector. In the computation of total economic capital, a financial institution faces a considerable amount of model uncertainty related to the estimation of the interdependence amongst the marginal risks. In this paper, we propose to apply a clustering procedure in order to partition a risk portfolio into independent subgroups of positively dependent risks. Based on available data, the portfolio partition so obtained can be statistically validated and allows for a reduction of capital and the corresponding model uncertainty. We illustrate the proposed methodology in a simulation study and two case studies considering an Operational and a Market Risk portfolio. A rule of thumb stems from the various examples proposed: in a mathematical model where the risk portfolio is split into independent subsets with comonotonic dependence within, the smallest VaR-based capital estimate (at the high regulatory probability levels typically used) is produced by assuming that the infinite-mean risks are comonotonic and the finite-mean risks are independent. The largest VaR estimate is instead generated by obtaining the maximum number of independent infinite-mean sums.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Ding-et-al-2017b,
  author         = {Ding, Jiajun and He, Xiongxiong and Yuan, Junqing and Jiang, Bo},
  date           = {2017-08-02},
  journaltitle   = {Soft Computing},
  title          = {Automatic clustering based on density peak detection using generalized extreme value distribution},
  doi            = {10.1007/s00500-017-2748-7},
  issn           = {1432-7643},
  number         = {9},
  pages          = {1--20},
  volume         = {22},
  abstract       = {Density peaks clustering (DPC) algorithm is able to get a satisfactory result with the help of artificial selecting the clustering centers, but such selection can be hard for a large amount of clustering tasks or the data set with a complex decision diagram. The purpose of this paper is to propose an automatic clustering approach without human intervention. Inspired by the visual selection rule of DPC, the judgment index which equals the lower value within density and distance (after normalization) is proposed for selecting the clustering centers. The judgment index approximately follows the generalized extreme value (GEV) distribution, and each clustering center judgment index is much higher. Hence, it is reasonable that the points are selected as clustering centers if their judgment indices are larger than the upper quantile of GEV. This proposed method is called density peaks clustering based on generalized extreme value distribution (DPC-GEV). Furthermore, taking the computational complexity into account, an alternative method based on density peak detection using Chebyshev inequality (DPC-CI) is also given. Experiments on both synthetic and real-world data sets show that DPC-GEV and DPC-CI can achieve the same accuracy as DPC on most data sets but consume much less time.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Durante-et-al-2013,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2013-12},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Clustering of financial time series in risky scenarios},
  doi                  = {10.1007/s11634-013-0160-4},
  issn                 = {1862-5347},
  number               = {4},
  pages                = {359--376},
  volume               = {8},
  abstract             = {A methodology is presented for clustering financial time series according to the association in the tail of their distribution. The procedure is based on the calculation of suitable pairwise conditional Spearman's correlation coefficients extracted from the series. The performance of the method has been tested via a simulation study. As an illustration, an analysis of the components of the Italian FTSE-MIB is presented. The results could be applied to construct financial portfolios that can manage to reduce the risk in case of simultaneous large losses in several markets.},
  citeulike-article-id = {14150074},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-013-0160-4},
  day                  = {22},
  groups               = {Networks and investment management, Scenario generation, Scenario_Market, Scenario_TimeSeries},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:04},
  timestamp            = {2020-02-27 03:56},
}

@Article{Durante-et-al-2015,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2015},
  journaltitle         = {Statistical Papers},
  title                = {Clustering of time series via non-parametric tail dependence estimation},
  doi                  = {10.1007/s00362-014-0605-7},
  number               = {3},
  pages                = {701--721},
  volume               = {56},
  abstract             = {We present a procedure for clustering time series according to their tail dependence behaviour as measured via a suitable copula-based tail coefficient, estimated in a non-parametric way. Simulation results about the proposed methodology together with an application to financial data are presented showing the usefulness of the proposed approach.},
  citeulike-article-id = {14150076},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00362-014-0605-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00362-014-0605-7},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:35},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Durante-Pappada-2015,
  author               = {Durante, Fabrizio and Pappada, Roberta},
  booktitle            = {Strengthening Links Between Data Analysis and Soft Computing},
  date                 = {2015},
  title                = {Cluster Analysis of Time Series via Kendall Distribution},
  doi                  = {10.1007/978-3-319-10765-3\_25},
  editor               = {Grzegorzewski, Przemyslaw and Gagolewski, Marek and Hryniewicz, Olgierd and Gil, Mara},
  pages                = {209--216},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {315},
  abstract             = {We present a method to cluster time series according to the calculation of the pairwise Kendall distribution function between them. A case study with environmental data illustrates the introduced methodology.},
  citeulike-article-id = {14150077},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-10765-325},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-10765-325},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:46:12},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Durstewitz-2017e,
  author         = {Durstewitz, Daniel},
  booktitle      = {Advanced data analysis in neuroscience},
  date           = {2017},
  title          = {Clustering and density estimation},
  doi            = {10.1007/978-3-319-59976-2\_5},
  isbn           = {978-3-319-59974-8},
  pages          = {85--103},
  publisher      = {Springer International Publishing},
  series         = {Bernstein series in computational neuroscience},
  abstract       = {In classification approaches as described in Chap. 3, we have a training sample X with known class labels C, and we use this information either to estimate the conditional probabilities p(C = k), or to set up class boundaries (decision surfaces) by some other more direct criterion. In clustering we likewise assume that there is some underlying class structure in the data, just that we don know it and have no access to class labels C for our sample X, so that we have to infer it from X alone. This is also called an unsupervised statistical learning problem. In neurobiology this problem frequently occurs, for instance, when we suspect that neural cells in a brain area from their morphological and/or electrophysiological characteristics into different types, when gene sets cluster in functional pathways, when we believe that neural spiking patterns generated spontaneously in a given area are not arranged along a continuum but come from discrete categories (as possibly indicative of an attractor dynamics, see Chap. 9), or when rodents appear to utilize a discrete set of behavioral patterns or response strategies. In many such circumstances, we may feel that similarities between observations (observed feature sets) speak for an underlying mechanism that produces discrete types, but how could we extract such apparent structure and characterize it more formally? In fact, we may not just search for one such specific partition but may aim for a hierarchically nested set of partitions, that is, classes may split into subclasses and so on, as is the case with many natural categories and biological taxonomies. For instance, at a superordinate level, we may group cortical cell types into pyramidal cells and interneurons, which then in turn would split into several subclasses (like fast-spiking, bursting, etc.).},
  f1000-projects = {QuantInvest},
  issn           = {2520-{159X}},
  timestamp      = {2020-02-27 03:56},
}

@Article{Farrokhnia-Karimi-2016,
  author               = {Farrokhnia, Maryam and Karimi, Sadegh},
  date                 = {2016-01},
  journaltitle         = {Analytica Chimica Acta},
  title                = {Variable selection in multivariate calibration based on clustering of variable concept},
  doi                  = {10.1016/j.aca.2015.11.002},
  issn                 = {0003-2670},
  pages                = {70--81},
  volume               = {902},
  abstract             = {A new and efficient variable selection based on clustering of variable concept has been suggested for PLS. Selection the most useful variable is simple and straightforward. CLoVA concept can be used as alternative instead of using interval based variable selections for PLS. Analyses of different data sets indicate the superiority of CLoVA respect to available variable selection algorithms. Recently we have proposed a new variable selection algorithm, based on clustering of variable concept (CLoVA) in classification problem. With the same idea, this new concept has been applied to a regression problem and then the obtained results have been compared with conventional variable selection strategies for PLS. The basic idea behind the clustering of variable is that, the instrument channels are clustered into different clusters via clustering algorithms. Then, the spectral data of each cluster are subjected to PLS regression. Different real data sets (Cargill corn, Biscuit dough, ACE QSAR, Soy, and Tablet) have been used to evaluate the influence of the clustering of variables on the prediction performances of PLS. Almost in the all cases, the statistical parameter especially in prediction error shows the superiority of CLoVA-PLS respect to other variable selection strategies. Finally the synergy clustering of variable (sCLoVA-PLS), which is used the combination of cluster, has been proposed as an efficient and modification of CLoVA algorithm. The obtained statistical parameter indicates that variable clustering can split useful part from redundant ones, and then based on informative cluster; stable model can be reached.},
  citeulike-article-id = {14071205},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.aca.2015.11.002},
  owner                = {zkgst0c},
  posted-at            = {2016-06-17 22:22:46},
  timestamp            = {2020-02-27 03:56},
}

@Article{Fenn-et-al-2012,
  author               = {Fenn, Daniel J. and Porter, Mason A. and Mucha, Peter J. and McDonald, Mark and Williams, Stacy and Johnson, Neil F. and Jones, Nick S.},
  date                 = {2012-10-01},
  journaltitle         = {Quantitative Finance},
  title                = {Dynamical clustering of exchange rates},
  doi                  = {10.1080/14697688.2012.668288},
  number               = {10},
  pages                = {1493--1520},
  volume               = {12},
  abstract             = {We use techniques from network science to study correlations in the foreign exchange (FX) market during the period 1991?2008. We consider an FX market network in which each node represents an exchange rate and each weighted edge represents a time-dependent correlation between the rates. To provide insights into the clustering of the exchange-rate time series, we investigate dynamic communities in the network. We show that there is a relationship between an exchange rate's functional role within the market and its position within its community and use a node-centric community analysis to track the temporal dynamics of such roles. This reveals which exchange rates dominate the market at particular times and also identifies exchange rates that experienced significant changes in market role. We also use the community dynamics to uncover major structural changes that occurred in the FX market. Our techniques are general and will be similarly useful for investigating correlations in other markets.},
  citeulike-article-id = {14460851},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.668288},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.668288},
  day                  = {1},
  posted-at            = {2017-10-19 00:29:20},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 03:56},
}

@Article{Ferraro-et-al-2019,
  author    = {Maria Brigida Ferraro and Paolo Giordani and Alessio Serafini},
  title     = {{fclust: An R Package for Fuzzy Clustering}},
  doi       = {10.32614/RJ-2019-017},
  url       = {https://doi.org/10.32614/RJ-2019-017},
  abstract  = {Fuzzy clustering methods discover fuzzy partitions where observations can be softly assigned to more than one cluster. The package fclust is a toolbox for fuzzy clustering in the R programming language. It not only implements the widely used fuzzy k-means (FkM) algorithm, but also many FkM variants. Fuzzy cluster similarity measures, cluster validity indices and cluster visualization tools are also offered. In the current version, all the functions are rewritten in the C++ language allowing their application in large-size problems. Moreover, new fuzzy relational clustering algorithms for partitioning qualitative/mixed data are provided together with an improved version of the so-called Gustafson-Kessel algorithm to avoid singularity in the cluster covariance matrices. Finally, it is now possible to automatically select the number of clusters by means of the available fuzzy cluster validity indices.},
  journal   = {{The R Journal}},
  timestamp = {2020-02-27 03:56},
  year      = {2019},
}

@Article{Ferreira-Zhao-2015,
  author               = {Ferreira, Leonardo N. and Zhao, Liang},
  date                 = {2015-08-19},
  journaltitle         = {Information Sciences},
  title                = {Time Series Clustering via Community Detection in Networks},
  doi                  = {10.1016/j.ins.2015.07.046},
  issn                 = {0020-0255},
  pages                = {227--242},
  volume               = {326},
  abstract             = {In this paper, we propose a technique for time series clustering using community detection in complex networks. Firstly, we present a method to transform a set of time series into a network using different distance functions, where each time series is represented by a vertex and the most similar ones are connected. Then, we apply community detection algorithms to identify groups of strongly connected vertices (called a community) and, consequently, identify time series clusters. Still in this paper, we make a comprehensive analysis on the influence of various combinations of time series distance functions, network generation methods and community detection techniques on clustering results. Experimental study shows that the proposed network-based approach achieves better results than various classic or up-to-date clustering techniques under consideration. Statistical tests confirm that the proposed method outperforms some classic clustering algorithms, such as k-medoids, diana, median-linkage and centroid-linkage in various data sets. Interestingly, the proposed method can effectively detect shape patterns presented in time series due to the topological structure of the underlying network constructed in the clustering process. At the same time, other techniques fail to identify such patterns. Moreover, the proposed method is robust enough to group time series presenting similar pattern but with time shifts and/or amplitude variations. In summary, the main point of the proposed method is the transformation of time series from time-space domain to topological domain. Therefore, we hope that our approach contributes not only for time series clustering, but also for general time series analysis tasks.},
  citeulike-article-id = {14042013},
  citeulike-linkout-0  = {http://arxiv.org/abs/1508.04757},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1508.04757},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.ins.2015.07.046},
  day                  = {19},
  posted-at            = {2018-01-02 02:20:04},
  timestamp            = {2020-02-27 03:56},
}

@Article{Fop-Murphy-2017,
  author         = {Fop, Michael and Murphy, Thomas Brendan},
  date           = {2017-07-02},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Variable Selection Methods for Model-based Clustering},
  url            = {https://arxiv.org/abs/1707.00306},
  abstract       = {Model-based clustering is a popular approach for clustering multivariate data which has seen applications in numerous fields. Nowadays, high-dimensional data are more and more common and the model-based clustering approach has adapted to deal with the increasing dimensionality. In particular, the development of variable selection techniques has received a lot of attention and research effort in recent years. Even for small size problems, variable selection has been advocated to facilitate the interpretation of the clustering results. This review provides a summary of the methods developed for variable selection in model-based clustering. Existing R packages implementing the different methods are indicated and illustrated in application to two data analysis examples.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Fragkiskos-Bauman-2018,
  author               = {Fragkiskos, Apollon and Bauman, Evgeny},
  date                 = {2018},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Factor Based Clustering},
  url                  = {https://ssrn.com/abstract=3089985},
  abstract             = {We propose a novel approach to cluster funds based on their factor exposures. The approach uses investment returns as input data and calculates similarity scores across funds, which are then used to form clusters. The derived clusters avoid common pitfalls that correlation based or other cluster methods fall into. They can be used as peer group alternatives to what vendors provide or to further refine existing categories that might be too obscure to make sense of. When tested against long/short equity funds, we find that we can form clusters with relatively high levels of stability across time.},
  citeulike-article-id = {14516008},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3089985},
  groups               = {Invest_Network},
  posted-at            = {2018-01-12 20:49:21},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Gagolewski-et-al-2016a,
  author               = {Gagolewski, Marek and Cena, Anna and Bartoszuk, Maciej},
  booktitle            = {Modeling Decisions for Artificial Intelligence},
  date                 = {2016},
  title                = {Hierarchical Clustering via Penalty-Based Aggregation and the Genie Approach},
  doi                  = {10.1007/978-3-319-45656-0\_16},
  editor               = {Torra, Vicenc and Narukawa, Yasuo and Navarro-Arribas, Guillermo and Yanez, Cristina},
  pages                = {191--202},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {9880},
  abstract             = {The paper discusses a generalization of the nearest centroid hierarchical clustering algorithm. A first extension deals with the incorporation of generic distance-based penalty minimizers instead of the classical aggregation by means of centroids. Due to that the presented algorithm can be applied in spaces equipped with an arbitrary dissimilarity measure (images, DNA sequences, etc.). Secondly, a correction preventing the formation of clusters of too highly unbalanced sizes is applied: just like in the recently introduced Genie approach, which extends the single linkage scheme, the new method averts a chosen inequity measure (e.g., the Gini-, de Vergottini-, or Bonferroni-index) of cluster sizes from raising above a predefined threshold. Numerous benchmarks indicate that the introduction of such a correction increases the quality of the resulting clusterings significantly.},
  citeulike-article-id = {14468579},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-45656-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-45656-016},
  posted-at            = {2017-10-29 19:59:00},
  timestamp            = {2020-02-27 03:56},
}

@Article{Gan-2013,
  author               = {Gan, Guojun},
  date                 = {2013-11},
  journaltitle         = {Insurance: Mathematics and Economics},
  title                = {Application of data clustering and machine learning in variable annuity valuation},
  doi                  = {10.1016/j.insmatheco.2013.09.021},
  issn                 = {0167-6687},
  number               = {3},
  pages                = {795--801},
  volume               = {53},
  abstract             = {We study the pricing of a large portfolio of VA policies. A clustering method is used to select representative policies. A machine learning method is used to estimate the guarantee value. The proposed method performs well in terms of accuracy and speed. The valuation of variable annuity guarantees has been studied extensively in the past four decades. However, almost all the studies focus on the valuation of guarantees embedded in a single variable annuity contract. How to efficiently price the guarantees for a large portfolio of variable annuity contracts has not received enough attention. This paper fills the gap by introducing a novel method based on data clustering and machine learning to price the guarantees for a large portfolio of variable annuity contracts. Our test results show that this method performs very well in terms of accuracy and speed.},
  citeulike-article-id = {13934351},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.insmatheco.2013.09.021},
  groups               = {Networks and investment management, Machine learning and investment strategies, Annuities, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:15:25},
  timestamp            = {2020-02-27 03:56},
}

@Article{Garvey-Madhavan-2019,
  author         = {Garvey, Gerald and Madhavan, Ananth},
  date           = {2019-09-09},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Reconstructing Emerging and Developed Markets Using Hierarchical Clustering},
  url            = {https://jfds.pm-research.com/content/early/2019/09/08/jfds.2019.1.014},
  urldate        = {2019-09-10},
  day            = {9},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-27 03:56},
}

@Article{Gates-Ahn-2017,
  author               = {Gates, Alexander J. and Ahn, Yong-Yeol},
  date                 = {2017-01},
  journaltitle         = {arXiv Electronic Journal},
  title                = {The Impact of Random Models on Clustering Similarity},
  eprint               = {1701.06508},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1701.06508},
  abstract             = {Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, while multiple runs of K-means clustering returns clusterings with a fixed number of clusters, the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on results and argue that the choice should be carefully justified.},
  citeulike-article-id = {14262970},
  citeulike-linkout-0  = {http://arxiv.org/abs/1701.06508},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1701.06508},
  day                  = {23},
  posted-at            = {2017-01-26 15:41:14},
  timestamp            = {2020-02-27 03:56},
}

@Article{Ghoshdastidar-et-al-2018,
  author         = {Ghoshdastidar, Debarghya and Perrot, Michael and von Luxburg, Ulrike},
  date           = {2018-11-02},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Foundations of Comparison-Based Hierarchical Clustering},
  url            = {https://arxiv.org/abs/1811.00928},
  urldate        = {2019-12-18},
  abstract       = {We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form "objects ii and jj are more similar than objects kk and ll." Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Goswami-et-al-2017,
  author         = {Goswami, Saptarsi and Das, Amit Kumar and Chakrabarti, Amlan and Chakraborty, Basabi},
  date           = {2017-08},
  journaltitle   = {Expert Systems with Applications},
  title          = {A feature cluster taxonomy based feature selection technique},
  doi            = {10.1016/j.eswa.2017.01.044},
  issn           = {0957-4174},
  pages          = {76--89},
  volume         = {79},
  abstract       = {Feature subset selection is basically an optimization problem for choosing the most important features from various alternatives in order to facilitate classification or mining problems. Though lots of algorithms have been developed so far, none is considered to be the best for all situations and researchers are still trying to come up with better solutions. In this work, a flexible and user-guided feature subset selection algorithm, named as FCTFS (Feature Cluster Taxonomy based Feature Selection) has been proposed for selecting suitable feature subset from a large feature set. The proposed algorithm falls under the genre of clustering based feature selection techniques in which features are initially clustered according to their intrinsic characteristics following the filter approach. In the second step the most suitable feature is selected from each cluster to form the final subset following a wrapper approach. The two stage hybrid process lowers the computational cost of subset selection, especially for large feature data sets. One of the main novelty of the proposed approach lies in the process of determining optimal number of feature clusters. Unlike currently available methods, which mostly employ a trial and error approach, the proposed method characterises and quantifies the feature clusters according to the quality of the features inside the clusters and defines a taxonomy of the feature clusters. The selection of individual features from a feature cluster can be done judiciously considering both the relevancy and redundancy according to user intention and requirement. The algorithm has been verified by simulation experiments with different bench mark data set containing features ranging from 10 to more than 800 and compared with other currently used feature selection algorithms. The simulation results prove the superiority of our proposal in terms of model performance, flexibility of use in practical problems and extendibility to large feature sets. Though the current proposal is verified in the domain of unsupervised classification, it can be easily used in case of supervised classification.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Grun-2018,
  author         = {Grun, Bettina},
  date           = {2018-07-05},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Model-based Clustering},
  url            = {https://arxiv.org/abs/1807.01987},
  abstract       = {Mixture models extend the toolbox of clustering methods available to the data analyst. They allow for an explicit definition of the cluster shapes and structure within a probabilistic framework and exploit estimation and inference techniques available for statistical models in general. In this chapter an introduction to cluster analysis is provided, model-based clustering is related to standard heuristic clustering methods and an overview on different ways to specify the cluster model is given. Post-processing methods to determine a suitable clustering, infer cluster distribution characteristics and validate the cluster solution are discussed. The versatility of the model-based clustering approach is illustrated by giving an overview on the different areas of applications.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Guenther-Lee-2016,
  author               = {Guenther, John and Lee, Herbert K. H.},
  date                 = {2016},
  journaltitle         = {Applied Mathematics},
  title                = {Cluster Search Algorithm for Finding Multiple Optima},
  doi                  = {10.4236/am.2016.77067},
  issn                 = {2152-7385},
  number               = {07},
  pages                = {736--752},
  volume               = {07},
  abstract             = {The black box functions found in computer experiments often result in multimodal optimization programs. Optimization that focuses on a single best optimum may not achieve the goal of getting the best answer for the purposes of the experiment. This paper builds upon an algorithm introduced in [1] that is successful for finding multiple optima within the input space of the objective function. Here we introduce an alternative cluster search algorithm for finding these optima, making use of clustering. The cluster search algorithm has several advantages over the earlier algorithm. It gives a forward view of the optima that are present in the input space so the user has a preview of what to expect as the optimization process continues. It employs pattern search, in many instances, closer to the minimum's location in input space, saving on simulator point computations. At termination, this algorithm does not need additional verification that a minimum is a duplicate of a previously found minimum, which also saves on simulator point computations. Finally, it finds minima that can be "hidden" by close larger minima.},
  citeulike-article-id = {14497899},
  citeulike-linkout-0  = {http://dx.doi.org/10.4236/am.2016.77067},
  posted-at            = {2017-12-06 19:43:47},
  timestamp            = {2020-02-27 03:56},
}

@Article{GuijoRubio-et-al-2018,
  author         = {Guijo-Rubio, David and Duran-Rosal, Antonio Manuel and Gutierrez, Pedro Antonio and Troncoso, Alicia and Hervas-Martinez, Cesar},
  date           = {2018-10-27},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Time series clustering based on the characterisation of segment typologies},
  url            = {https://arxiv.org/abs/1810.11624},
  abstract       = {Time series clustering is the process of grouping time series with respect to their similarity or characteristics. Previous approaches usually combine a specific distance measure for time series and a standard clustering method. However, these approaches do not take the similarity of the different subsequences of each time series into account, which can be used to better compare the time series objects of the dataset. In this paper, we propose a novel technique of time series clustering based on two clustering stages. In a first step, a least squares polynomial segmentation procedure is applied to each time series, which is based on a growing window technique that returns different-length segments. Then, all the segments are projected into same dimensional space, based on the coefficients of the model that approximates the segment and a set of statistical features. After mapping, a first hierarchical clustering phase is applied to all mapped segments, returning groups of segments for each time series. These clusters are used to represent all time series in the same dimensional space, after defining another specific mapping process. In a second and final clustering stage, all the time series objects are grouped. We consider internal clustering quality to automatically adjust the main parameter of the algorithm, which is an error threshold for the segmenta- tion. The results obtained on 84 datasets from the UCR Time Series Classification Archive have been compared against two state-of-the-art methods, showing that the performance of this methodology is very promising.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Gu-Louca-2019,
  author         = {Gu, Ariel and Louca, Christodoulos},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {What drives new mutual fund clustering?},
  doi            = {10.2139/ssrn.3409491},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3409491},
  urldate        = {2019-11-28},
  abstract       = {Over time, the aggregate new mutual fund volume is considerably larger in markets. The naive explanation that new fund volume correlates with the economic environment is incomplete because active mutual funds, on average, underperform. However, we demonstrate that fund families exploit IPO-related investment opportunities, which correlate cross-sectionally for economy-wide reasons, more by creating new funds than by using existing funds. This naturally leads to new fund volume clustering that is correlated with the economic environment. In addition, consistent with fund families strategically exploiting the economic environment, new funds with access to IPO offerings outperform and attract higher investment flows.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@InCollection{Gupta-Chatterjee-2018,
  author               = {Gupta, Kartikay and Chatterjee, Niladri},
  booktitle            = {Information and Communication Technology for Intelligent Systems (ICTIS 2017) - Volume 2},
  date                 = {2018},
  title                = {Financial Time Series Clustering},
  doi                  = {10.1007/978-3-319-63645-0\_16},
  editor               = {Satapathy, Suresh C. and Joshi, Amit},
  pages                = {146--156},
  publisher            = {Springer International Publishing},
  series               = {Smart Innovation, Systems and Technologies},
  volume               = {84},
  abstract             = {Financial time series clustering finds application in forecasting, noise reduction and enhanced index tracking. The central theme in all the available clustering algorithms is the dissimilarity measure employed by the algorithm. The dissimilarity measures, applicable in financial domain, as used or suggested in past researches, are correlation based dissimilarity measure, temporal correlation based dissimilarity measure and dynamic time wrapping (DTW) based dissimilarity measure. One shortcoming of these dissimilarity measures is that they do not take into account the lead or lag existing between the returns of different stocks which changes with time. Mostly, such stocks with high value of correlation at some lead or lag belong to the same cluster (or sector). The present paper, proposes two new dissimilarity measures which show superior clustering results as compared to past measures when compared over 3 data sets comprising of 526 companies.},
  citeulike-article-id = {14435138},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-63645-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-63645-016},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-09-21 00:23:24},
  timestamp            = {2020-02-27 03:56},
}

@Article{Han-Ge-2020,
  author         = {Han, Jingti and Ge, Zhipeng},
  date           = {2020-01},
  journaltitle   = {Expert systems with applications},
  title          = {Effect of dimensionality reduction on stock selection with cluster analysis in different market situations},
  doi            = {10.1016/j.eswa.2020.113226},
  issn           = {0957-4174},
  pages          = {113226},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S095741742030052X}},
  urldate        = {2020-01-23},
  abstract       = {Dimensionality reduction is inevitable in stock selection with cluster analysis. Considering relations among dimensionality reduction, noise trading, and market situations, we empirically investigate the effect of dimensionality-reduction methods-principal component analysis, stacked autoencoder, and stacked restricted Boltzmann machine-on stock selection with cluster analysis in different market situations. Based on the index fluctuation, the market is divided into sideways and trend situations. For the CSI 100 and Nikkei 225 constituent stocks, experimental results show that: (1) in sideways situations, dimensionality reduction hardly improves the performance of stock selection with cluster analysis; (2) the advantage of dimensionality reduction is mainly reflected in trend situations, but whether it is in an up or down trend depends on the market analyzed. More importantly, according to the above findings and assuming that the dimensionality-reduction effect will continue, we propose a rotation strategy with and without dimensionality reduction. The results of experiments show that the proposed rotation strategy outperforms the stock market indices as well as the stock-selection strategies based on dimensionality reduction and cluster analysis. These findings offer practical insights into how dimensionality reduction can be efficiently used for stock selection.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hennig-et-al-2018,
  author         = {Hennig, Christian and Viroli, Cinzia and Anderlucci, Laura},
  date           = {2018-06-27},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Quantile-based clustering},
  url            = {https://arxiv.org/abs/1806.10403},
  abstract       = {A new cluster analysis method, K-quantiles clustering, is introduced. K-quantiles clustering can be computed by a simple greedy algorithm in the style of the classical Lloyd's algorithm for K-means. It can be applied to large and high-dimensional datasets. It allows for within-cluster skewness and internal variable scaling based on within-cluster variation. Different versions allow for different levels of parsimony and computational efficiency. Although K-quantiles clustering is conceived as nonparametric, it can be connected to a fixed partition model of generalized asymmetric Laplace-distributions. The consistency of K-quantiles clustering is proved, and it is shown that K-quantiles clusters correspond to well separated mixture components in a nonparametric mixture. In a simulation, K-quantiles clustering is compared with a number of popular clustering methods with good results. A high-dimensional microarray dataset is clustered by K-quantiles.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hofmeyr-2017,
  author               = {Hofmeyr, David P.},
  date                 = {2017-08-01},
  journaltitle         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title                = {Clustering by Minimum Cut Hyperplanes},
  doi                  = {10.1109/tpami.2016.2609929},
  issn                 = {0162-8828},
  number               = {8},
  pages                = {1547--1560},
  volume               = {39},
  abstract             = {Minimum normalised graph cuts are highly effective ways of partitioning unlabeled data, having been made popular by the success of spectral clustering. This work presents a novel method for learning hyperplane separators which minimise this graph cut objective, when data are embedded in Euclidean space. The optimisation problem associated with the proposed method can be formulated as a sequence of univariate subproblems, in which the optimal hyperplane orthogonal to a given vector is determined. These subproblems can be solved in log-linear time, by exploiting the trivial factorisation of the exponential function. Experimentation suggests that the empirical runtime of the overall algorithm is also log-linear in the number of data. Asymptotic properties of the minimum cut hyperplane, both for a finite sample, and for an increasing sample assumed to arise from an underlying probability distribution are discussed. In the finite sample case the minimum cut hyperplane converges to the maximum margin hyperplane as the scaling parameter is reduced to zero. Applying the proposed methodology, both for fixed scaling, and the large margin asymptotes, is shown to produce high quality clustering models in comparison with state-of-the-art clustering algorithms in experiments using a large collection of benchmark datasets.},
  citeulike-article-id = {14486419},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tpami.2016.2609929},
  day                  = {1},
  posted-at            = {2017-11-30 19:03:59},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Hofmeyr-Pavlidis-2015,
  author               = {Hofmeyr, David and Pavlidis, Nicos},
  booktitle            = {IEEE Symposium Series on Computational Intelligence},
  date                 = {2015-12},
  title                = {Maximum Clusterability Divisive Clustering},
  doi                  = {10.1109/ssci.2015.116},
  isbn                 = {978-1-4799-7560-0},
  location             = {Cape Town, South Africa},
  pages                = {780--786},
  publisher            = {IEEE},
  abstract             = {The notion of cluster ability is often used to determine how strong the cluster structure within a set of data is, as well as to assess the quality of a clustering model. In multivariate applications, however, the cluster ability of a data set can be obscured by irrelevant or noisy features. We study the problem of finding low dimensional projections which maximise the cluster ability of a data set. In particular, we seek low dimensional representations of the data which maximise the quality of a binary partition. We use this bi-partitioning recursively to generate high quality clustering models. We illustrate the improvement over standard dimension reduction and clustering techniques, and evaluate our method in experiments on real and simulated data sets.},
  citeulike-article-id = {14486417},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ssci.2015.116},
  posted-at            = {2017-11-30 19:02:58},
  timestamp            = {2020-02-27 03:56},
}

@InProceedings{Holst-et-al-2019,
  author         = {Holst, Anders and Bae, Juhee and Karlsson, Alexander and Bouguelia, Mohamed-Rafik},
  booktitle      = {Proceedings of the Workshop on Interactive Data Mining - WIDM'19},
  date           = {2019-02-15},
  title          = {Interactive clustering for exploring multiple data streams at different time scales and granularity},
  doi            = {10.1145/3304079.3310286},
  isbn           = {9781450362962},
  location       = {New York, New York, USA},
  pages          = {1--7},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3304079.3310286},
  urldate        = {2019-09-06},
  abstract       = {We approach the problem of identifying and interpreting clusters over different time scales and granularity in multivariate time series data. We extract statistical features over a sliding window of each time series, and then use a Gaussian mixture model to identify clusters which are then projected back on the data streams. The human analyst can then further analyze this projection and adjust the size of the sliding window and the number of clusters in order to capture the different types of clusters over different time scales. We demonstrate the effectiveness of our approach in two different application scenarios: (1) fleet management and (2) district heating, wherein each scenario, several different types of meaningful clusters can be identified when varying over these dimensions.},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hong-et-al-2017,
  author               = {Hong, Dezhi and Gu, Quanquan and Whitehouse, Kamin},
  date                 = {2017},
  journaltitle         = {Proceedings of Machine Learning Research},
  title                = {High-dimensional Time Series Clustering via Cross-Predictability},
  url                  = {http://proceedings.mlr.press/v54/hong17a.html},
  abstract             = {The key to time series clustering is how to characterize the similarity between any two time series. In this paper, we explore a new similarity metric called "cross-predictability": the degree to which a future value in each time series is predicted by past values of the others. However, it is challenging to estimate such cross-predictability among time series in the high-dimensional regime, where the number of time series is much larger than the length of each time series. We address this challenge with a sparsity assumption: only time series in the same cluster have significant cross-predictability with each other. We demonstrate that this approach is computationally attractive, and provide a theoretical proof that the proposed algorithm will identify the correct clustering structure with high probability under certain conditions. To the best of our knowledge, this is the first practical high-dimensional time series clustering algorithm with a provable guarantee. We evaluate with experiments on both synthetic data and real-world data, and results indicate that our method can achieve more than 80\% clustering accuracy on real-world data, which is 20\% higher than the state-of-art baselines.},
  citeulike-article-id = {14435135},
  groups               = {Predictability_FinInfo, ML_ClustTimeSrs},
  posted-at            = {2017-09-20 23:56:17},
  timestamp            = {2020-02-27 03:56},
}

@Article{Horel-et-al-2019,
  author         = {Horel, Enguerrand and Giesecke, Kay and Storchan, Victor and Chittar, Naren},
  date           = {2019},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Explainable Clustering and Application to Wealth Management Compliance},
  url            = {https://arxiv.org/abs/1909.13381},
  urldate        = {2019-09-07},
  abstract       = {Many applications from the financial industry successfully leverage clustering algorithms to reveal meaningful patterns among a vast amount of unstructured financial data. However, these algorithms suffer from a lack of interpretability that is required both at a business and regulatory level. In order to overcome this issue, we propose a novel two-steps method to explain clusters. A classifier is first trained to predict the clusters labels, then the Single Feature Introduction Test (SFIT) method is run on the model to identify the statistically significant features that characterise each cluster. We describe a real wealth management compliance use-case that highlights the necessity of such an interpretable clustering method. We illustrate the performance of our method through an experiment on financial ratios of U.S. companies.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Huang-et-al-2013,
  author               = {Huang, Hanwen and Liu, Yufeng and Yuan, Ming and Marron, J. S.},
  date                 = {2013},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Statistical Significance of Clustering using Soft Thresholding},
  eprint               = {1305.5879},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1305.5879},
  abstract             = {Clustering methods have led to a number of important discoveries in bioinformatics and beyond. A major challenge in their use is determining which clusters represent important underlying structure, as opposed to spurious sampling artifacts. This challenge is especially serious, and very few methods are available when the data are very high in dimension. Statistical Significance of Clustering (SigClust) is a recently developed cluster evaluation tool for high dimensional low sample size data. An important component of the SigClust approach is the very definition of a single cluster as a subset of data sampled from a multivariate Gaussian distribution. The implementation of SigClust requires the estimation of the eigenvalues of the covariance matrix for the null multivariate Gaussian distribution. We show that the original eigenvalue estimation can lead to a test that suffers from severe inflation of type-I error, in the important case where there are huge single spikes in the eigenvalues. This paper addresses this critical challenge using a novel likelihood based soft thresholding approach to estimate these eigenvalues which leads to a much improved SigClust. These major improvements in SigClust performance are shown by both theoretical work and an extensive simulation study. Applications to some cancer genomic data further demonstrate the usefulness of these improvements.},
  citeulike-article-id = {14444684},
  citeulike-linkout-0  = {http://arxiv.org/abs/1305.5879},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1305.5879},
  day                  = {29},
  posted-at            = {2017-10-03 17:02:54},
  timestamp            = {2020-02-27 03:56},
  year                 = {2013},
}

@Article{Huang-et-al-2017a,
  author               = {Huang, Jinlong and Zhu, Qingsheng and Yang, Lijun and Cheng, Dongdong and Wu, Quanwang},
  date                 = {2017},
  journaltitle         = {Machine Learning},
  title                = {QCC: a novel clustering algorithm based on Quasi-Cluster Centers},
  doi                  = {10.1007/s10994-016-5608-2},
  number               = {3},
  pages                = {337--357},
  volume               = {106},
  abstract             = {Cluster analysis aims at classifying objects into categories on the basis of their similarity and has been widely used in many areas such as pattern recognition and image processing. In this paper, we propose a novel clustering algorithm called QCC mainly based on the following ideas: the density of a cluster center is the highest in its K nearest neighborhood or reverse K nearest neighborhood, and clusters are divided by sparse regions. Besides, we define a novel concept of similarity between clusters to solve the complex-manifold problem. In experiments, we compare the proposed algorithm QCC with DBSCAN, DP and DAAP algorithms on synthetic and real-world datasets. Results show that QCC performs the best, and its superiority on clustering non-spherical data and complex-manifold data is especially large.},
  citeulike-article-id = {14334821},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10994-016-5608-2},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10994-016-5608-2},
  posted-at            = {2017-04-10 01:08:53},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 03:56},
}

@Article{Huang-Ribeiro-2016,
  author               = {Huang, Weiyu and Ribeiro, Alejandro},
  date                 = {2016-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Hierarchical Clustering Given Confidence Intervals of Metric Distances},
  eprint               = {1610.04274},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.04274},
  abstract             = {This paper considers metric spaces where distances between a pair of nodes are represented by distance intervals. The goal is to study methods for the determination of hierarchical clusters, i.e., a family of nested partitions indexed by a resolution parameter, induced from the given distance intervals of the metric spaces. Our construction of hierarchical clustering methods is based on defining admissible methods to be those methods that abide to the axioms of value - nodes in a metric space with two nodes are clustered together at the convex combination of the distance bounds between them - and transformation - when both distance bounds are reduced, the output may become more clustered but not less. Two admissible methods are constructed and are shown to provide universal upper and lower bounds in the space of admissible methods. Practical implications are explored by clustering moving points via snapshots and by clustering networks representing brain structural connectivity using the lower and upper bounds of the network distance. The proposed clustering methods succeed in identifying underlying clustering structures via the maximum and minimum distances in all snapshots, as well as in differentiating brain connectivity networks of patients from those of healthy controls.},
  citeulike-article-id = {14170685},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.04274},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.04274},
  day                  = {13},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:06:50},
  timestamp            = {2020-02-27 03:56},
}

@Article{Isogai-2017,
  author               = {Isogai, Takashi},
  date                 = {2017-05},
  journaltitle         = {Applied Network Science},
  title                = {Dynamic correlation network analysis of financial asset returns with network clustering},
  doi                  = {10.1007/s41109-017-0031-6},
  issn                 = {2364-8228},
  number               = {1},
  volume               = {2},
  abstract             = {In this study, we propose a novel approach to analyze a dynamic correlation network of highly volatile financial asset returns by using a network clustering algorithm to deal with high dimensionality issues. We analyze the dynamic correlation network of selected Japanese stock returns as an empirical study of the correlation dynamics at the market level by applying the proposed method. Two types of network clustering algorithms are employed for the dimensionality reduction. Firstly, several stock groups instead of the existing business sector classification are generated by the hierarchical recursive network clustering of filtered stock returns in order to overcome the high dimensionality problem due to the large number of stocks. The stock returns are then filtered in advance to control for volatility fluctuations that can distort the correlation between stocks. Thus, the correlation network of individual stock returns is transformed into a correlation network of group-based portfolio returns. Secondly, the reduced size of the correlation network is extended to a dynamic one by using a model-based correlation estimation method. A time series of adjacency matrices is created on a daily basis as a dynamic correlation network from the estimation results. Then, the correlation network is summarized into only three representative correlation networks by clustering along the time axis. Some intertemporal comparisons of the dynamic correlation network are conducted by examining the differences between the three sub-period networks. Our dynamic correlation network analysis framework is not limited to stock returns, but can be applied to many other financial and non-financial volatile time series data.},
  citeulike-article-id = {14399609},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s41109-017-0031-6},
  day                  = {23},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network, Vol_Cluster},
  posted-at            = {2017-07-26 14:35:01},
  timestamp            = {2020-02-27 03:56},
}

@Article{Izakian-et-al-2014,
  author               = {Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
  date                 = {2015-03},
  journaltitle         = {Engineering Applications of Artificial Intelligence},
  title                = {Fuzzy clustering of time series data using dynamic time warping distance},
  doi                  = {10.1016/j.engappai.2014.12.015},
  issn                 = {0952-1976},
  pages                = {235--244},
  volume               = {39},
  abstract             = {Clustering is a powerful vehicle to reveal and visualize structure of data. When dealing with time series, selecting a suitable measure to evaluate the similarities/dissimilarities within the data becomes necessary and subsequently it exhibits a significant impact on the results of clustering. This selection should be based upon the nature of time series and the application itself. When grouping time series based on their shape information is of interest (shape-based clustering), using a Dynamic Time Warping (DTW) distance is a desirable choice. Using stretching or compressing segments of temporal data, DTW determines an optimal match between any two time series. In this way, time series exhibiting similar patterns occurring at different time periods, are considered as being similar. Although DTW is a suitable choice for comparing data with respect to their shape information, calculating the average of a collection of time series (which is required in clustering methods) based on this distance becomes a challenging problem. As the result, employing clustering techniques like K-Means and Fuzzy C-Means (where the cluster centers - prototypes are calculated through averaging the data) along with the DTW distance is a challenging task and may produce unsatisfactory results. In this study, three alternatives for fuzzy clustering of time series using DTW distance are proposed. In the first method, a DTW-based averaging technique proposed in the literature, has been applied to the Fuzzy C-Means clustering. The second method considers a Fuzzy C-Medoids clustering, while the third alternative comes as a hybrid technique, which exploits the advantages of both the Fuzzy C-Means and Fuzzy C-Medoids when clustering time series. Experimental studies are reported over a set of time series coming from the UCR time series database.},
  citeulike-article-id = {14485204},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.engappai.2014.12.015},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-11-28 14:19:15},
  timestamp            = {2020-02-27 03:56},
}

@InProceedings{Jha-et-al-2015,
  author         = {Jha, Abhay and Ray, Shubhankar and Seaman, Brian and Dhillon, Inderjit S.},
  booktitle      = {2015 IEEE 31st International Conference on Data Engineering},
  date           = {2015-04-13},
  title          = {Clustering to forecast sparse time-series data},
  doi            = {10.1109/{ICDE}.2015.7113385},
  isbn           = {978-1-4799-7964-6},
  pages          = {1388--1399},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/7113385/},
  urldate        = {2019-09-11},
  abstract       = {Forecasting accurately is essential to successful inventory planning in retail. Unfortunately, there is not always enough historical data to forecast items individually- this is particularly true in e-commerce where there is a long tail of low selling items, and items are introduced and phased out quite frequently, unlike physical stores. In such scenarios, it is preferable to forecast items in well-designed groups of similar items, so that data for different items can be pooled together to fit a single model. In this paper, we first discuss the desiderata for such a grouping and how it differs from the traditional clustering problem. We then describe our approach which is a scalable local search heuristic that can naturally handle the constraints required in this setting, besides being capable of producing solutions competitive with well-known clustering algorithms. We also address the complementary problem of estimating similarity, particularly in the case of new items which have no past sales. Our solution is to regress the sales profile of items against their semantic features, so that given just the semantic features of a new item we can predict its relation to other items, in terms of as yet unobserved sales. Our experiments demonstrate both the scalability of our approach and implications for forecast accuracy.},
  day            = {13},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Jiang-et-al-2014a,
  author               = {Jiang, Kening and Li, Duan and Gao, Jianjun and Yu, Jeffrey X.},
  date                 = {2014},
  journaltitle         = {IFAC Proceedings Volumes},
  title                = {Factor Model Based Clustering Approach for Cardinality Constrained Portfolio Selection},
  doi                  = {10.3182/20140824-6-za-1003.00663},
  issn                 = {1474-6670},
  number               = {3},
  pages                = {10713--10718},
  volume               = {47},
  abstract             = {Portfolio selection concerns identifying an optimal composition of various risky assets and their corresponding holding amounts such that the corresponding investment strategy strikes a balance between maximizing the expected investment return and minimizing investment risk. While market frictions make full diversification impractical, cardinality constrained mean-variance (CCMV) portfolio selection problem emerges as a natural remedy: Given an asset pool with total n assets and a given cardinality s n, optimally choose s assets from the entire asset pool such as to achieve a mean-variance efficiency. Unfortunately, CCMV has been proved to be NP hard and has been posted in front of optimization society as a long-standing challenge. By invoking structural market information and utilizing fast clustering algorithm for classification, we develop in this paper an effective heuristic scheme to identify approximate solutions for large-scale CCMV problems. More specifically, by constructing grouping constraints generated from factor-model based clustering algorithm and attaching them to the mixed integer programming formulation associated with the CCMV problem, we are able to significantly reduce the computational complexity, thus offering a fast algorithm with relatively high quality solution.},
  citeulike-article-id = {14321944},
  citeulike-linkout-0  = {http://dx.doi.org/10.3182/20140824-6-za-1003.00663},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-28 18:07:33},
  timestamp            = {2020-02-27 04:01},
}

@Article{Ji-et-al-2018,
  author         = {Ji, Hao and Wang, Hao and Liseo, Brunero},
  date           = {2018-09},
  journaltitle   = {Australian economic papers},
  title          = {Portfolio Diversification Strategy Via Tail-Dependence Clustering and ARMA-GARCH Vine Copula Approach},
  doi            = {10.1111/1467-8454.12126},
  issn           = {0004-900X},
  number         = {3},
  pages          = {265--283},
  urldate        = {2019-12-04},
  volume         = {57},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Joe-Sang-2016,
  author               = {Joe, Harry and Sang, Peijun},
  date                 = {2016-05},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Multivariate models for dependent clusters of variables with conditional independence given aggregation variables},
  doi                  = {10.1016/j.csda.2015.12.001},
  issn                 = {0167-9473},
  pages                = {114--132},
  volume               = {97},
  abstract             = {A general multivariate distributional approach, with conditional independence given aggregation variables, is presented to combine group-based submodels when variables are naturally divided into several non-overlapping groups. When the distributions are all multivariate Gaussian, the dependence among different groups is parsimonious based on conditional independence given linear combinations of variables in each group. For the case of multivariate t distributions in each group, a grouped t distribution is obtained. The approach can be extended so that the copula for each group is based on a skew-t distribution, and an application of this is given to financial returns of stocks in several different sectors. Another example of the modeling approach is given with variables separated into groups based on their units of measurements.},
  citeulike-article-id = {14219060},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.csda.2015.12.001},
  groups               = {Data_Independence},
  owner                = {cristi},
  posted-at            = {2016-12-02 15:44:07},
  timestamp            = {2020-02-27 04:01},
}

@Article{Jung-Chang-2016,
  author               = {Jung, Sean S. and Chang, Woojin},
  date                 = {2016-11},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Clustering stocks using partial correlation coefficients},
  doi                  = {10.1016/j.physa.2016.06.094},
  issn                 = {0378-4371},
  pages                = {410--420},
  volume               = {462},
  abstract             = {Correlation analyses are conducted on Korean stock market. Agglomerative hierarchical clustering is performed based on correlation matrices. Each cluster consists of firms from multiple business sectors. A partial correlation analysis is performed on the Korean stock market (KOSPI). The difference between Pearson correlation and the partial correlation is analyzed and it is found that when conditioned on the market return, Pearson correlation coefficients are generally greater than those of the partial correlation, which implies that the market return tends to drive up the correlation between stock returns. A clustering analysis is then performed to study the market structure given by the partial correlation analysis and the members of the clusters are compared with the Global Industry Classification Standard (GICS). The initial hypothesis is that the firms in the same GICS sector are clustered together since they are in a similar business and environment. However, the result is inconsistent with the hypothesis and most clusters are a mix of multiple sectors suggesting that the traditional approach of using sectors to determine the proximity between stocks may not be sufficient enough to diversify a portfolio.},
  citeulike-article-id = {14149947},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2016.06.094},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:50:28},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kamalzadeh-et-al-2019,
  author         = {Kamalzadeh, Hossein and Ahmadi, Abbas and Mansour, Saeed},
  date           = {2019-12-05},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Time-Series by a Novel Slope-Based Similarity Measure Considering Particle Swarm Optimization},
  url            = {https://arxiv.org/abs/1912.02405},
  urldate        = {2019-12-22},
  abstract       = {Recently there has been an increase in the studies on time-series data mining specifically time-series clustering due to the vast existence of time-series in various domains. The large volume of data in the form of time-series makes it necessary to employ various techniques such as clustering to understand the data and to extract information and hidden patterns. In the field of clustering specifically, time-series clustering, the most important aspects are the similarity measure used and the algorithm employed to conduct the clustering. In this paper, a new similarity measure for time-series clustering is developed based on a combination of a simple representation of time-series, slope of each segment of time-series, Euclidean distance and the so-called dynamic time warping. It is proved in this paper that the proposed distance measure is metric and thus indexing can be applied. For the task of clustering, the Particle Swarm Optimization algorithm is employed. The proposed similarity measure is compared to three existing measures in terms of various criteria used for the evaluation of clustering algorithms. The results indicate that the proposed similarity measure outperforms the rest in almost every dataset used in this paper.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Kang-et-al-2015,
  author               = {Kang, Zhao and Peng, Chong and Cheng, Qiang},
  date                 = {2015-11},
  journaltitle         = {IEEE Signal Processing Letters},
  title                = {Robust Subspace Clustering via Smoothed Rank Approximation},
  doi                  = {10.1109/lsp.2015.2460737},
  issn                 = {1070-9908},
  number               = {11},
  pages                = {2088--2092},
  volume               = {22},
  abstract             = {Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this letter, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms.},
  citeulike-article-id = {14351210},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/lsp.2015.2460737},
  posted-at            = {2017-05-05 01:31:46},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kang-et-al-2017b,
  author               = {Kang, Zhao and Peng, Chong and Cheng, Qiang},
  date                 = {2017-05},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Twin Learning for Similarity and Clustering: A Unified Kernel Approach},
  eprint               = {1705.00678},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1705.00678},
  abstract             = {Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.},
  citeulike-article-id = {14351185},
  citeulike-linkout-0  = {http://arxiv.org/abs/1705.00678},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1705.00678},
  day                  = {3},
  posted-at            = {2017-05-04 22:18:58},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kauffmann-et-al-2019,
  author         = {Kauffmann, Jacob and Esders, Malte and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date           = {2019-06-18},
  journaltitle   = {arXiv Electronic Journal},
  title          = {From Clustering to Cluster Explanations via Neural Networks},
  url            = {https://arxiv.org/abs/1906.07633},
  urldate        = {2019-10-11},
  abstract       = {A wealth of algorithms have been developed to extract natural cluster structure in data. Identifying this structure is desirable but not always sufficient: We may also want to understand why the data points have been assigned to a given cluster. Clustering algorithms do not offer a systematic answer to this simple question. Hence we propose a new framework that can, for the first time, explain cluster assignments in terms of input features in a comprehensive manner. It is based on the novel theoretical insight that clustering models can be rewritten as neural networks, or 'neuralized'. Predictions of the obtained networks can then be quickly and accurately attributed to the input features. Several showcases demonstrate the ability of our method to assess the quality of learned clusters and to extract novel insights from the analyzed data and representations.},
  day            = {18},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Kawamoto-Kabashima-2017a,
  author               = {Kawamoto, Tatsuro and Kabashima, Yoshiyuki},
  date                 = {2017-06},
  journaltitle         = {Scientific Reports},
  title                = {Cross-validation estimate of the number of clusters in a network},
  doi                  = {10.1038/s41598-017-03623-x},
  issn                 = {2045-2322},
  number               = {1},
  volume               = {7},
  abstract             = {Network science investigates methodologies that summarise relational data to obtain better interpretability. Identifying modular structures is a fundamental task, and assessment of the coarse-grain level is its crucial step. Here, we propose principled, scalable, and widely applicable assessment criteria to determine the number of clusters in modular networks based on the leave-one-out cross-validation estimate of the edge prediction error.},
  citeulike-article-id = {14449671},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/s41598-017-03623-x},
  day                  = {12},
  posted-at            = {2017-10-12 23:55:55},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kazor-Hering-2015,
  author               = {Kazor, Karen and Hering, AmandaS},
  date                 = {2015},
  journaltitle         = {Journal of Agricultural, Biological, and Environmental Statistics},
  title                = {Assessing the Performance of Model-Based Clustering Methods in Multivariate Time Series with Application to Identifying Regional Wind Regimes},
  doi                  = {10.1007/s13253-015-0203-8},
  number               = {2},
  pages                = {192--217},
  volume               = {20},
  abstract             = {The desire to group observations generated from multivariate time series is common in many applications with the goal to distinguish not only between differences in the means of individual variables but also changes in their covariances and in the temporal dependence of observations. In this analysis, we compare ten model-based clustering methods in terms of their ability to identify such features under four scenarios in which data are simulated with varying levels of variable and temporal dependence. To consider these methods in a realistic environment, we focus our analysis on wind data, where observations are often strongly correlated in time, and the dependence of variables is known to vary across different regional weather patterns. In particular, we assess each method's performance when applied to wind data simulated under a realistic two-regime Markov-switching vector autoregressive (VAR) model with a diurnally varying mean. A Gaussian mixture model and a basic Markov-switching model outperform the other methods considered in terms of misclassification rates and number of clusters identified. These two methods and an additional Markov-switching VAR model are then applied to one year of averaged hourly wind data from twenty meteorological stations, and we find that the methods can identify very different features in the data. Supplementary materials accompanying this paper appear on-line.},
  citeulike-article-id = {14014328},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s13253-015-0203-8},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s13253-015-0203-8},
  owner                = {cristi},
  posted-at            = {2016-04-17 17:30:34},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:01},
}

@Article{Khaleghi-et-al-2016,
  author               = {Khaleghi, Azadeh and Ryabko, Daniil and Mary, Jeremie and Preux, Philippe},
  date                 = {2016},
  journaltitle         = {The Journal of Machine Learning Research},
  title                = {Consistent algorithms for clustering time series},
  pages                = {1--32},
  url                  = {http://jmlr.org/papers/v17/khaleghi16a.html},
  volume               = {17},
  abstract             = {The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.},
  citeulike-article-id = {14148585},
  groups               = {Networks and investment management, Clustering and network analysis, ML_ClustTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:51:44},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kou-et-al-2014,
  author               = {Kou, Gang and Peng, Yi and Wang, Guoxun},
  date                 = {2014-08},
  journaltitle         = {Information Sciences},
  title                = {Evaluation of clustering algorithms for financial risk analysis using MCDM methods},
  doi                  = {10.1016/j.ins.2014.02.137},
  issn                 = {0020-0255},
  pages                = {1--12},
  volume               = {275},
  abstract             = {The evaluation of clustering algorithms is intrinsically difficult because of the lack of objective measures. Since the evaluation of clustering algorithms normally involves multiple criteria, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper presents an MCDM-based approach to rank a selection of popular clustering algorithms in the domain of financial risk analysis. An experimental study is designed to validate the proposed approach using three MCDM methods, six clustering algorithms, and eleven cluster validity indices over three real-life credit risk and bankruptcy risk data sets. The results demonstrate the effectiveness of MCDM methods in evaluating clustering algorithms and indicate that the repeated-bisection method leads to good 2-way clustering solutions on the selected financial risk data sets.},
  citeulike-article-id = {14435123},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ins.2014.02.137},
  posted-at            = {2017-09-20 22:42:34},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kozdoba-Mannor-2016,
  author               = {Kozdoba, Mark and Mannor, Shie},
  date                 = {2016-09},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Clustering Time Series and the Surprising Robustness of HMMs},
  eprint               = {1605.02531},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1605.02531},
  abstract             = {Suppose that we are given a time series where consecutive samples are believed to come from a probabilistic source, that the source changes from time to time and that the total number of sources is fixed. Our objective is to estimate the distributions of the sources. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, since the data often lacks the Markov or the stationarity properties of an HMM, one can ask whether this approach is still suitable or perhaps another approach is required. In this paper we show that a maximum likelihood HMM estimator can be used to approximate the source distributions in a much larger class of models than HMMs. Specifically, we propose a natural and fairly general non-stationary model of the data, where the only restriction is that the sources do not change too often. Our main result shows that for this model, a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments.},
  citeulike-article-id = {14357388},
  citeulike-linkout-0  = {http://arxiv.org/abs/1605.02531},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1605.02531},
  day                  = {14},
  groups               = {Clustering and network analysis, NonStatry_FinTimeSrs},
  posted-at            = {2017-05-15 17:42:58},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Lemieux-et-al-2014,
  author               = {Lemieux, Victoria and Rahmdel, Payam S. and Walker, Rick and Wong, B. L. William and Flood, Mark},
  booktitle            = {Proceedings of the International Workshop on Data Science for Macro-Modeling},
  date                 = {2014},
  title                = {Clustering Techniques And Their Effect on Portfolio Formation and Risk Analysis},
  doi                  = {10.1145/2630729.2630749},
  isbn                 = {978-1-4503-3012-1},
  location             = {Snowbird, UT, USA},
  publisher            = {ACM},
  series               = {DSMM'14},
  abstract             = {This paper explores the application of three different portfolio formation rules using standard clustering techniques---K-means, K-mediods, and hierarchical---to a large financial data set (16 years of daily CRSP stock data) to determine how the choice of clustering technique may affect analysts' perceptions of the riskiness of different portfolios in the context of a prototype visual analytics system designed for financial stability monitoring. We use a two-phased experimental approach with visualizations to explore the effects of the different clustering techniques. The choice of clustering technique matters. There is significant variation among techniques, resulting in different "pictures"of the riskiness of the same underlying data when plotted to the visual analytics tool. This sensitivity to clustering methodolgy has the potential to mislead analysts about the riskiness of portfolios. We conclude that further research into the implications of portfolio formation rules is needed, and that visual analytics tools should not limit analysts to a single clustering technique, but instead should provide the facility to explore the data using different techniques.},
  address              = {New York, NY, USA},
  citeulike-article-id = {14148038},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=2630749},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/2630729.2630749},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:10:37},
  timestamp            = {2020-02-27 04:01},
}

@Article{Leon-et-al-2017,
  author               = {Leon, Diego and Aragon, Arbey and Sandoval, Javier and Hernandez, German and Arevalo, Andres and Nino, Jaime},
  date                 = {2017},
  journaltitle         = {Procedia Computer Science},
  title                = {Clustering algorithms for Risk-Adjusted Portfolio Construction},
  doi                  = {10.1016/j.procs.2017.05.185},
  issn                 = {1877-0509},
  pages                = {1334--1343},
  volume               = {108},
  abstract             = {This paper presents the performance of seven portfolios created using clustering analysis techniques to sort out assets into categories and then applying classical optimization inside every cluster to select best assets inside each asset category. The proposed clustering algorithms are tested constructing portfolios and measuring their performances over a two month dataset of 1-minute asset returns from a sample of 175 assets of the Russell 1000 index. A three-week sliding window is used for model calibration, leaving an out of sample period of five weeks for testing. Model calibration is done weekly. Three different rebalancing periods are tested: every 1, 2 and 4 hours. The results show that all clustering algorithms produce more stable portfolios with similar volatility. In this sense, the portfolios volatilities generated by the clustering algorithms are smaller when compare to the portfolio obtained using classical Mean-Variance Optimization (MVO) over all the dataset. Hierarchical clustering algorithms achieve the best financial performance obtaining an adequate trade-off between accumulated financial returns and the risk-adjusted measure, Omega Ratio, during the out of sample testing period.},
  citeulike-article-id = {14379414},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.procs.2017.05.185},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, Invest_Network, Vol_Cluster},
  posted-at            = {2017-06-19 22:00:54},
  timestamp            = {2020-02-27 04:01},
}

@Article{Leon-et-al-2017a,
  author               = {Leon, Carlos and Kim, Geun-Young and Martinez, Constanza and Lee, Daeyup},
  date                 = {2017-08},
  journaltitle         = {Quantitative Finance},
  title                = {Equity markets' clustering and the global financial crisis},
  doi                  = {10.1080/14697688.2017.1357970},
  pages                = {1--18},
  abstract             = {The effect of the Global Financial Crisis (GFC) has been substantial across markets and countries worldwide. We examine how the GFC has changed the way equity markets group together based on the similarity of stock indices? daily returns. Our examination is based on agglomerative clustering methods, which yield a hierarchical structure that represents how stock markets relate to each other based on their cross-section similarity. Main results show that both hierarchical structures, before and after the GFC, are readily interpretable, and indicate that geographical factors dominate the hierarchy. The main features of equity markets? hierarchical structure agree with most stylized facts reported in related literature. The most noticeable change after the GFC is an increase in (geographical) clustering. However, the increase in clusters? compactness and the decrease in clusters? separateness point out that world equity markets became more interconnected after the GFC. Some changes in the hierarchy that do not conform to geographical clustering are explained by well-known idiosyncratic features or shocks.},
  citeulike-article-id = {14438484},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1357970},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2017.1357970},
  day                  = {25},
  posted-at            = {2017-09-26 21:07:18},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Leverger-et-al-2019,
  author         = {Leverger, Colin and Malinowski, Simon and Guyet, Thomas and Lemaire, Vincent and Bondu, Alexis and Termier, Alexandre},
  booktitle      = {Intelligent data engineering and automated learning - IDEAL 2019: 20th international conference, manchester, UK, november 14-16, 2019, proceedings, part I},
  date           = {2019},
  title          = {Toward a framework for seasonal time series forecasting using clustering},
  doi            = {10.1007/978-3-030-33607-3\_36},
  editor         = {Yin, Hujun and Camacho, David and Tino, Peter and Tallon-Ballesteros, Antonio J. and Menezes, Ronaldo and Allmendinger, Richard},
  isbn           = {978-3-030-33606-6},
  pages          = {328--340},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-33607-3\_36},
  urldate        = {2020-01-03},
  volume         = {11871},
  abstract       = {Seasonal behaviours are widely encountered in various applications. For instance, requests on web servers are highly influenced by our daily activities. Seasonal forecasting consists in forecasting the whole next season for a given seasonal time series. It may help a service provider to provision correctly the potentially required resources, avoiding critical situations of over- or under provision. In this article, we propose a generic framework to make seasonal time series forecasting. The framework combines machine learning techniques (1) to identify the typical seasons and (2) to forecast the likelihood of having a season type in one season ahead. We study this framework by comparing the mean squared errors of forecasts for various settings and various datasets. The best setting is then compared to state-of-the-art time series forecasting methods. We show that it is competitive with them.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:01},
}

@Article{Liechti-Bonhoeffer-2019,
  author         = {Liechti, Jonas I. and Bonhoeffer, Sebastian},
  date           = {2019-12-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A time resolved clustering method revealing longterm structures and their short-term internal dynamics},
  url            = {https://arxiv.org/abs/1912.04261v1},
  urldate        = {2019-12-15},
  abstract       = {The last decades have not only been characterized by an explosive growth of data, but also an increasing appreciation of data as a valuable resource. It's value comes with the ability to extract meaningful patterns that are of economic, societal or scientific relevance. A particular challenge is to identify patterns across time, including patterns that might only become apparent when the temporal dimension is taken into account. Here, we present a novel method that aims to achieve this by detecting dynamic clusters, i.e. structural elements that can be present over prolonged durations. It is based on an adaptive identification of majority overlaps between groups at different time points and allows the accommodation of transient decompositions in otherwise persistent dynamic clusters. As such, our method enables the detection of persistent structural elements with internal dynamics and can be applied to any classifiable data, ranging from social contact networks to arbitrary sets of time stamped feature vectors. It provides a unique tool to study systems with non-trivial temporal dynamics with a broad applicability to scientific, societal and economic data.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lin-et-al-2018a,
  author         = {Lin, Alexander and Zhang, Yingzhuo and Heng, Jeremy and Allsop, Stephen A. and Tye, Kay M. and Jacob, Pierre E. and Ba, Demba},
  date           = {2018-10-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach},
  url            = {https://arxiv.org/abs/1810.09920},
  abstract       = {We propose a general statistical framework for clustering multiple time series that exhibit nonlinear dynamics into an a-priori-unknown number of sub-groups. Our motivation comes from neuroscience, where an important problem is to identify, within a large assembly of neurons, subsets that respond similarly to a stimulus or contingency. Upon modeling the multiple time series as the output of a Dirichlet process mixture of nonlinear state-space models, we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that alternates between sampling cluster assignments and sampling parameter values that form the basis of the clustering. The Metropolis step employs recent innovations in particle-based methods. We apply the framework to clustering time series acquired from the prefrontal cortex of mice in an experiment designed to characterize the neural underpinnings of fear.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lin-Li-2017,
  author               = {Lin, Lin and Li, Jia},
  date                 = {2017},
  journaltitle         = {Journal of Machine Learning Research},
  title                = {Clustering with Hidden Markov Model on Variable Blocks},
  number               = {110},
  pages                = {1--49},
  url                  = {http://jmlr.org/papers/v18/16-342.html},
  volume               = {18},
  abstract             = {Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods.},
  citeulike-article-id = {14509669},
  citeulike-linkout-0  = {http://jmlr.org/papers/v18/16-342.html},
  keywords             = {*file-import-17-12-29},
  posted-at            = {2017-12-29 02:10:57},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lisi-Menardi-2015,
  author               = {Lisi, Francesco and Menardi, Giovanna},
  date                 = {2015},
  journaltitle         = {Electronic Journal of Applied Statistical Analysis},
  title                = {Double clustering for rating mutual funds},
  number               = {1},
  pages                = {44--56},
  url                  = {http://siba-ese.unisalento.it/index.php/ejasa/article/view/13761},
  volume               = {8},
  abstract             = {Due to the increasing proliferation of mutual funds, in-depth evaluation of the available products for portfolio selection purposes is a difficult task. Hence, classiffication schemes giving quick information about which funds are worth to be monitored, are often provided. The aim of this work is to show an application of clustering methods to the mutual funds historical data. Starting from the monthly time series of the Net Asset Values of a specific style-based category, namely the Large Blend US mutual funds, we apply distance-based clustering methods twice on a set of return, risk and performance measures: firstly, with the aim of reducing data dimension, and secondly to cluster funds in homogeneous classes. The adopted procedure claims the feature of producing a partition of funds that are readily interpretable from a financial point of view and it is further possible to rank the identified groups, thus obtaining a rating of funds that turns out to account for different propensities toward the risk exposure.},
  citeulike-article-id = {14367329},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-06-02 22:26:50},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lisi-Otranto-2010,
  author       = {Francesco Lisi and Edoardo Otranto},
  date         = {2010},
  journaltitle = {Mathematical and Statistical Methods for Actuarial Sciences and Finance},
  title        = {Clustering mutual funds by return and risk levels},
  url          = {https://link.springer.com/chapter/10.1007/978-88-470-1481-7_19},
  abstract     = {Mutual funds classifications, often made by rating agencies, are very common and sometimes criticised. In this work, a three-step statistical procedure for mutual funds classification is proposed. In the first step fund time series are characterised in terms of returns. In the second step, a clustering analysis is performed in order to obtain classes of homogeneous funds with respect to the risk levels. In particular, the risk is defined starting from an Asymmetric Threshold-GARCH model aimed to describe minimum, normal and turmoil risk. The third step merges the previous two. An application to 75 European funds belonging to 5 different categories is presented.},
  timestamp    = {2020-02-27 04:01},
}

@Article{Li-Zhu-2016,
  author               = {Li, Jinghua and Zhu, Dunlin},
  date                 = {2016-06},
  journaltitle         = {IET Renewable Power Generation},
  title                = {Combination of moment-matching, Cholesky and clustering methods to approximate discrete probability distribution of multiple wind farms},
  doi                  = {10.1049/iet-rpg.2015.0568},
  issn                 = {1752-1416},
  abstract             = {This study focuses on approximating a reduced discrete probability distribution (RDPD) of wind power from the original discrete probability distribution (ODPD), consisting of a large number of observed original scenarios (OSs), to relieve the burden of solving stochastic programs of wind power generation. The proposed method, namely, the MMCC method, aims to achieve high approximation accuracy and computational efficiency by combining an improved moment-matching (MM) method with the clustering (C) method and the Cholesky decomposition (CD) method. First, the C method is used to reduce the number of OSs by minimising the space distance between the reduced scenarios (RSs) and the OSs. Next, the CD method is used to rectify the correlation of the RSs to satisfy that of the ODPD. Finally, the RS probabilities are optimally determined by the MM method in order to minimise the stochastic features (first four moments and correlation matrix) between the RDPD and the ODPD. Simulations of RDPD approximation for three wind farms with 10, 20, 40, 60, 80, and 100 scenarios were carried out using the Latin hypercube sampling, importance sampling, C, moment-matching-clustering (MMC), and MMCC methods. The results showed that the MMCC method exhibits the best performance in terms of capturing the features of the ODPD.},
  citeulike-article-id = {14171142},
  citeulike-linkout-0  = {http://dx.doi.org/10.1049/iet-rpg.2015.0568},
  day                  = {13},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:11:06},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lorimer-et-al-2017,
  author         = {Lorimer, Tom and Held, Jenny and Stoop, Ruedi},
  date           = {2017-06-28},
  journaltitle   = {Philos Transact A Math Phys Eng Sci},
  title          = {Clustering: how much bias do we need?},
  doi            = {10.1098/rsta.2016.0293},
  number         = {2096},
  volume         = {375},
  abstract       = {Scientific investigations in medicine and beyond increasingly require observations to be described by more features than can be simultaneously visualized. Simply reducing the dimensionality by projections destroys essential relationships in the data. Similarly, traditional clustering algorithms introduce data bias that prevents detection of natural structures expected from generic nonlinear processes. We examine how these problems can best be addressed, where in particular we focus on two recent clustering approaches, Phenograph and Hebbian learning clustering, applied to synthetic and natural data examples. Our results reveal that already for very basic questions, minimizing clustering bias is essential, but that results can benefit further from biased post-processing.This article is part of the themed issue 'Mathematical methods in medicine: neuroscience, cardiology and pathology'.},
  day            = {28},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5434083},
  pmid           = {28507238},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lu-et-al-2018a,
  author         = {Lu, Ya-Nan and Li, Sai-Ping and Zhong, Li-Xin and Jiang, Xiong-Fei and Ren, Fei},
  date           = {2018-12},
  journaltitle   = {Chaos, Solitons \& Fractals},
  title          = {A clustering-based portfolio strategy incorporating momentum effect and market trend prediction},
  doi            = {10.1016/j.chaos.2018.10.012},
  issn           = {0960-0779},
  pages          = {1--15},
  volume         = {117},
  abstract       = {Abstract The hierarchical clustering algorithm has been proved useful in portfolio investment, which is one of the hottest issues in finance. In our new portfolio strategy, central, peripheral and dispersed portfolios constructed from clusters detected using unweighted and weighted modularity are compared according to their past performances, and the optimal portfolio is used in the investment period only if the market index return predicted by the LR, WMA or BP models is positive to avoid losses when the market drops. Our strategy is tested using the daily data of Chinese A-share market from January 4, 2008 and December 31, 2016, and the average investment return during different moving investment periods and 200 repeated runs is calculated. We find that although incorporating dispersed portfolio into our strategy has no significant effect in raising the investment return, it shows a similar performance as the peripheral portfolio, and the strategy constructed using unweighted modularity generally outperforms its counterpart by using weighted modularity. In addition, the market trend prediction can refine the investment return of our strategy. In brief, the strategy constructed using the BP model and unweighted modularity has the best investment return, which also outperforms the Markowitz portfolio.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lu-Wan-2013,
  author               = {Lu, Yonggang and Wan, Yi},
  date                 = {2013-05},
  journaltitle         = {Pattern Recognition},
  title                = {PHA: A fast potential-based hierarchical agglomerative clustering method},
  doi                  = {10.1016/j.patcog.2012.11.017},
  issn                 = {0031-3203},
  number               = {5},
  pages                = {1227--1239},
  volume               = {46},
  abstract             = {A novel potential-based hierarchical agglomerative (PHA) clustering method is proposed. In this method, we first construct a hypothetical potential field of all the data points, and show that this potential field is closely related to nonparametric estimation of the global probability density function of the data points. Then we propose a new similarity metric incorporating both the potential field which represents global data distribution information and the distance matrix which represents local data distribution information. Finally we develop another equivalent similarity metric based on an edge weighted tree of all the data points, which leads to a fast agglomerative clustering algorithm with time complexity O(N2). The proposed PHA method is evaluated by comparing with six other typical agglomerative clustering methods on four synthetic data sets and two real data sets. Experiments show that it runs much faster than the other methods and produces the most satisfying results in most cases. A potential field is used to represent global data distribution information. Both local and global data distribution information are used in the clustering. Designed an efficient hierarchical clustering method based on an edge-weighted tree. The potential field can be viewed as an estimated probability density function. PHA usually produces more satisfying results in much less time than other methods.},
  citeulike-article-id = {14148672},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2012.11.017},
  owner                = {cristi},
  posted-at            = {2016-09-28 22:54:06},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Mackrechi-2016,
  author               = {Makrehchi, Masoud},
  booktitle            = {IEEE/WIC/ACM International Conference on Web Intelligence (WI)},
  date                 = {2016-10},
  title                = {Hierarchical Agglomerative Clustering Using Common Neighbours Similarity},
  doi                  = {10.1109/wi.2016.0093},
  isbn                 = {978-1-5090-4470-2},
  location             = {Omaha, NE, USA},
  pages                = {546--551},
  publisher            = {IEEE},
  abstract             = {Hierarchical clustering has been well-studied in the community of machine learning. Hierarchical clustering algorithms are deterministic, stable, and do not need a pre-determined number of clusters as input. However, they are not scalable for very large data due to their non-linear complexity. In this paper, a new approach is proposed to reduce the complexity of Hierarchical Clustering, improve the purity of the clustering algorithm, and reduce the chaining factor. The proposed method has the following components: (i) A new combination similarity based on common-neighbours of graph theory is proposed, (ii) In every iteration, instead of calculating the centroids for new clusters, new centroids are estimated from centroids in previous iteration, and (iii) In each iteration, instead of merging only one pair of objects, multiple pairs are merged at the same time. In addition to the proposed combination similarity, four well-known methods including centroid-based, group-based, complete-link, and single-link, have been also implemented. All five methods are tested and evaluated using two metrics: purity and imbalance or chaining factor. We show that our proposed algorithm outperforms other classic methods.},
  citeulike-article-id = {14335040},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/wi.2016.0093},
  posted-at            = {2017-04-10 12:18:55},
  timestamp            = {2020-02-27 04:01},
}

@Book{Maharaj-et-al-2019,
  author    = {Elizabeth Ann Maharaj and Pierpaolo {D'Urso} and Jorge Caiado},
  date      = {2019},
  title     = {Time Series Clustering and Classification},
  publisher = {CRC Press},
  url       = {https://www.crcpress.com/Time-Series-Clustering-and-Classification/Maharaj-DUrso-Caiado/p/book/9781498773218},
  timestamp = {2020-02-27 04:01},
}

@Article{Marbac-Sedki-2017,
  author         = {Marbac, Matthieu and Sedki, Mohammed},
  date           = {2017-07},
  journaltitle   = {Statistics and Computing},
  title          = {Variable selection for model-based clustering using the integrated complete-data likelihood},
  doi            = {10.1007/s11222-016-9670-1},
  issn           = {0960-3174},
  number         = {4},
  pages          = {1049--1063},
  volume         = {27},
  abstract       = {Variable selection in cluster analysis is important yet challenging. It can be achieved by regularization methods, which realize a trade-off between the clustering accuracy and the number of selected variables by using a lasso-type penalty. However, the calibration of the penalty term can suffer from criticisms. Model selection methods are an efficient alternative, yet they require a difficult optimization of an information criterion which involves combinatorial problems. First, most of these optimization algorithms are based on a suboptimal procedure (e.g. stepwise method). Second, the algorithms are often computationally expensive because they need multiple calls of EM algorithms. Here we propose to use a new information criterion based on the integrated complete-data likelihood. It does not require the maximum likelihood estimate and its maximization appears to be simple and computationally efficient. The original contribution of our approach is to perform the model selection without requiring any parameter estimation. Then, parameter inference is needed only for the unique selected model. This approach is used for the variable selection of a Gaussian mixture model with conditional independence assumed. The numerical experiments on simulated and benchmark datasets show that the proposed method often outperforms two classical approaches for variable selection. The proposed approach is implemented in the R package VarSelLCM available on CRAN.},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood},
  timestamp      = {2020-02-27 04:01},
}

@InCollection{Markovic-et-al-2019,
  author         = {Markovic, Ivana P. and Stankovic, Jelena Z. and Stojanovic, Milos B. and Stankovic, Jovica M.},
  booktitle      = {ICT innovations 2019. big data processing and mining: 11th international conference, ICT innovations 2019, ohrid, north macedonia, october 17-19, 2019, proceedings},
  date           = {2019},
  title          = {A Hybrid Model for Financial Portfolio Optimization Based on LS-SVM and a Clustering Algorithm},
  doi            = {10.1007/978-3-030-33110-8\_15},
  editor         = {Gievska, Sonja and Madjarov, Gjorgji},
  isbn           = {978-3-030-33109-2},
  pages          = {173--186},
  publisher      = {Springer International Publishing},
  series         = {Communications in computer and information science},
  url            = {http://link.springer.com/10.1007/978-3-030-33110-8\_15},
  urldate        = {2019-12-14},
  volume         = {1110},
  abstract       = {An investment decision is one of the most important financial decisions. With the aim of optimizing investment in securities from the aspect of return and risk, investors usually diversify their portfolio securities. This paper presents a hybrid model for portfolio optimization, which consist of two steps. The first step predicts future returns on the shares, and the second step, by applying hierarchical clustering algorithm, identifies various groups of shares. The test results indicate that the suggested model is suitable for optimization of a financial portfolio as a hybrid model based on selected shares, which if included in the portfolio, enable the diversification of risk.},
  f1000-projects = {QuantInvest},
  issn           = {1865-0929},
  timestamp      = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2016-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Clustering Financial Time Series: How Long is Enough?},
  eprint               = {1603.04017},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.04017},
  abstract             = {Researchers have used from 30 days to several years of daily returns as source data for clustering financial time series based on their correlations. This paper sets up a statistical framework to study the validity of such practices. We first show that clustering correlated random variables from their observed values is statistically consistent. Then, we also give a first empirical answer to the much debated question: How long should the time series be? If too short, the clusters found can be spurious; if too long, dynamics can be smoothed out.},
  citeulike-article-id = {13987284},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.04017},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.04017},
  day                  = {13},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-03-24 14:12:20},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016a,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  date                 = {2016-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {On clustering financial time series: a need for distances between dependent random variables},
  eprint               = {1603.07822},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.07822},
  abstract             = {The following working document summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimate correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14147455},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.07822},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.07822},
  day                  = {25},
  owner                = {cristi},
  posted-at            = {2016-09-27 19:10:13},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016c,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2016-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering},
  eprint               = {1610.09659},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.09659},
  abstract             = {We propose a methodology to explore and measure the pairwise correlations that exist between variables in a dataset. The methodology leverages copulas for encoding dependence between two variables, state-of-the-art optimal transport for providing a relevant geometry to the copulas, and clustering for summarizing the main dependence patterns found between the variables. Some of the clusters centers can be used to parameterize a novel dependence coefficient which can target or forget specific dependence patterns. Finally, we illustrate and benchmark the methodology on several datasets. Code and numerical experiments are available online for reproducible research.},
  citeulike-article-id = {14291484},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.09659},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.09659},
  day                  = {30},
  posted-at            = {2017-03-03 18:30:47},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Marti-et-al-2017a,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  booktitle            = {Computational Information Geometry},
  date                 = {2017},
  title                = {On Clustering Financial Time Series: A Need for Distances Between Dependent Random Variables},
  doi                  = {10.1007/978-3-319-47058-0\_8},
  editor               = {Nielsen, Frank and Critchley, Frank and Dodson, Christopher T. J.},
  pages                = {149--174},
  publisher            = {Springer International Publishing},
  series               = {Signals and Communication Technology},
  abstract             = {This artilce summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimated correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14324929},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-47058-08},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-47058-08},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-03-30 22:23:54},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2019,
  author               = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  date                 = {2019-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  eprinttype           = {arXiv},
  url                  = {https:://arxiv.org/abs/1703.00485},
  urldate              = {2018-10-07},
  abstract             = {This document is a preliminary version of an in-depth review on the state of the art of clustering financial time series and the study of correlation networks. This preliminary document is intended for researchers in this field so that they can feedback to allow amendments, corrections and addition of new material unknown to the authors of this review. The aim of the document is to gather in one place the relevant material that can help the researcher in the field to have a bigger picture, the quantitative researcher to play with this alternative modeling of the financial time series, and the decision maker to leverage the insights obtained from these methods. We hope that this document will form a basis for implementation of an open toolbox of standard tools to study correlations, hierarchies, networks and clustering in financial markets. We also plan to maintain pointers to online material and an updated version of this work at www.datagrapple.com/Tech.},
  citeulike-article-id = {14291433},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.00485},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.00485},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  posted-at            = {2017-03-03 16:58:42},
  timestamp            = {2020-02-27 04:01},
}

@Article{Micciche-et-al-2005,
  author               = {Micciche, S. and Lillo, F. and Mantegna, R. N.},
  date                 = {2005-09},
  journaltitle         = {Workshop of the International School of Solid State Physics},
  title                = {Correlation Based Hierarchical Clustering in Financial Time Series},
  doi                  = {10.1142/9789812701558\_0037},
  pages                = {327--335},
  abstract             = {Abstract We review a correlation based clustering procedure applied to a portfolio of assets synchronously traded in a financial market. The portfolio considered consists of the set of 500 highly capitalized stocks traded at the New York Stock Exchange during the time period 1987-1998. We show that meaningful economic information can be extracted from correlation matrices.},
  booktitle            = {Complexity, Metastability and Nonextensivity},
  citeulike-article-id = {14150002},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/97898127015580037},
  citeulike-linkout-1  = {http://adsabs.harvard.edu/cgi-bin/nph-bibquery?bibcode=2005cmn..conf..327M},
  citeulike-linkout-2  = {http://www.worldscientific.com/doi/abs/10.1142/97898127015580037},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:38:57},
  publisher            = {WORLD SCIENTIFIC},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Mitra-et-al-2019,
  author         = {Mitra, Arup and Das, Abhra and Goswami, Saptarsi and Mustafi, Joy and Jalan, A. K.},
  booktitle      = {Computational intelligence, communications, and business analytics: second international conference, CICBA 2018, kalyani, india, july 27-28, 2018, revised selected papers, part II},
  date           = {2019},
  title          = {Portfolio management by time series clustering using correlation for stocks},
  doi            = {10.1007/978-981-13-8581-0\_11},
  editor         = {Mandal, Jyotsna Kumar and Mukhopadhyay, Somnath and Dutta, Paramartha and Dasgupta, Kousik},
  isbn           = {978-981-13-8580-3},
  location       = {Singapore},
  pages          = {134--144},
  publisher      = {Springer Singapore},
  series         = {Communications in computer and information science},
  url            = {http://link.springer.com/10.1007/978-981-13-8581-0\_11},
  urldate        = {2019-12-14},
  volume         = {1031},
  abstract       = {Investment diversification and portfolio building has been a great interest for share market investors, so as to minimize risk and maximize profit in a sensitive stock market. This paper gives an inside view of application of clustering for grouping 79 stocks (NSE), which can be used to build a diversified portfolio. Manually trying out different groupings to diversify portfolio is a computationally expensive task. In this paper, the closing price, time series of the stocks have been considered. Common effect due to market has been discounted using partial correlation, and a correlation based dissimilarity measure has been used for clustering. An equal investment strategy has been adopted to compare the portfolio performance with SENSEX. The empirical results of the portfolios have been studied and presented in details.},
  f1000-projects = {QuantInvest},
  issn           = {1865-0929},
  timestamp      = {2020-02-27 04:01},
}

@Article{Montero-Vilar-2015,
  author       = {Pablo Montero and Jose A. Vilar},
  date         = {2015},
  journaltitle = {Journal of Statistical Software},
  title        = {TSclust: An R Package for Time Series Clustering},
  url          = {https://www.jstatsoft.org/article/view/v062i01},
  volume       = {62},
  abstract     = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
  groups       = {FrcstQWIM_TimeSrs},
  timestamp    = {2020-02-27 04:01},
}

@Article{Muca-et-al-2015,
  author               = {Muca, Markela and Kutrolli, Gleda and Kutrolli, Maksi},
  date                 = {2015},
  journaltitle         = {European Scientific Journal},
  title                = {A proposed algorithm for determining the optimal number of clusters},
  url                  = {https://eujournal.org/index.php/esj/article/view/6756},
  abstract             = {Data clustering is a data exploration technique that allows objects with similar characteristics to be grouped together in order to facilitate their further processing. The K-means algorithm is a popular data-clustering algorithm. However, one of its drawbacks is the requirement for the number of clusters, K, to be specified before the algorithm is applied. This paper first reviews existing methods for selecting the number of clusters for the algorithm. Factors that affect this selection are then discussed and an improvement of the existing k-means algorithm to assist the selection is proposed. The paper concludes with an analysis of the results of using cluster validation referring to some measures that are classified as internal and external indexes to determine the optimal number of clusters for the K-means algorithm. There are applied some stopping criterion referring to those indexes for evaluating a clustering against a gold standard.},
  citeulike-article-id = {14435139},
  posted-at            = {2017-09-21 00:35:42},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2011,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2011-04},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Methods of Hierarchical Clustering},
  eprint               = {1105.0121},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1105.0121},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
  citeulike-article-id = {10504808},
  citeulike-linkout-0  = {http://arxiv.org/abs/1105.0121},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1105.0121},
  day                  = {30},
  groups               = {Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:36:48},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2012,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2012-01},
  journaltitle         = {WIREs Data Mining Knowl Discov},
  title                = {Algorithms for hierarchical clustering: an overview},
  doi                  = {10.1002/widm.53},
  number               = {1},
  pages                = {86--97},
  volume               = {2},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
  citeulike-article-id = {13987836},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/widm.53},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-03-25 04:22:46},
  publisher            = {John Wiley and Sons, Inc.},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2017,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2017-11},
  journaltitle         = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  title                = {Algorithms for hierarchical clustering: an overview, II},
  doi                  = {10.1002/widm.1219},
  issn                 = {1942-4787},
  number               = {6},
  pages                = {e1219--n/a},
  volume               = {7},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. This review adds to the earlier version, Murtagh F, Contreras P. Algorithms for hierarchical clustering: an overview, Wiley Interdiscip Rev: Data Mining Knowl Discov 2012, 2, 86-97. WIREs Data Mining Knowl Discov 2017, 7:e1219. doi: 10.1002/widm.1219 For further resources related to this article, please visit the WIREs website.},
  citeulike-article-id = {14449844},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/widm.1219},
  day                  = {1},
  journal              = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  posted-at            = {2017-11-23 20:46:27},
  publisher            = {Wiley Periodicals, Inc},
  timestamp            = {2020-02-27 04:01},
  year                 = {2017},
}

@Article{Murtagh-Legendre-2014,
  author               = {Murtagh, Fionn and Legendre, Pierre},
  date                 = {2014-11},
  journaltitle         = {Journal of Classification},
  title                = {Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward's Criterion?},
  doi                  = {10.1007/s00357-014-9161-z},
  number               = {3},
  pages                = {274--295},
  volume               = {31},
  abstract             = {The Ward error sum of squares hierarchical clustering method has been very widely used since its first description by Ward in a 1963 publication. It has also been generalized in various ways. Two algorithms are found in the literature and software, both announcing that they implement the Ward clustering method. When applied to the same distance matrix, they produce different results. One algorithm preserves Ward's criterion, the other does not. Our survey work and case studies will be useful for all those involved in developing software for data analysis using Ward's hierarchical clustering method.},
  citeulike-article-id = {14482080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00357-014-9161-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00357-014-9161-z},
  posted-at            = {2017-11-23 20:38:36},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:01},
  year                 = {2014},
}

@Article{Narabin-Boongasame-2018,
  author       = {Santit Narabin and Laor Boongasame},
  date         = {2018},
  journaltitle = {International Conference on Big Data and Artificial Intelligence},
  title        = {A Cluster Analysis of Mutual Funds Data},
  url          = {https://ieeexplore.ieee.org/abstract/document/8546679},
  abstract     = {The factors of clustering mutual fund (such as Net Asset Value (NAV)) do not direct to both return and risk of mutual funds which they are important factors for investors. This research helps an investor can estimate profit and loss rate of the mutual fund in his/her portfolio by using the net asset value change ratios (NAVCR). Then, both the NAVCR and value of each mutual fund will be used for clustering. For building a portfolio, the mutual funds could be selected from the diversified groups in order to reduce risk. The mutual fund data at different times from the set for the fiscal year 2010 - 2017 are used. The results of our analysis show that our models offer significantly better performance than the portfolio management model derived from the random portfolio management.},
  timestamp    = {2020-02-27 04:01},
}

@Article{Nguyen-et-al-2017a,
  author         = {Nguyen, Hien D. and McLachlan, Geoffrey J. and Orban, Pierre and Bellec, Pierre and Janke, Andrew L.},
  date           = {2017-04},
  journaltitle   = {Neural Computation},
  title          = {Maximum Pseudolikelihood Estimation for Model-Based Clustering of Time Series Data.},
  doi            = {10.1162/{NECO\_a\_00938}},
  number         = {4},
  pages          = {990--1020},
  volume         = {29},
  abstract       = {Mixture of autoregressions (MoAR) models provide a model-based approach to the clustering of time series data. The maximum likelihood (ML) estimation of MoAR models requires evaluating products of large numbers of densities of normal random variables. In practical scenarios, these products converge to zero as the length of the time series increases, and thus the ML estimation of MoAR models becomes infeasible without the use of numerical tricks. We propose a maximum pseudolikelihood (MPL) estimation approach as an alternative to the use of numerical tricks. The MPL estimator is proved to be consistent and can be computed with an EM (expectation-maximization) algorithm. Simulations are used to assess the performance of the MPL estimator against that of the ML estimator in cases where the latter was able to be calculated. An application to the clustering of time series data arising from a resting state fMRI experiment is presented as a demonstration of the methodology.},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood},
  pmid           = {28095191},
  timestamp      = {2020-02-27 04:01},
}

@Article{Nie-2017,
  author               = {Nie, Chun-Xiao},
  date                 = {2017-06},
  journaltitle         = {Chaos, Solitons and Fractals},
  title                = {Dynamics of cluster structure in financial correlation matrix},
  doi                  = {10.1016/j.chaos.2017.05.039},
  issn                 = {0960-0779},
  abstract             = {The relationship between the correlation dimension of the financial market and the cluster structure in the correlation coefficient matrix is studied. Empirical results based on model data show that the clearer cluster structure corresponds to a smaller dimension. We use the algorithm to verify the relationship between the cluster structure of the real market data and the correlation dimension. The correlation dimensions in the financial market are calculated and used as a measure to study the cluster structure in the correlation coefficient matrix. First, based on the existing model, we present a toy model. Using the model-generated data, we find that the clearer cluster structure corresponds to a smaller dimension. It implies that the correlation dimension can be used as a measure of the cluster structure in the correlation coefficient matrix. Finally, we use the algorithm to compute the clusters in the real market and verify the previous empirical evidence. The results show that the cluster structure in the financial correlation coefficient matrix may change with time. The correlation dimension is smaller after the financial crisis, indicating that the cluster structure is clearer after the financial crisis.},
  citeulike-article-id = {14384085},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.chaos.2017.05.039},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-06-28 18:36:06},
  timestamp            = {2020-02-27 04:01},
}

@Article{Nie-2017a,
  author               = {Nie, Chun-Xiao},
  date                 = {2017-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Cluster structure in the correlation coefficient matrix can be characterized by abnormal eigenvalues},
  doi                  = {10.1016/j.physa.2017.09.066},
  issn                 = {0378-4371},
  abstract             = {n a large number of previous studies, the researchers found that some of the eigenvalues of the financial correlation matrix were greater than the predicted values of the random matrix theory (). Here, we call these eigenvalues as anomalous eigenvalues. In order to reveal the hidden meaning of these anomalous eigenvalues, we study the toy model with cluster structure and find that these eigenvalues are related to the cluster structure of the correlation coefficient matrix. In this paper, model-based experiments show that in most cases, the number of anomalous eigenvalues of the correlation matrix is equal to the number of clusters. In addition, empirical studies show that the sum of the anomalous eigenvalues is related to the clarity of the cluster structure and is negatively correlated with the correlation dimension.},
  citeulike-article-id = {14444598},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.09.066},
  posted-at            = {2017-10-03 13:16:43},
  timestamp            = {2020-02-27 04:01},
}

@Article{Nieminen-et-al-2013,
  author               = {Nieminen, Paavo and Polonen, Ilkka and Sipola, Tuomo},
  date                 = {2013-10},
  journaltitle         = {Journal of Informetrics},
  title                = {Research literature clustering using diffusion maps},
  doi                  = {10.1016/j.joi.2013.08.004},
  issn                 = {1751-1577},
  number               = {4},
  pages                = {874--886},
  volume               = {7},
  abstract             = {Adapting knowledge discovery process to scientometrics. Diffusion map-based clustering framework introduced to scientometrics. Structure of data mining literature revealed as a case study. We apply the knowledge discovery process to the mapping of current topics in a particular field of science. We are interested in how articles form clusters and what are the contents of the found clusters. A framework involving web scraping, keyword extraction, dimensionality reduction and clustering using the diffusion map algorithm is presented. We use publicly available information about articles in high-impact journals. The method should be of use to practitioners or scientists who want to overview recent research in a field of science. As a case study, we map the topics in data mining literature in the year 2011.},
  citeulike-article-id = {14212401},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.joi.2013.08.004},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:04:14},
  timestamp            = {2020-02-27 04:01},
}

@Article{Ning-et-al-2015,
  author               = {Ning, Cathy and Xu, Dinghai and Wirjanto, Tony S.},
  date                 = {2015-03},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Is volatility clustering of asset returns asymmetric?},
  doi                  = {10.1016/j.jbankfin.2014.11.016},
  issn                 = {0378-4266},
  pages                = {62--76},
  volume               = {52},
  abstract             = {We investigate the structure of volatility clustering of asset returns. We employ copula-based univariate time-series models and realized kernel volatility. We find that volatility clustering is highly nonlinear and strongly asymmetric. Our paper is the first one that models and finds asymmetric nonlinear volatility clustering. This finding is important in asset pricing, derivatives pricing, and risk management. Volatility clustering is a well-known stylized feature of financial asset returns. This paper investigates asymmetric pattern in volatility clustering by employing a univariate copula approach of Chen and Fan (2006). Using daily realized kernel volatilities constructed from high frequency data from stock and foreign exchange markets, we find evidence that volatility clustering is highly nonlinear and strongly asymmetric in that clusters of high volatility occur more often than clusters of low volatility. To the best of our knowledge, this paper is the first one to address and uncover this phenomenon. In particular, the asymmetry in volatility clustering is found to be more pronounced in the stock markets than in the foreign exchange markets. Further, the volatility clusters are shown to remain persistent for over a month and asymmetric across different time periods. Our findings have important implications for risk management. A simulation study indicates that models which accommodate asymmetric volatility clustering can significantly improve the out-of-sample forecasts of Value-at-Risk},
  citeulike-article-id = {14313629},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2014.11.016},
  groups               = {Vol_Cluster},
  posted-at            = {2017-03-19 03:08:58},
  timestamp            = {2020-02-27 04:01},
}

@Article{NoelKoide-2016a,
  author               = {Noel-Koide, Kevin},
  date                 = {2016-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Asset Allocation and Intra-Sector Allocation Using Covariance and Precision Matrix Clustering},
  url                  = {https://ssrn.com/abstract=2745727},
  abstract             = {We investigate simply the usage of clustering method by inverse covariance estimation for asset allocation in finance. Allocation across various sectors of the market (i.e. sector ETF) is usually well understood through the usage of mean variance allocation. However, stocks inside a same sector (i.e. Biotechnology, IT,..) are little studied in the literature since high correlation between stocks leads to poor differentiation across stocks.Here, we apply covariance clustering method to study the behavior of one specific market sector: US listed biotechnology stocks.

We show that this methodology allows differentiating highly correlated stocks and it provides more detailed information inside this sector. Additionally, the use of higher frequency data (intraday data) allows refining the clustering method and characterizing further each stock. This method can give further insight to select stocks inside a same sector.},
  citeulike-article-id = {13978597},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2745727},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2745727code2498383.pdf?abstractid=2745727 and mirid=1},
  day                  = {11},
  groups               = {Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-03-12 22:40:18},
  timestamp            = {2020-02-27 04:01},
}

@Article{Otranto-Gargano-2015,
  author               = {Otranto, Edoardo and Gargano, Romana},
  date                 = {2015},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Financial clustering in presence of dominant markets},
  doi                  = {10.1007/s11634-014-0189-z},
  number               = {3},
  pages                = {315--339},
  volume               = {9},
  abstract             = {Clustering financial time series is a recent topic of statistical literature with important fields of applications, in particular portfolio composition and risk evaluation. The risk is generally linked to the volatility of the asset, but its level of predictability also plays a basic role in investment decisions. In particular, the classification of a certain asset could be linked to its dependence on the volatility of a dominant market: movements in the volatility of the dominant market can provide similar movements in the volatility of the asset and its predictability would depend on the strength of this dependence. Working in a model based framework, we base the classification of the volatility of an asset not only on its volatility level, but also on the presence of spillover effects from a dominant market, such as the US one, and on the similarity of the dynamics of the asset and the dominant market. The method is carried out using an extended version of the Multiplicative Error Model and is applied to a set of European assets, also performing a historical simulation experiment.},
  citeulike-article-id = {14014267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-014-0189-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-014-0189-z},
  groups               = {Networks and investment management, Clustering and network analysis, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-04-17 16:15:18},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 04:01},
}

@InProceedings{Palupi-et-al-2019,
  author         = {Palupi, Irma and Wahyudi, Bambang Ari and Indwiarti, Indwiarti},
  booktitle      = {7th International Conference on Information and Communication Technology (ICoICT)},
  date           = {2019-07-24},
  title          = {The clustering algorithms approach for decision efficiency in investment portfolio diversification},
  doi            = {10.1109/{ICoICT}.2019.8835314},
  isbn           = {978-1-5386-8052-0},
  pages          = {1--6},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8835314/},
  urldate        = {2019-12-03},
  abstract       = {This paper performs a clustering algorithm for portfolio investment diversification. The clustering process is applied to choose the preferred assets among hundreds of assets provided in the market under the related features. This work experimentally provides four features as the coordinate of assets that are mean, variance, skewness, and kurtosis of the returns. The used data is the daily close price of 175 assets in Indonesian exchange (IDX). We perform 4 clustering algorithms to locate the assets that have the same similarity into the same cluster. Since the four features are being considered, the spherical-shape of data is difficult to observe. In fact, the portfolio return of all algorithm's outcome show the Agglomerative and DBScan algorithm yield higher performance evaluation with no dominant asset included. Assets representatives from each cluster are determined to be in the portfolio formation. By using the portfolio theory by Markowitz, i.e Mean-variance optimization (MVO) and Sharp ration optimization, the proportion of contained assets are computed, and we test their performance by applying the formation into the market data a month after as a testing data. Several interesting results are provided.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Paparrizos-Gravano-2017,
  author               = {Paparrizos, John and Gravano, Luis},
  date                 = {2017-06},
  journaltitle         = {ACM Transactions on Database Systems},
  title                = {Fast and Accurate Time-Series Clustering},
  doi                  = {10.1145/3044711},
  issn                 = {0362-5915},
  number               = {2},
  volume               = {42},
  abstract             = {The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k-Shape and k-MultiShapes (k-MS), two novel algorithms for time-series clustering. k-Shape and k-MS rely on a scalable iterative refinement procedure. As their distance measure, k-Shape and k-MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k-Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k-MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k-Shape, and k-MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k-Shape and k-MS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable methods in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable approaches, with one exception, namely k-medoids with DTW, which achieves similar accuracy. However, unlike k-Shape, this approach requires tuning of its distance measure and is significantly slower than k-Shape. k-MS performs similarly to k-Shape in comparison to rival methods, but k-MS is significantly more accurate than k-Shape. Beyond clustering, we demonstrate the effectiveness of k-Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k-Shape, and k-MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications.},
  citeulike-article-id = {14435136},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3086510.3044711},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3044711},
  groups               = {Scenario_TimeSeries},
  location             = {New York, NY, USA},
  posted-at            = {2017-09-21 00:10:48},
  publisher            = {ACM},
  timestamp            = {2020-02-27 04:04},
}

@Article{Park-2020,
  author         = {Park, Jinwoo},
  date           = {2020-01-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Approaches for Global Minimum Variance Portfolio},
  url            = {https://arxiv.org/abs/2001.02966v2},
  urldate        = {2020-01-17},
  abstract       = {The only input to attain the portfolio weights of global minimum variance portfolio (GMVP) is the covariance matrix of returns of assets being considered for investment. Since the population covariance matrix is not known, investors use historical data to estimate it. Even though sample covariance matrix is an unbiased estimator of the population covariance matrix, it includes a great amount of estimation error especially when the number of observed data is not much bigger than number of assets. As it is difficult to estimate the covariance matrix with high dimensionality all at once, clustering stocks is proposed to come up with covariance matrix in two steps: firstly, within a cluster and secondly, between clusters. It decreases the estimation error by reducing the number of features in the data matrix. The motivation of this dissertation is that the estimation error can still remain high even after clustering, if a large amount of stocks is clustered together in a single group. This research proposes to utilize a bounded clustering method in order to limit the maximum cluster size. The result of experiments shows that not only the gap between in-sample volatility and out-of-sample volatility decreases, but also the out-of-sample volatility gets reduced. It implies that we need a bounded clustering algorithm so that maximum clustering size can be precisely controlled to find the best portfolio performance.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Pasteris-et-al-2018,
  author         = {Pasteris, Stephen and Vitale, Fabio and Gentile, Claudio and Herbster, Mark},
  date           = {2018-04-09},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {On Similarity Prediction and Pairwise Clustering},
  url            = {http://proceedings.mlr.press/v83/pasteris18a.html},
  urldate        = {2019-09-15},
  abstract       = {We consider the problem of clustering a finite set of items from pairwise similarity information. Unlike what is done in the literature on this subject, we do so in a passive learning setting, and with no specific constraints on the cluster shapes other than their size. We investigate the problem in different settings: i. an online setting, where we provide a tight characterization of the prediction complexity in the mistake bound model, and ii. a standard stochastic batch setting, where we give tight upper and lower bounds on the achievable generalization error. Prediction performance is measured both in terms of the ability to recover the similarity function encoding the hidden clustering and in terms of how well we classify each item within the set. The proposed algorithms are time efficient.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Patel-Thakral-2016,
  author               = {Patel, K. M. Archana and Thakral, Prateek},
  booktitle            = {International Conference on Communication and Signal Processing (ICCSP)},
  date                 = {2016-04},
  title                = {The best clustering algorithms in data mining},
  doi                  = {10.1109/iccsp.2016.7754534},
  isbn                 = {978-1-5090-0396-9},
  location             = {Melmaruvathur, Tamilnadu, India},
  pages                = {2042--2046},
  publisher            = {IEEE},
  abstract             = {In data mining, Clustering is the most popular, powerful and commonly used unsupervised learning technique. It is a way of locating similar data objects into clusters based on some similarity. Clustering algorithms can be categorized into seven groups, namely Hierarchical clustering algorithm, Density-based clustering algorithm, Partitioning clustering algorithm, Graph-based algorithm, Grid-based algorithm, Model-based clustering algorithm and Combinational clustering algorithm. These clustering algorithms give different result according to the conditions. Some clustering techniques are better for large data set and some gives good result for finding cluster with arbitrary shapes. This paper is planned to learn and relates various data mining clustering algorithms. Algorithms which are under exploration as follows: K-Means algorithm, K-Medoids, Distributed K-Means clustering algorithm, Hierarchical clustering algorithm, Grid-based Algorithm and Density based clustering algorithm. This paper compared all these clustering algorithms according to the many factors. After comparison of these clustering algorithms I describe that which clustering algorithms should be used in different conditions for getting the best result.},
  citeulike-article-id = {14335037},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccsp.2016.7754534},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-04-10 12:14:30},
  timestamp            = {2020-02-27 04:04},
}

@Article{Pattarin-et-al-2004,
  author               = {Pattarin, Francesco and Paterlini, Sandra and Minerva, Tommaso},
  date                 = {2004-09},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Clustering financial time series: an application to mutual funds style analysis},
  doi                  = {10.1016/j.csda.2003.11.009},
  issn                 = {0167-9473},
  number               = {2},
  pages                = {353--372},
  volume               = {47},
  abstract             = {Classification can be useful in giving a synthetic and informative description of contexts characterized by high degrees of complexity. Different approaches could be adopted to tackle the classification problem: statistical tools may contribute to increase the degree of confidence in the classification scheme. A classification algorithm for mutual funds style analysis is proposed, which combines different statistical techniques and exploits information readily available at low cost. Objective, representative, consistent and empirically testable classification schemes are strongly sought for in this field in order to give reliable information to investors and fund managers who are interested in evaluating and comparing different financial products. Institutional classification schemes, when available, do not always provide consistent and representative peer groups of funds. A "return-based"classification scheme is proposed, which aims at identifying mutual funds' styles by analysing time series of past returns. The proposed classification procedure consists of three basic steps: (a) a dimensionality reduction step based on principal component analysis, (b) a clustering step that exploits a robust evolutionary clustering methodology, and (c) a style identification step via a constrained regression model first proposed by William Sharpe. The algorithm is tested on a sample of Italian mutual funds and achieves satisfactory results with respect to (i) the agreement with the existing institutional classification and (ii) the explanatory power of out of sample variability in the cross-section of returns.},
  citeulike-article-id = {14367330},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.csda.2003.11.009},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-06-02 22:29:31},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Peker-Aktan-2014,
  author               = {Peker, Sinem and Aktan, Bora},
  booktitle            = {The 8th International Scientific Conference "Business and Management 2014"},
  date                 = {2014},
  title                = {Clustering In European Stock Indices In Crisis And Non-Crisis Periods},
  doi                  = {10.3846/bm.2014.037},
  isbn                 = {9786094576508},
  location             = {Vilnius, Lithuania},
  publisher            = {Vilnius Gediminas Technical University Publishing House Technika},
  abstract             = {Grouping the major indices of stock markets based on their homogeneities may facilitate the selection period for investors especially today's information rich financial world. This paper attempts to detect and group the homogenous stock indices in Europe both throughout the crisis and non-crisis periods. The daily index returns of leading stock exchanges over the period 03.01.2007-09.04.2013 are considered; one of the hierarchical clustering techniques so-called Ward's Method is applied and similar cases are evaluated respectively. Then, Wilcoxon signed rank test is employed for the same periods on daily index returns and meaningful differences are found.},
  citeulike-article-id = {14149920},
  citeulike-linkout-0  = {http://dx.doi.org/10.3846/bm.2014.037},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:28:33},
  timestamp            = {2020-02-27 04:04},
}

@Article{Peng-et-al-2016,
  author               = {Peng, Chong and Kang, Zhao and Yang, Ming and Cheng, Qiang},
  date                 = {2016-07},
  journaltitle         = {IEEE Signal Processing Letters},
  title                = {Feature Selection Embedded Subspace Clustering},
  doi                  = {10.1109/lsp.2016.2573159},
  issn                 = {1070-9908},
  number               = {7},
  pages                = {1018--1022},
  volume               = {23},
  abstract             = {We propose a new subspace clustering method that integrates feature selection into subspace clustering. Rather than using all features to construct a low-rank representation of the data, we find such a representation using only relevant features, which helps in revealing more accurate data relationships. Two variants are proposed by using both convex and nonconvex rank approximations. Extensive experimental results confirm the effectiveness of the proposed method and models.},
  citeulike-article-id = {14351207},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/lsp.2016.2573159},
  posted-at            = {2017-05-05 01:22:20},
  timestamp            = {2020-02-27 04:04},
}

@Article{Piccardi-et-al-2011,
  author               = {Piccardi, Carlo and Calatroni, Lisa and Bertoni, Fabio},
  date                 = {2011-01},
  journaltitle         = {International Journal of Modern Physics C},
  title                = {Clustering financial time series by network community analysis},
  doi                  = {10.1142/s012918311101604x},
  number               = {01},
  pages                = {35--50},
  volume               = {22},
  abstract             = {In this paper, we describe a method for clustering financial time series which is based on community analysis, a recently developed approach for partitioning the nodes of a network (graph). A network with N nodes is associated to the set of N time series. The weight of the link (i, j), which quantifies the similarity between the two corresponding time series, is defined according to a metric based on symbolic time series analysis, which has recently proved effective in the context of financial time series. Then, searching for network communities allows one to identify groups of nodes (and then time series) with strong similarity. A quantitative assessment of the significance of the obtained partition is also provided. The method is applied to two distinct case-studies concerning the US and Italy Stock Exchange, respectively. In the US case, the stability of the partitions over time is also thoroughly investigated. The results favorably compare with those obtained with the standard tools typically used for clustering financial time series, such as the minimal spanning tree and the hierarchical tree.},
  citeulike-article-id = {14150069},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/s012918311101604x},
  citeulike-linkout-1  = {http://www.worldscientific.com/doi/abs/10.1142/S012918311101604X},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:30:18},
  publisher            = {World Scientific Publishing Co.},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ponta-Carbone-2019,
  author         = {Ponta, L. and Carbone, A.},
  date           = {2019-08-01},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Quantifying horizon dependence of asset prices: a cluster entropy approach},
  url            = {https://arxiv.org/abs/1908.00257},
  urldate        = {2019-08-11},
  abstract       = {Market dynamic is studied by quantifying the dependence of the entropy S(,n) of the clusters formed by the series of the prices pt and its moving average pt,n on temporal horizon M. We report results of the analysis performed on high-frequency data of the Nasdaq Composite, Dow Jones Industrial Avg and Standard \& Poor 500 indexes downloaded from the Bloomberg terminal this http URL. Both raw and sampled data series have been analysed for a broad range of horizons M, varying from one to twelve months over the year 2018. A systematic dependence of the cluster entropy function S(,n) on the horizon M has been evidenced in the analysed assets. Hence, the cluster entropy function is integrated over the cluster to yield a synthetic indicator of price evolution: the Market Dynamic Index I(M,n). Moreover, the Market Horizon Dependence defined as H(M,n)=I(M,n)-I(1,n) is calculated and compared with the values of the horizon dependence of the pricing kernel with different representative agent models obtained by a Kullback-Leibler entropy approach.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Raffinot-2018,
  author               = {Raffinot, Thomas},
  date                 = {2017-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Hierarchical Clustering-Based Asset Allocation},
  doi                  = {10.3905/jpm.2018.44.2.089},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {89--99},
  volume               = {44},
  abstract             = {This article proposes a hierarchical clustering-based asset allocation method, which uses graph theory and machine learning techniques. Hierarchical clustering refers to the formation of a recursive clustering, suggested by the data, not defined a priori. Several hierarchical clustering methods are presented and tested. Once the assets are hierarchically clustered, the authors compute a simple and efficient capital allocation within and across clusters of assets, so that many correlated assets receive the same total allocation as a single uncorrelated one. The out-of-sample performances of hierarchical clustering-based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets, which differ in term of the number of assets and the assets' composition. To avoid data snooping, the authors assess the comparison of profit measures using the bootstrap-based model confidence set procedure. Their empirical results indicate that hierarchical clustering-based portfolios are robust and truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14510373},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.089},
  day                  = {22},
  groups               = {Network_Invest, ML_Network_QWIM, PortfOptim_Network, Invest_Network, ML_InvestSelect},
  posted-at            = {2017-12-30 13:27:26},
  timestamp            = {2020-02-27 04:04},
}

@Article{Rastelli-Friel-2017,
  author         = {Rastelli, Riccardo and Friel, Nial},
  date           = {2017-10-31},
  journaltitle   = {Statistics and Computing},
  title          = {Optimal Bayesian estimators for latent variable cluster models},
  doi            = {10.1007/s11222-017-9786-y},
  issn           = {0960-3174},
  pages          = {1--18},
  abstract       = {In cluster analysis interest lies in probabilistically capturing partitions of individuals, items or observations into groups, such that those belonging to the same group share similar attributes or relational profiles. Bayesian posterior samples for the latent allocation variables can be effectively obtained in a wide range of clustering models, including finite mixtures, infinite mixtures, hidden Markov models and block models for networks. However, due to the categorical nature of the clustering variables and the lack of scalable algorithms, summary tools that can interpret such samples are not available. We adopt a Bayesian decision theoretical approach to define an optimality criterion for clusterings and propose a fast and context-independent greedy algorithm to find the best allocations. One important facet of our approach is that the optimal number of groups is automatically selected, thereby solving the clustering and the model-choice problems at the same time. We consider several loss functions to compare partitions and show that our approach can accommodate a wide range of cases. Finally, we illustrate our approach on both artificial and real datasets for three different clustering models: Gaussian mixtures, stochastic block models and latent block models for networks.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Proba_Bayes},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ren-et-al-2016,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2016-08},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Dynamic portfolio strategy using clustering approach},
  eprint               = {1608.03058},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1608.03058},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. This paper proposes a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: selecting the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, i.e., degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion, then using the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index or the sum of the amplitudes of the trading days with rising index to the total number of trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that the peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all the possible optimal portfolio strategy based on different parameters to select portfolios and different criteria to identify market conditions, 65dollar; of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market and the proportion is 70dollar; for the Shenzhen A-Share market.},
  citeulike-article-id = {14148628},
  citeulike-linkout-0  = {http://arxiv.org/abs/1608.03058},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1608.03058},
  day                  = {10},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, Invest_Dynamic, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:56:42},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ren-et-al-2017,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2017-01},
  journaltitle         = {PLOS ONE},
  title                = {Dynamic Portfolio Strategy Using Clustering Approach},
  doi                  = {10.1371/journal.pone.0169299},
  number               = {1},
  pages                = {e0169299+},
  volume               = {12},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. We here propose a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: First, select the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, namely degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion. Second, use the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index to the total number of trading days, or the sum of the amplitudes of the trading days with rising index to the sum of the amplitudes of the total trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all possible optimal portfolio strategies based on different parameters to select portfolios and different criteria to identify market conditions, 65 percent of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market while the proportion is 70 percent for the Shenzhen A-Share market.},
  citeulike-article-id = {14291490},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0169299},
  day                  = {27},
  groups               = {Networks and investment management, Network_Invest, PortfOptim_Dynamic, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-03 18:42:12},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Article{Roick-et-al-2019,
  author         = {Roick, Tyler and Karlis, Dimitris and McNicholas, Paul D.},
  date           = {2019-01-26},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Discrete Valued Time Series},
  url            = {https://arxiv.org/abs/1901.09249},
  urldate        = {2019-03-07},
  abstract       = {There is a need for the development of models that are able to account for discreteness in data, along with its time series properties and correlation. Our focus falls on INteger-valued AutoRegressive (INAR) type models. The INAR type models can be used in conjunction with existing model-based clustering techniques to cluster discrete valued time series data. With the use of a finite mixture model, several existing techniques such as the selection of the number of clusters, estimation using expectation-maximization and model selection are applicable. The proposed model is then demonstrated on real data to illustrate its clustering applications.},
  day            = {26},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ronan-et-al-2018,
  author       = {Tom Ronan and Shawn Anastasio and Zhijie Qi and Pedro Henrique S. Vieira Tavares and Roman Sloutsky and Kristen M. Naegle},
  date         = {2018},
  journaltitle = {Journal of Machine Learning Research},
  title        = {OpenEnsembles: A Python Resource for Ensemble Clustering},
  number       = {26},
  pages        = {1--6},
  url          = {http://jmlr.org/papers/v19/18-100.html},
  volume       = {19},
  abstract     = {In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets. OpenEnsembles is released under the GNU General Public License version 3, can be installed via Conda or the Python Package Index (pip), and is available at https://github.com/NaegleLab/OpenEnsembles.},
  timestamp    = {2020-02-27 04:04},
}

@Article{Ross-2015a,
  author               = {Ross, Gordon J.},
  date                 = {2015-05},
  journaltitle         = {Physical Review E},
  title                = {Dynamic Multi-Factor Clustering of Financial Networks},
  doi                  = {10.1103/physreve.89.022809},
  eprint               = {1505.01550},
  eprinttype           = {arXiv},
  issn                 = {1539-3755},
  number               = {2},
  volume               = {89},
  abstract             = {We investigate the tendency for financial instruments to form clusters when there are multiple factors influencing the correlation structure. Specifically, we consider a stock portfolio which contains companies from different industrial sectors, located in several different countries. Both sector membership and geography combine to create a complex clustering structure where companies seem to first be divided based on sector, with geographical subclusters emerging within each industrial sector. We argue that standard techniques for detecting overlapping clusters and communities are not able to capture this type of structure, and show how robust regression techniques can instead be used to remove the influence of both sector and geography from the correlation matrix separately. Our analysis reveals that prior to the 2008 financial crisis, companies did not tend to form clusters based on geography. This changed immediately following the crisis, with geography becoming a more important determinant of clustering.},
  citeulike-article-id = {14150018},
  citeulike-linkout-0  = {http://arxiv.org/abs/1505.01550},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1505.01550},
  citeulike-linkout-2  = {http://dx.doi.org/10.1103/physreve.89.022809},
  day                  = {7},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 17:00:42},
  timestamp            = {2020-02-27 04:04},
}

@Article{Roy-et-al-2017,
  author               = {Roy, Aurko and Pokutta, Sebastian},
  date                 = {2017},
  journaltitle         = {Journal of Machine Learning Research},
  title                = {Hierarchical Clustering via Spreading Metrics},
  number               = {88},
  pages                = {1--35},
  url                  = {http://jmlr.org/papers/v18/17-081.html},
  volume               = {18},
  abstract             = {We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most O(alpha n log n)O(alpha n log n) times the cost of the optimal hierarchical clustering, where alpha is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao- Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O(log 3/2n)O(log 3/2n) times the cost of the optimal solution. We improve this by giving an O(log n)O(log n))-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)O(log n)- approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis.},
  citeulike-article-id = {14509660},
  citeulike-linkout-0  = {http://jmlr.org/papers/v18/17-081.html},
  keywords             = {*file-import-17-12-29},
  posted-at            = {2017-12-29 01:34:17},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Roy-Mandal-2015,
  author               = {Roy, Parthajit and Mandal, J. K.},
  booktitle            = {Computational Intelligence in Data Mining - Volume 3},
  date                 = {2015},
  title                = {Performance Evaluation of Some Clustering Indices},
  doi                  = {10.1007/978-81-322-2202-6\_46},
  editor               = {Jain, Lakhmi C. and Behera, Himansu S. and Mandal, Jyotsna K. and Mohapatra, Durga P.},
  pages                = {509--517},
  publisher            = {Springer India},
  series               = {Smart Innovation, Systems and Technologies},
  url                  = {http://dx.doi.org/10.1007/978-81-322-2202-6\_46},
  volume               = {33},
  abstract             = {This paper analyzes the performances of four internal and five external cluster validity indices. The internal indices are Banfeld-Raftery index, Davies-Bouldin index, Ray-Turi index and Scott-Symons index. Jaccard index, Folkes-Mallows index, Rand index, Rogers-Tanimoto index and Kulczynski index are the external indices considered. The standard K-Means algorithm and CLARA algorithm has been considered as testing models. Four standard data sets, namely Iris, Seeds, Wine and Flame data sets has been chosen for testing the performance of the indices. The performance of the indices with the increasing number of parameters of the data set is measured. The results are compared and analyzed.},
  citeulike-article-id = {14435141},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-81-322-2202-646},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-81-322-2202-646},
  posted-at            = {2017-09-21 00:37:03},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ryazanov-2016,
  author               = {Ryazanov, Vladimir},
  date                 = {2016-07},
  journaltitle         = {Intelligent Data Analysis},
  title                = {About estimation of quality of clustering results via its stability},
  doi                  = {10.3233/ida-160842},
  issn                 = {1088-467X},
  number               = {s1},
  pages                = {S5--S15},
  volume               = {20},
  abstract             = {We know that there are many clustering methods for the case of a known/unknown number of clusters. Clustering is a result of fulfillment of some stopping criterion. Usually, optimisation of some quality criterion is performed or iterative processes are accomplished. How to estimate the quality of clustering obtained by some method? Is the obtained clustering result corresponding to the objective reality or some stopping criterion of the algorithm is made and we have obtained only some partition? Here, a practical approach and the common general criteria based on an estimation of the stability of clustering are submitted. The criterion does not use any probabilistic assumptions or distances in feature space. For some well-known clustering algorithms, efficient methods for computing the introduced stability criteria according to the training set are obtained. Some illustrative real and artificial examples for various situations are shown.},
  citeulike-article-id = {14357928},
  citeulike-linkout-0  = {http://dx.doi.org/10.3233/ida-160842},
  day                  = {13},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-05-16 14:07:47},
  timestamp            = {2020-02-27 04:04},
}

@Article{Saha-et-al-2015,
  author               = {Saha, Biswajit and Mandal, Amitabha and Tripathy, Soumendu B. and Mukherjee, Debaprasad},
  date                 = {2015-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Complex Networks, Communities and Clustering: A survey},
  eprint               = {1503.06277},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1503.06277},
  abstract             = {This paper is an extensive survey of literature on complex network communities and clustering. Complex networks describe a widespread variety of systems in nature and society especially systems composed by a large number of highly interconnected dynamical entities. Complex networks like real networks can also have community structure. There are several types of methods and algorithms for detection and identification of communities in complex networks. Several complex networks have the property of clustering or network transitivity. Some of the important concepts in the field of complex networks are small-world and scale-robustness, degree distributions, clustering, network correlations, random graph models, models of network growth, dynamical processes on networks, etc. Some current areas of research on complex network communities are those on community evolution, overlapping communities, communities in directed networks, community characterization and interpretation, etc. Many of the algorithms or methods proposed for network community detection through clustering are modified versions of or inspired from the concepts of minimum-cut based algorithms, hierarchical connectivity based algorithms, the original GirvanNewman algorithm, concepts of modularity maximization, algorithms utilizing metrics from information and coding theory, and clique based algorithms.},
  citeulike-article-id = {14150071},
  citeulike-linkout-0  = {http://arxiv.org/abs/1503.06277},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1503.06277},
  day                  = {21},
  groups               = {Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:32:16},
  timestamp            = {2020-02-27 04:04},
}

@Article{Sakakibara-et-al-2015,
  author               = {Sakakibara, Takumasa and Matsui, Tohgoroh and Mutoh, Atsuko and Inuzuka, Nobuhiro},
  date                 = {2015},
  journaltitle         = {Procedia Computer Science},
  title                = {Clustering Mutual Funds Based on Investment Similarity},
  doi                  = {10.1016/j.procs.2015.08.251},
  issn                 = {1877-0509},
  pages                = {881--890},
  volume               = {60},
  abstract             = {It is risky to invest to single or similar mutual funds because the variance of the return becomes large. Mutual funds are categorized based on the investment strategy by a company that rated funds based on performance, but the fund categories are different from its actual operations. While some previous studies have proposed methods to cluster mutual funds based on the historical performances, we cannot apply these methods to new mutual funds. In this paper, we clusters mutual funds based on the investment similarity instead of the historical performances. The contributions of this paper are: 1. To propose two new methods for classifying mutual funds based on the investment similarity, 2. To evaluate the proposed methods based on actual 551 Japanese mutual funds.},
  citeulike-article-id = {14212405},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.procs.2015.08.251},
  groups               = {Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:10:38},
  timestamp            = {2020-02-27 04:04},
}

@Article{SardaEspinosa-2017,
  author       = {Alexis Sarda-Espinosa},
  date         = {2019},
  journaltitle = {The R Journal},
  title        = {Comparing Time-Series Clustering Algorithms in R Using the dtwclust Package},
  url          = {https://journal.r-project.org/archive/2019/RJ-2019-023/index.html},
  abstract     = {Most clustering strategies have not changed considerably since their initial definition. Most of the improvements are either related to the distance measure used to assess dissimilarity, or the function used to calculate prototypes or centroids. Time-series clustering is no exception, with the Dynamic Time Warping distance being particularly popular in that context. This distance is computationally expensive, so many related optimizations have been developed over the years. Since no single clustering algorithm can be said to perform best on all datasets, different strategies must be tested and compared, so a common infrastructure can be advantageous. In this manuscript, a general overview of time-series clustering is given, including many specifics related to Dynamic Time Warping and other recently proposed techniques. At the same time, a description of the dtwclust package for the R statistical software is provided, showcasing how it can be used to evaluate many different time-series clustering procedures.},
  howpublished = {Available at https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf},
  timestamp    = {2020-02-27 04:04},
}

@Article{Schmitt-Westerhoff-2017,
  author               = {Schmitt, Noemi and Westerhoff, Frank},
  date                 = {2017-02},
  journaltitle         = {Quantitative Finance},
  title                = {Herding behaviour and volatility clustering in financial markets},
  doi                  = {10.1080/14697688.2016.1267391},
  pages                = {1--17},
  abstract             = {We propose a financial market model in which speculators follow a linear mix of technical and fundamental trading rules to determine their orders. Volatility clustering arises in our model due to speculators? herding behaviour. In case of heightened uncertainty, speculators observe other speculators? actions more closely. Since speculators? trading behaviour then becomes less heterogeneous, the market maker faces a less balanced excess demand and consequently adjusts prices more strongly. Estimating our model using the method of simulated moments reveals that it is able to explain a number of stylized facts of financial markets quite well. Various robustness checks with respect to the model setup reveal that our results are quite stable.},
  citeulike-article-id = {14316708},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2016.1267391},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1267391},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis, Vol_Cluster},
  posted-at            = {2017-03-23 08:51:27},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 04:04},
}

@Article{Sekula-et-al-2017,
  author               = {Sekula, Michael and Datta, Somnath and Datta, Susmita},
  date                 = {2017-03},
  journaltitle         = {Bioinformation},
  title                = {optCluster: An R Package for Determining the Optimal Clustering Algorithm},
  doi                  = {10.6026/97320630013101},
  issn                 = {0973-8894},
  number               = {03},
  pages                = {101--103},
  volume               = {13},
  abstract             = {There exist numerous programs and packages that perform validation for a given clustering solution; however, clustering algorithms fare differently as judged by different validation measures. If more than one performance measure is used to evaluate multiple clustering partitions, an optimal result is often difficult to determine by visual inspection alone. This paper introduces optCluster, an R package that uses a single function to simultaneously compare numerous clustering partitions (created by different algorithms and/or numbers of clusters) and obtain a "best" option for a given dataset. The method of weighted rank aggregation is utilized by this package to objectively aggregate various performance measure scores, thereby taking away the guesswork that often follows a visual inspection of cluster results. The optCluster package contains biological validation measures as well as clustering algorithms developed specifically for RNA sequencing data, making it a useful tool for clustering genomic data.},
  citeulike-article-id = {14435134},
  citeulike-linkout-0  = {http://dx.doi.org/10.6026/97320630013101},
  day                  = {31},
  posted-at            = {2017-09-20 23:40:37},
  timestamp            = {2020-02-27 04:04},
}

@Article{Serviss-et-al-2017,
  author         = {Serviss, Jason T. and Gaadin, Jesper R. and Eriksson, Per and Folkersen, Lasse and Grander, Dan},
  date           = {2017-10-01},
  journaltitle   = {Bioinformatics},
  title          = {ClusterSignificance: a bioconductor package facilitating statistical analysis of class cluster separations in dimensionality reduced data},
  doi            = {10.1093/bioinformatics/btx393},
  number         = {19},
  pages          = {3126--3128},
  volume         = {33},
  abstract       = {Summary: Multi-dimensional data generated via high-throughput experiments is increasingly used in conjunction with dimensionality reduction methods to ascertain if resulting separations of the data correspond with known classes. This is particularly useful to determine if a subset of the variables, e.g. genes in a specific pathway, alone can separate samples into these established classes. Despite this, the evaluation of class separations is often subjective and performed via visualization. Here we present the ClusterSignificance package; a set of tools designed to assess the statistical significance of class separations downstream of dimensionality reduction algorithms. In addition, we demonstrate the design and utility of the ClusterSignificance package and utilize it to determine the importance of long non-coding RNA expression in the identity of multiple hematological malignancies. Availability and implementation: ClusterSignificance is an R package available via Bioconductor (https://bioconductor.org/packages/ClusterSignificance) under GPL-3. Contact: dan.grander@ki.se. Supplementary information: Supplementary data are available at Bioinformatics online.},
  day            = {1},
  f1000-projects = {QuantInvest},
  pmid           = {28957498},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Sherkat-et-al-2018,
  author         = {Sherkat, Ehsan and Nourashrafeddin, Seyednaser and Milios, Evangelos E. and Minghim, Rosane},
  booktitle      = {Proceedings of the 2018 Conference on Human Information Interaction\&Retrieval - IUI '18},
  date           = {2018-03-07},
  title          = {Interactive document clustering revisited: A visual analytics approach},
  doi            = {10.1145/3172944.3172964},
  isbn           = {9781450349451},
  location       = {New York, New York, USA},
  pages          = {281--292},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3172944.3172964},
  abstract       = {Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user's perspectives. To incorporate the users perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.},
  day            = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Song-et-al-2012b,
  author               = {Song, Won-Min and Di Matteo, T. and Aste, Tomaso},
  date                 = {2012-03},
  journaltitle         = {PLoS ONE},
  title                = {Hierarchical Information Clustering by Means of Topologically Embedded Graphs},
  doi                  = {10.1371/journal.pone.0031929},
  number               = {3},
  pages                = {e31929+},
  volume               = {7},
  abstract             = {We introduce a graph-theoretic approach to extract clusters and hierarchies in complex data-sets in an unsupervised and deterministic manner, without the use of any prior information. This is achieved by building topologically embedded networks containing the subset of most significant links and analyzing the network structure. For a planar embedding, this method provides both the intra-cluster hierarchy, which describes the way clusters are composed, and the inter-cluster hierarchy which describes how clusters gather together. We discuss performance, robustness and reliability of this method by first investigating several artificial data-sets, finding that it can outperform significantly other established approaches. Then we show that our method can successfully differentiate meaningful clusters and hierarchies in a variety of real data-sets. In particular, we find that the application to gene expression patterns of lymphoma samples uncovers biologically significant groups of genes which play key-roles in diagnosis, prognosis and treatment of some of the most relevant human lymphoid malignancies.},
  citeulike-article-id = {10441261},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0031929},
  day                  = {9},
  owner                = {cristi},
  posted-at            = {2016-09-27 15:16:15},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Article{So-Yip-2012,
  author               = {So, Mike K. P. and Yip, Iris W. H.},
  date                 = {2012-08},
  journaltitle         = {Journal of Forecasting},
  title                = {Multivariate GARCH Models with Correlation Clustering},
  doi                  = {10.1002/for.1234},
  number               = {5},
  pages                = {443--468},
  volume               = {31},
  abstract             = {A new clustered correlation multivariate generalized autoregressive conditional heteroskedasticity (CC-MGARCH) model that allows conditional correlations to form clusters is proposed. This model generalizes the time-varying correlation structure of Tse and Tsui (2002, Journal of Business and Economic Statistics 20: 351-361) by classifying the correlations among the series into groups.

To estimate the proposed model, Markov chain Monte Carlo methods are adopted. Two efficient sampling schemes for drawing discrete indicators are also developed. Simulations show that these efficient sampling schemes can lead to substantial savings in computation time in Monte Carlo procedures involving discrete indicators.

Empirical examples using stock market and exchange rate data are presented in which two-cluster and three-cluster models are selected using posterior probabilities. This implies that the conditional correlation equation is likely to be governed by more than one set of decaying parameters.},
  citeulike-article-id = {13935025},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.1234},
  day                  = {1},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:21:10},
  publisher            = {John Wiley and Sons, Ltd},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Strapasson-et-al-2016,
  author               = {Strapasson, Joao E. and Pinele, Julianna and Costa, Sueli I. R.},
  booktitle            = {IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM)},
  date                 = {2016-07},
  title                = {Clustering using the Fisher-Rao distance},
  doi                  = {10.1109/sam.2016.7569717},
  isbn                 = {978-1-5090-2103-1},
  location             = {Rio de Janerio, Brazil},
  pages                = {1--5},
  publisher            = {IEEE},
  abstract             = {In this paper we consider the Fisher-Rao distance in the space of the multivariate diagonal Gaussian distributions for clustering methods. Centroids in this space are derived and used to introduce two clustering algorithms for diagonal Gaussian mixture models associated to this metric: the k-means and the hierarchical clustering. These algorithms allow to reduce the number of components of such mixture models in the context of image segmentation. The algorithms presented here are compared with the Bregman hard and hierarchical clustering algorithms regarding the advantages of each method in different situations.},
  citeulike-article-id = {14335039},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/sam.2016.7569717},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-04-10 12:16:22},
  timestamp            = {2020-02-27 04:04},
}

@Article{Tang-et-al-2017,
  author         = {Tang, Yang and Browne, Ryan P. and McNicholas, Paul D.},
  date           = {2017-05-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Flexible Clustering for High-Dimensional Data via Mixtures of Joint Generalized Hyperbolic Models},
  url            = {https://arxiv.org/abs/1705.03130},
  abstract       = {A mixture of joint generalized hyperbolic distributions (MJGHD) is introduced for asymmetric clustering for high-dimensional data. The MJGHD approach takes into account the cluster-specific subspace, thereby limiting the number of parameters to estimate while also facilitating visualization of results. Identifiability is discussed, and a multi-cycle ECM algorithm is outlined for parameter estimation. The MJGHD approach is illustrated on two real data sets, where the Bayesian information criterion is used for model selection.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Taylor-2018,
  author         = {Taylor, Stephen Michael},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Clustering financial return distributions using the fisher information metric},
  doi            = {10.2139/ssrn.3182914},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3182914},
  abstract       = {Information Geometry provides a correspondence between differential geometry and statistics through the Fisher Information matrix. In particular, given two models from the same parametric family of distributions, one can define the distance between these models as the length of the shortest geodesic connecting them in a Riemannian manifold whose metric is given by the model Fisher Information matrix. One limitation that had hinder the adoption of this similarity measure in practical applications is that this distance is typically difficult to compute in a robust manner. We review such complications and provide a general form for the distance function for one parameter models. We next focus on two higher dimensional extreme value models including the Generalized Pareto and Generalized Extreme Value distributions that will be used in financial risk applications. Specifically, we first develop a technique to identify the nearest neighbors of a target security in the sense that their best fit model distributions have minimal Fisher distance to that of target. Second, we develop a hierarchical clustering technique that compares Generalized Extreme Value distributions fit to block maxima of a set of equity loss distributions to group together securities whose worst single day yearly loss distributions exhibit commonalities.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  timestamp      = {2020-02-27 04:04},
}

@Article{Teklehaymanot-et-al-2018,
  author         = {Teklehaymanot, Freweyni K. and Muma, Michael and Zoubir, Abdelhak M.},
  date           = {2018-11-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Robust Bayesian Cluster Enumeration},
  url            = {https://arxiv.org/abs/1811.12337},
  urldate        = {2019-04-25},
  abstract       = {A major challenge in cluster analysis is that the number of data clusters is mostly unknown and it must be estimated prior to clustering the observed data. In real-world applications, the observed data is often subject to heavy tailed noise and outliers which obscure the true underlying structure of the data. Consequently, estimating the number of clusters becomes challenging. To this end, we derive a robust cluster enumeration criterion by formulating the problem of estimating the number of clusters as maximization of the posterior probability of multivariate t candidate models. We utilize Bayes' theorem and asymptotic approximations to come up with a robust criterion that possesses a closed-form expression. Further, we refine the derivation and provide a robust cluster enumeration criterion for the finite sample regime. The robust criteria require an estimate of cluster parameters for each candidate model as an input. Hence, we propose a two-step cluster enumeration algorithm that uses the expectation maximization algorithm to partition the data and estimate cluster parameters prior to the calculation of one of the robust criteria. The performance of the proposed algorithm is tested and compared to existing cluster enumeration methods using numerical and real data experiments.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Tellaroli-et-al-2016,
  author               = {Tellaroli, Paola and Bazzi, Marco and Donato, Michele and Brazzale, Alessandra R. and Draghici, Sorin},
  date                 = {2016-03},
  journaltitle         = {PLOS ONE},
  title                = {Cross-Clustering: A Partial Clustering Algorithm with Automatic Estimation of the Number of Clusters},
  doi                  = {10.1371/journal.pone.0152333},
  number               = {3},
  pages                = {e0152333+},
  volume               = {11},
  abstract             = {Four of the most common limitations of the many available clustering methods are: i) the lack of a proper strategy to deal with outliers; ii) the need for a good a priori estimate of the number of clusters to obtain reasonable results; iii) the lack of a method able to detect when partitioning of a specific data set is not appropriate; and iv) the dependence of the result on the initialization. Here we propose Cross-clustering (CC), a partial clustering algorithm that overcomes these four limitations by combining the principles of two well established hierarchical clustering algorithms: Ward's minimum variance and Complete-linkage. We validated CC by comparing it with a number of existing clustering methods, including Ward's and Complete-linkage. We show on both simulated and real datasets, that CC performs better than the other methods in terms of: the identification of the correct number of clusters, the identification of outliers, and the determination of real cluster memberships. We used CC to cluster samples in order to identify disease subtypes, and on gene profiles, in order to determine groups of genes with the same behavior. Results obtained on a non-biological dataset show that the method is general enough to be successfully used in such diverse applications. The algorithm has been implemented in the statistical language R and is freely available from the CRAN contributed packages repository.},
  citeulike-article-id = {14005861},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0152333},
  day                  = {25},
  posted-at            = {2017-09-21 00:25:43},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Book{Thrun-2018,
  author         = {Thrun, Michael Christoph},
  date           = {2018},
  title          = {Projection-Based Clustering through Self-Organization and Swarm Intelligence},
  doi            = {10.1007/978-3-658-20540-9},
  isbn           = {978-3-658-20539-3},
  location       = {Wiesbaden},
  publisher      = {Springer Fachmedien Wiesbaden},
  abstract       = {It covers aspects of unsupervised machine learning used for knowledge discovery in data science and introduces a data-driven approach to cluster analysis, the Databionic swarm(DBS). DBS consists of the 3D landscape visualization and clustering of data. The 3D landscape enables 3D printing of high-dimensional data structures.The clustering and number of clusters or an absence of cluster structure are verified by the 3D landscape at a glance. DBS is the first swarm-based technique that shows emergent properties while exploiting concepts of swarm intelligence, self-organization and the Nash equilibrium concept from game theory. It results in the elimination of a global objective function and the setting of parameters. By downloading the R package DBS can be applied to data drawn from diverse research fields and used even by non-professionals in the field of data mining.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Tola-et-al-2008,
  author               = {Tola, Vincenzo and Lillo, Fabrizio and Gallegati, Mauro and Mantegna, Rosario N.},
  date                 = {2008-01},
  journaltitle         = {Journal of Economic Dynamics and Control},
  title                = {Cluster analysis for portfolio optimization},
  doi                  = {10.1016/j.jedc.2007.01.034},
  issn                 = {0165-1889},
  number               = {1},
  pages                = {235--258},
  volume               = {32},
  abstract             = {We consider the problem of the statistical uncertainty of the correlation matrix in the optimization of a financial portfolio. By assuming idealized conditions of perfect forecast ability for the future return and volatility of stocks and short selling, we show that the use of clustering algorithms can improve the reliability of the portfolio in terms of the ratio between predicted and realized risk. Bootstrap analysis indicates that this improvement is obtained in a wide range of the parameters N (number of assets) and T (investment horizon). The predicted and realized risk level and the relative portfolio composition of the selected portfolio for a given value of the portfolio return are also investigated for each considered filtering method. We also show that several of the results obtained by assuming idealized conditions are still observed under the more realistic assumptions of no short selling and mean return and volatility forecasting based on historical data.},
  citeulike-article-id = {14148037},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jedc.2007.01.034},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, FrcstQWIM_ShortTerm, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:09:06},
  timestamp            = {2020-02-27 04:04},
}

@Article{Torrente-Brazma-2017,
  author               = {Torrente, Aurora and Brazma, Alvis},
  date                 = {2017-08},
  journaltitle         = {Bioinformatics},
  title                = {clustComp, a bioconductor package for the comparison of clustering results},
  doi                  = {10.1093/bioinformatics/btx532},
  issn                 = {1367-4803},
  abstract             = {clustComp is an open source Bioconductor package that implements different techniques for the comparison of two gene expression clustering results. These include flat versus flat and hierarchical versus flat comparisons. The visualization of the similarities is provided by means of a bipartite graph, whose layout is heuristically optimized. Its flexibility allows a suitable visualization for both small and large datasets.},
  citeulike-article-id = {14468380},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/bioinformatics/btx532},
  day                  = {23},
  posted-at            = {2017-10-28 18:52:23},
  timestamp            = {2020-02-27 04:04},
}

@Article{Turkmen-2015,
  author               = {Turkmen, Ali C.},
  date                 = {2015-08},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A Review of Nonnegative Matrix Factorization Methods for Clustering},
  eprint               = {1507.03194},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1507.03194},
  abstract             = {Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank matrix approximation technique, and has enjoyed a wide area of applications. Although NMF does not seem related to the clustering problem at first, it was shown that they are closely linked. In this report, we provide a gentle introduction to clustering and NMF before reviewing the theoretical relationship between them. We then explore several NMF variants, namely Sparse NMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along with their clustering interpretations.},
  citeulike-article-id = {13671514},
  citeulike-linkout-0  = {http://arxiv.org/abs/1507.03194},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1507.03194},
  day                  = {28},
  owner                = {cristi},
  posted-at            = {2016-03-24 14:49:31},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Vaquez-et-al-2019,
  author         = {Vaquez, Iago and Villar, Jose R. and Sedano, Javier and Simic, Svetlana},
  booktitle      = {14th international conference on soft computing models in industrial and environmental applications (SOCO 2019)},
  date           = {2019},
  title          = {A preliminary study on multivariate time series clustering},
  doi            = {10.1007/978-3-030-20055-8\_45},
  editor         = {Martinez Alvarez, Francisco and Troncoso Lora, Alicia and Saez Munoz, Jose Antonio and Quintian, Hector and Corchado, Emilio},
  isbn           = {978-3-030-20054-1},
  pages          = {473--480},
  publisher      = {Springer International Publishing},
  series         = {Advances in intelligent systems and computing},
  url            = {http://link.springer.com/10.1007/978-3-030-20055-8\_45},
  urldate        = {2019-10-05},
  volume         = {950},
  abstract       = {Time Series (TS) clustering is one of the most effervescent research fields due to the Big Data and the IoT explosion. The problem gets more challenging if we consider the multivariate TS. In the field of Business and Management, multivariate TS are becoming more and more interesting as they allow to match events the co-occur in time but that is hardly noticeable. In this study, Recurrent Neural Networks and transfer learning have been used to analyze each example, measuring similarities between variables. All the results are finally aggregated to create an adjacency matrix that allows extracting the groups. Proof-of-concept experimentation has been included, showing that the solution might be valid after several improvements.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs},
  issn           = {2194-5357},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Veenstra-et-al-2016a,
  author               = {Veenstra, Patrick and Cooper, Colin and Phelps, Steve},
  booktitle            = {8th Computer Science and Electronic Engineering (CEEC)},
  date                 = {2016-09},
  title                = {Spectral clustering using the kNN-MST similarity graph},
  doi                  = {10.1109/ceec.2016.7835917},
  isbn                 = {978-1-5090-2050-8},
  location             = {Colchester, United Kingdom},
  pages                = {222--227},
  publisher            = {IEEE},
  abstract             = {Spectral clustering is a technique that uses the spectrum of a similarity graph to cluster data. Part of this procedure involves calculating the similarity between data points and creating a similarity graph from the resulting similarity matrix. This is ordinarily achieved by creating a k-nearest neighbour (kNN) graph. In this paper, we show the benefits of using a different similarity graph, namely the union of the kNN graph and the minimum spanning tree of the negated similarity matrix (kNN-MST). We show that this has some distinct advantages on both synthetic and real datasets. Specifically, the clustering accuracy of kNN-MST is less dependent on the choice of k than kNN is.},
  citeulike-article-id = {14444472},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ceec.2016.7835917},
  posted-at            = {2017-10-03 08:33:03},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Veldt-et-al-2017,
  author               = {Veldt, Nate and Wirth, Anthony I. and Gleich, David F.},
  booktitle            = {Proceedings of the 26th International Conference on World Wide Web},
  date                 = {2017},
  title                = {Correlation Clustering with Low-Rank Matrices},
  doi                  = {10.1145/3038912.3052586},
  isbn                 = {978-1-4503-4913-0},
  location             = {Perth, Australia},
  pages                = {1025--1034},
  publisher            = {International World Wide Web Conferences Steering Committee},
  series               = {WWW '17},
  abstract             = {Correlation clustering is a technique for aggregating data based on qualitative information about which pairs of objects are labeled `similar' or `dissimilar.' Because the optimization problem is NP-hard, much of the previous literature focuses on finding approximation algorithms. In this paper we explore how to solve the correlation clustering objective exactly when the data to be clustered can be represented by a low-rank matrix. We prove in particular that correlation clustering can be solved in polynomial time when the underlying matrix is positive semidefinite with small constant rank, but that the task remains NP-hard in the presence of even one negative eigenvalue. Based on our theoretical results, we develop an algorithm for efficiently ``solving'' low-rank positive semidefinite correlation clustering by employing a procedure for zonotope vertex enumeration. We demonstrate the effectiveness and speed of our algorithm by using it to solve several clustering problems on both synthetic and real-world data.},
  address              = {Republic and Canton of Geneva, Switzerland},
  citeulike-article-id = {14398863},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3038912.3052586},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3038912.3052586},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-07-24 17:07:45},
  timestamp            = {2020-02-27 04:04},
}

@Article{Verma-et-al-2018,
  author         = {Verma, A. and Buonocore, R. J. and Di Matteo, T.},
  date           = {2018-11-14},
  journaltitle   = {Quantitative Finance},
  title          = {A cluster driven log-volatility factor model: a deepening on the source of the volatility clustering},
  doi            = {10.1080/14697688.2018.1535183},
  issn           = {1469-7688},
  pages          = {1--16},
  abstract       = {We introduce a new factor model for log volatilities that considers contributions, and performs dimensionality reduction, at a global level through the market, and at a local level through clusters and their interactions. We do not assume a-priori the number of clusters in the data, instead using the Directed Bubble Hierarchical Tree algorithm to fix the number of factors. We use the factor model to study how the log volatility contributes to volatility clustering, quantifying the strength of the volatility clustering using a new nonparametric integrated proxy. Indeed finding a link between volatility and volatility clustering, we find that a global analysis reveals that only the market contributes to the volatility clustering. A local analysis reveals that for some clusters, the cluster itself contributes statistically to the volatility clustering effect. This is significantly advantageous over other factor models, since it offers a way of selecting factors in a statistical way, whilst also keeping economically relevant factors. Finally, we show that the log volatility factor model explains a similar amount of memory to a principal components analysis factor model and an exploratory factor model.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {Vol_Cluster},
  timestamp      = {2020-02-27 04:04},
}

@Article{Vinod-Viole-2017,
  author               = {Vinod, Hrishikesh D. and Viole, Fred},
  date                 = {2017},
  journaltitle         = {Computational Economics},
  title                = {Nonparametric Regression Using Clusters},
  doi                  = {10.1007/s10614-017-9713-5},
  pages                = {1--18},
  abstract             = {We present a fundamentally unique method of nonparametric regression using clusters and test it against classically established methods. We compare two nonlinear regression estimation packages called 'NNS', Viole (NNS: nonlinear nonparametric statistics, 2016), and 'np', Hayfield and Racine (J Stat Softw 27(5):1-32, 2008), with the help of a simulation using deterministic (DT) and stochastic (ST) regressor models. We find the respective coefficients of determination (R2)(R2) are close for DT models, while finding an advantage to NNS in ST and large sample cases. Regression coefficients are sometimes regarded as approximations to partial derivatives, especially in social sciences. Then, NNS alone has the ability to compute a range of partials evaluated at points within the sample and also out-of-sample. Thus NNS can provide a viable alternative to kernel based nonparametric regressions without using bandwidths for smoothing.},
  citeulike-article-id = {14398901},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-017-9713-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-017-9713-5},
  groups               = {Clustering and network analysis, Regression_Nonlinear},
  posted-at            = {2017-07-24 20:27:05},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:04},
}

@Article{Viole-2017a,
  author               = {Viole, Fred},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Classification Using NNS Clustering Analysis},
  url                  = {https://ssrn.com/abstract=2864711},
  abstract             = {NNS stands for Nonlinear Nonparametric Statistics, henceforth "NNS". What is NNS clustering analysis? NNS clustering is a method of partitioning the joint distribution into partial moment quadrants (clustering), and assigning identifiers to observations (classification). NNS clustering is very similar to k-means clustering, and we direct the reader to Vinod and Viole [2016] for a proof and comparison between the methods. This article is intended to present working examples of several classification problems using NNS clustering analysis.

We demonstrate how NNS clustering is quite effective, as well as an alternative method NNS employs for classification tasks. We compare predictions of test sets with NNS, k-means using the "cl.predict" routine offered in R to "predict class ids or memberships from R objects representing partitions", K nearest neighbors classification using the "knn" routine in R-package "class", and a naive Bayes classification using the "e1071" package.

The methods and results presented immediately raise suspicions on the pervasive notion of dimension reduction given the consistent performance of the NNS Multivariate Regression.},
  citeulike-article-id = {14398900},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2864711},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-07-24 20:26:00},
  timestamp            = {2020-02-27 04:04},
}

@Article{Wang-et-al-2013,
  author               = {Wang, Yongning and Tsay, Ruey S. and Ledolter, Johannes and Shrestha, Keshab M.},
  date                 = {2013-12},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting Simultaneously High-Dimensional Time Series: A Robust Model-Based Clustering Approach},
  doi                  = {10.1002/for.2264},
  number               = {8},
  pages                = {673--684},
  volume               = {32},
  abstract             = {This paper considers the problem of forecasting high-dimensional time series. It employs a robust clustering approach to perform classification of the component series. Each series within a cluster is assumed to follow the same model and the data are then pooled for estimation. The classification is model-based and robust to outlier contamination. The robustness is achieved by using the intrinsic mode functions of the Hilbert-Huang transform at lower frequencies.

These functions are found to be robust to outlier contamination. The paper also compares out-of-sample forecast performance of the proposed method with several methods available in the literature. The other forecasting methods considered include vector autoregressive models with or without LASSO, group LASSO, principal component regression, and partial least squares.

The proposed method is found to perform well in out-of-sample forecasting of the monthly unemployment rates of 50 US states.},
  citeulike-article-id = {12519623},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2264},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:10:43},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Wang-et-al-2017,
  author               = {Wang, Hao and Pappada, Roberta and Durante, Fabrizio and Foscolo, Enrico},
  booktitle            = {Soft Methods for Data Science},
  date                 = {2017},
  title                = {A Portfolio Diversification Strategy via Tail Dependence Clustering},
  doi                  = {10.1007/978-3-319-42972-4\_63},
  editor               = {Ferraro, Maria B. and Giordani, Paolo and Vantaggi, Barbara and Gagolewski, Marek and Angeles Gil, Mara and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  pages                = {511--518},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {456},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures, a cluster analysis is carried out for discerning between assets with the same performance in risky scenarios. Then, the portfolio composition is determined by fixing a number of assets and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected assets in trouble periods may improve the performance of risk-averse investors.},
  citeulike-article-id = {14150080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-42972-463},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-42972-463},
  groups               = {Networks and investment management, Clustering and network analysis, Diversification_Measure, Diversified_Invest, Network_Invest, Invest_Network, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:54},
  timestamp            = {2020-02-27 04:04},
}

@Article{Wang-et-al-2018a,
  author         = {Wang, Min and Abrams, Zachary B and Kornblau, Steven M and Coombes, Kevin R},
  date           = {2018-01-08},
  journaltitle   = {BMC Bioinformatics},
  title          = {Thresher: determining the number of clusters while removing outliers.},
  doi            = {10.1186/s12859-017-1998-9},
  number         = {1},
  pages          = {9},
  volume         = {19},
  abstract       = {BACKGROUND: Cluster analysis is the most common unsupervised method for finding hidden groups in data. Clustering presents two main challenges: (1) finding the optimal number of clusters, and (2) removing "outliers" among the objects being clustered. Few clustering algorithms currently deal directly with the outlier problem. Furthermore, existing methods for identifying the number of clusters still have some drawbacks. Thus, there is a need for a better algorithm to tackle both challenges. RESULTS: We present a new approach, implemented in an R package called Thresher, to cluster objects in general datasets. Thresher combines ideas from principal component analysis, outlier filtering, and von Mises-Fisher mixture models in order to select the optimal number of clusters. We performed a large Monte Carlo simulation study to compare Thresher with other methods for detecting outliers and determining the number of clusters. We found that Thresher had good sensitivity and specificity for detecting and removing outliers. We also found that Thresher is the best method for estimating the optimal number of clusters when the number of objects being clustered is smaller than the number of variables used for clustering. Finally, we applied Thresher and eleven other methods to 25 sets of breast cancer data downloaded from the Gene Expression Omnibus; only Thresher consistently estimated the number of clusters to lie in the range of 4-7 that is consistent with the literature. CONCLUSIONS: Thresher is effective at automatically detecting and removing outliers. By thus cleaning the data, it produces better estimates of the optimal number of clusters when there are more variables than objects. When we applied Thresher to a variety of breast cancer datasets, it produced estimates that were both self-consistent and consistent with the literature. We expect Thresher to be useful for studying a wide variety of biological datasets.},
  day            = {8},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5759208},
  pmid           = {29310570},
  timestamp      = {2020-02-27 04:04},
}

@Article{Wenskovitch-et-al-2018,
  author         = {Wenskovitch, John and Crandell, Ian and Ramakrishnan, Naren and House, Leanna and Leman, Scotland and North, Chris},
  date           = {2018-01},
  journaltitle   = {IEEE Transactions on Visualization and Computer Graphics},
  title          = {Towards a systematic combination of dimension reduction and clustering in visual analytics.},
  doi            = {10.1109/{TVCG}.2017.2745258},
  number         = {1},
  pages          = {131--141},
  url            = {http://dx.doi.org/10.1109/{TVCG}.2017.2745258},
  volume         = {24},
  abstract       = {Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.},
  f1000-projects = {QuantInvest},
  groups         = {Dimens_Reduc},
  pmid           = {28866581},
  timestamp      = {2020-02-27 04:04},
}

@Article{Weylandt-et-al-2019,
  author         = {Weylandt, Michael and Nagorski, John and Allen, Genevera I.},
  date           = {2019-01-06},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization},
  url            = {https://arxiv.org/abs/1901.01477},
  urldate        = {2019-05-04},
  abstract       = {Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at this https URL.},
  day            = {6},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Whiteley-2019,
  author         = {Whiteley, Nick},
  date           = {2019-06-25},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Dynamic time series clustering via volatility change-points},
  url            = {https://arxiv.org/abs/1906.10372},
  urldate        = {2019-08-20},
  abstract       = {This note outlines a method for clustering time series based on a statistical model in which volatility shifts at unobserved change-points. The model accommodates some classical stylized features of returns and its relation to GARCH is discussed. Clustering is performed using a probability metric evaluated between posterior distributions of the most recent change-point associated with each series. This implies series are grouped together at a given time if there is evidence the most recent shifts in their respective volatilities were coincident or closely timed. The clustering method is dynamic, in that groupings may be updated in an online manner as data arrive. Numerical results are given analyzing daily returns of constituents of the S\&P 500.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Wong-2013,
  author               = {Wong, Man H.},
  date                 = {2013-06},
  journaltitle         = {European Journal of Operational Research},
  title                = {Investment models based on clustered scenario trees},
  doi                  = {10.1016/j.ejor.2012.11.051},
  issn                 = {0377-2217},
  number               = {2},
  pages                = {314--324},
  volume               = {227},
  abstract             = {Stochastic programming is widely applied in financial decision problems. In particular, when we need to carry out the actual calculations for portfolio selection problems, we have to assign a value for each expected return and the associated conditional probability in advance. These estimated random parameters often rely on a scenario tree representing the distribution of the underlying asset returns. One of the drawbacks is that the estimated parameters may be deviated from the actual ones. Therefore, robustness is considered so as to cope with the issue of parameter inaccuracy. In view of this, we propose a clustered scenario-tree approach, which accommodates the parameter inaccuracy problem in the context of a scenario tree. Proposed a new kind of scenario tree, called ? cluster tree ?. It accommodates the parameter inaccuracy in the context of a scenario tree. The idea is illustrated with portfolio selection problems. Three risk measures are considered: probability, downside risk and CVaR. OR techniques include fractional programming, interior point methods and SOCP.},
  citeulike-article-id = {13989083},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2012.11.051},
  groups               = {Network_Invest, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 18:42:57},
  timestamp            = {2020-02-27 04:04},
}

@Article{Xia-et-al-2018,
  author         = {Xia, Jiazhi and Gao, Le and Kong, Kezhi and Zhao, Ying and Chen, Yi and Kui, Xiaoyan and Liang, Yixiong},
  date           = {2018-10},
  journaltitle   = {Journal of Visual Languages \& Computing},
  title          = {Exploring linear projections for revealing clusters, outliers, and trends in subsets of multi-dimensional datasets},
  doi            = {10.1016/j.jvlc.2018.08.003},
  issn           = {1045-926X},
  pages          = {52--60},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S1045926X18301289}},
  volume         = {48},
  abstract       = {Identifying patterns in 2D linear projections is important in understanding multi-dimensional datasets. However, local patterns, which are composed of partial data points, are usually obscured by noises and missed in traditional quality measure approaches that measure the whole dataset. In this paper, we propose an interactive interface to explore 2D linear projections with visual patterns on subsets. First, we propose a voting-based algorithm to recommend optimal projection, in which the identified pattern looks the most salient. Specifically, we propose three kinds of point-wise quality metrics of 2D linear projections for outliers, clusterings, and trends, respectively. For each sampled projection, we measure its importance by accumulating the metrics of selected points. The projection with the highest importance is recommended. Second, we design an exploring interface with a scatterplot, a projection trail map, and a control panel. Our interface allows users to explore projections by specifying interested data subsets. At last, we employ three datasets and demonstrate the effectiveness of our approach through three case studies of exploring clusters, outliers, and trends.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Xiong-et-al-2017,
  author               = {Xiong, Caiming and Johnson, David M. and Corso, Jason J.},
  date                 = {2017-01},
  journaltitle         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title                = {Active Clustering with Model-Based Uncertainty Reduction},
  doi                  = {10.1109/tpami.2016.2539965},
  issn                 = {0162-8828},
  number               = {1},
  pages                = {5--17},
  volume               = {39},
  abstract             = {Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are passive in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an active clustering method, i.e., an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters.},
  citeulike-article-id = {14334833},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tpami.2016.2539965},
  day                  = {1},
  posted-at            = {2017-04-10 01:56:02},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Xue-et-al-2018,
  author         = {Xue, Jingming and Zhu, En and Liu, Qiang and Wang, Chuanli and Yin, Jianping},
  booktitle      = {Cloud Computing and Security: 4th International Conference, ICCCS 2018},
  date           = {2018},
  title          = {A Joint Approach to Data Clustering and Robo-Advisor},
  doi            = {10.1007/978-3-030-00006-6\_9},
  editor         = {Sun, Xingming and Pan, Zhaoqing and Bertino, Elisa},
  isbn           = {978-3-030-00005-9},
  pages          = {97--109},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-00006-6\_9},
  urldate        = {2019-10-12},
  volume         = {11063},
  abstract       = {Robo-advisor is a type of financial recommendation that can provide investors with financial advice or investment management online. Data clustering and item recommendation are both important and challenging in Robo-advisor. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, users in data clustering and group relationship in item recommendation are inherently related. For example, a large number of financial transactions include not only the user asset information, but also the user social information. The existence of relations between users and groups motivates us to jointly perform clustering and item recommendation for Robo-advisor in this paper. In particular, we provide a principle way to capture the relations between users and groups, and propose a novel framework CLURE, which fuses data CLUstering and item REcommendation into a coherent model. With experiments on benchmark and real-world datasets, we demonstrate that the proposed framework CLURE achieves superior performance on both tasks compared to the state-of-the-art methods.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:04},
}

@Article{Xu-Lange-2019,
  author         = {Xu, Jason and Lange, Kenneth},
  date           = {2019-05-24},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Power k-Means Clustering},
  url            = {http://proceedings.mlr.press/v97/xu19a.html},
  urldate        = {2019-09-15},
  abstract       = {Clustering is a fundamental task in unsupervised machine learning. Lloyd 1957 algorithm for k-means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power k-means, our method embeds the k-means problem in a continuous class of similar, better behaved problems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the appealing descent property and low complexity of Lloyd algorithm. Further, our method complements widely used seeding strategies, reaping marked improvements when used together as demonstrated on a suite of simulated and real data examples.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Yang-et-al-2016a,
  author    = {Hoseong Yang and Hye Jin Lee and Eugene Cho and Sungzoon Cho},
  booktitle = {IEEE International Conference on Big Data},
  date      = {2016},
  title     = {Automatic classification of securities using hierarchical clustering of the 10-Ks},
  url       = {https://ieeexplore.ieee.org/abstract/document/7841069},
  abstract  = {Industry classification has been rigorously utilized in academic research and business analytics. The existing classification schemes, however, have been constructed and maintained manually by domain experts, which require exhaustive time and human effort while vulnerable to subjectivity. Hence, the existing classification systems do not properly reflect the fast-changing trends of the firms and the capital market. As a remedy to such shortcomings, this paper proposes a new classification scheme, Business Text Industry Classification (BTIC), namely, that automatically clusters securities based on the textual information from the corporate disclosures. BTIC exploits the business section of the Form 10-Ks, in which firms provide their self-identities in a rich context. We employ doc2vec for document embedding and apply Ward's hierarchical clustering method to categorize securities into BTIC groups. Evaluation results using 12 financial ratios commonly found in financial research show that BTIC performs just as good as SIC and GICS in terms of inter- and intra-industry homogeneity, especially for the higher level of clustering. Given that, we claim that BTIC outperforms SIC and GICS in four aspects: process automation, objectivity, clustering flexibility, and result interpretability.},
  timestamp = {2020-02-27 04:04},
}

@InCollection{You-et-al-2016a,
  author               = {You, Shi Y. and Dan Wang, Yu and Luo, Lin K. and Peng, Hong},
  booktitle            = {11th International Conference on Computer Science and Education (ICCSE)},
  date                 = {2016-08},
  title                = {Finding the clusters with potential value in financial time series based on agglomerative hierarchical clustering},
  doi                  = {10.1109/iccse.2016.7581558},
  isbn                 = {978-1-5090-2218-2},
  location             = {Nagoya, Japan},
  pages                = {77--81},
  publisher            = {IEEE},
  abstract             = {It is interesting to find the clustering with potential value in financial time series. In this paper, we focus on this topic. The owned features of the clusters with potential value are provided firstly. Then, the agglomerative hierarchical clustering (AHC) is used to find those clusters automatically. There are two innovations in this paper. The first one is that the features of the clusters with potential value are embedded into the process of AHC, which reduces the time cost of clustering process. The second one is that we propose two indicators, whole similarity and trend similarity, to measure the persistence of the cluster. The experiment on ten time segments shows the obtained clusters is effective, in which both the whole similarity and the trend similarity on training data are markedly higher than that of randomized clustering. In addition, the persistence of these clusters on test data is also better that the result of randomized guess. We think that the strategy provided in this paper is helpful to find for the clustering with potential value.},
  citeulike-article-id = {14320267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccse.2016.7581558},
  posted-at            = {2017-03-26 18:26:06},
  timestamp            = {2020-02-27 04:04},
}

@Article{Yuan-2005,
  author               = {Yuan, Baosheng},
  date                 = {2006-12},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Scaling, Clustering and Dynamics of Volatility in Financial Time Series},
  url                  = {https://ssrn.com/abstract=950960},
  abstract             = {This thesis investigates volatility clustering, scaling and dynamics in financial series of asset returns and studies the underlying mechanism. We propose a direct measure of volatility clustering based on the conditional probability distribution (CPD) of the returns given the return in the previous time interval. We found that the CPDs of returns in real financial time series exhibits universal scaling, characterized by a collapse of the CPDs (of different time lags and of different returns in the previous interval) into to a universal curve exhibiting a power-law tail with an exponent of 4. We construct a simple phenomenological model to explain the emergence of VC and the associated volatility scaling. We also study agent-based models of financial markets, and explore the impact of dynamical risk aversion (DRA) of heterogeneous agents on the price fluctuations. We found that the DRA is the primary driving force responsible for excess price fluctuations and the associated volatility clustering. Both our models (phenomenological model and agent-based model) are able to generate time series that reproduces stylized facts of the market data on different time scales. We have also studied general herding behavior often exhibited in financial markets in the context of an evolutionary Minority Game. We discovered a general mechanism for the transition from segregation into opposing groups to clustering towards cautious behavior.},
  citeulike-article-id = {13988076},
  citeulike-linkout-0  = {http://ssrn.com/abstract=950960},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID950960code539232.pdf?abstractid=950960 and mirid=1},
  day                  = {22},
  groups               = {Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-03-25 16:56:07},
  timestamp            = {2020-02-27 04:04},
}

@Article{Yu-et-al-2018a,
  author         = {Yu, Han and Chapman, Brian and Di Florio, Arianna and Eischen, Ellen and Gotz, David and Jacob, Mathews and Blair, Rachael Hageman},
  date           = {2018-08-28},
  journaltitle   = {Computational statistics},
  title          = {Bootstrapping estimates of stability for clusters, observations and model selection},
  doi            = {10.1007/s00180-018-0830-y},
  issn           = {0943-4062},
  number         = {1},
  pages          = {349--372},
  urldate        = {2019-04-27},
  volume         = {34},
  abstract       = {Clustering is a challenging problem in unsupervised learning. In lieu of a gold standard, stability has become a valuable surrogate to performance and robustness. In this work, we propose a non-parametric bootstrapping approach to estimating the stability of a clustering method, which also captures stability of the individual clusters and observations. This flexible framework enables different types of comparisons between clusterings and can be used in connection with two possible bootstrap approaches for stability. The first approach, scheme 1, can be used to assess confidence (stability) around clustering from the original dataset based on bootstrap replications. A second approach, scheme 2, searches over the bootstrap clusterings for an optimally stable partitioning of the data. The two schemes accommodate different model assumptions that can be motivated by an investigator trust (or lack thereof) in the original data and additional computational considerations. We propose a hierarchical visualization extrapolated from the stability profiles that give insights into the separation of groups, and projected visualizations for the inspection of the stability of individual operations. Our approaches show good performance in simulation and on real data. These approaches can be implemented using the R package bootcluster that is available on the Comprehensive R Archive Network (CRAN).},
  day            = {28},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Yu-et-al-2105b,
  author               = {Yu, Meichen and Hillebrand, Arjan and Tewarie, Prejaas and Meier, Jil and van Dijk, Bob and Van Mieghem, Piet and Stam, Cornelis Jan J.},
  date                 = {2015-02},
  journaltitle         = {Chaos},
  title                = {Hierarchical clustering in minimum spanning trees.},
  issn                 = {1089-7682},
  number               = {2},
  url                  = {http://view.ncbi.nlm.nih.gov/pubmed/25725643},
  volume               = {25},
  abstract             = {The identification of clusters or communities in complex networks is a reappearing problem. The minimum spanning tree (MST), the tree connecting all nodes with minimum total weight, is regarded as an important transport backbone of the original weighted graph. We hypothesize that the clustering of the MST reveals insight in the hierarchical structure of weighted graphs. However, existing theories and algorithms have difficulties to define and identify clusters in trees. Here, we first define clustering in trees and then propose a tree agglomerative hierarchical clustering (TAHC) method for the detection of clusters in MSTs. We then demonstrate that the TAHC method can detect clusters in artificial trees, and also in MSTs of weighted social networks, for which the clusters are in agreement with the previously reported clusters of the original weighted networks. Our results therefore not only indicate that clusters can be found in MSTs, but also that the MSTs contain information about the underlying clusters of the original weighted network.},
  citeulike-article-id = {14150073},
  citeulike-linkout-0  = {http://view.ncbi.nlm.nih.gov/pubmed/25725643},
  citeulike-linkout-1  = {http://www.hubmed.org/display.cgi?uids=25725643},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  pmid                 = {25725643},
  posted-at            = {2016-10-01 20:43:49},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zambom-et-al-2019,
  author         = {Zambom, Adriano Zanin and Collazos, Julian A. and Dias, Ronaldo},
  date           = {2019-05-02},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Selection of the Number of Clusters in Functional Data Analysis},
  url            = {https://arxiv.org/abs/1905.00977},
  urldate        = {2019-08-10},
  abstract       = {Identifying the number KK of clusters in a dataset is one of the most difficult problems in clustering analysis. A choice of KK that correctly characterizes the features of the data is essential for building meaningful clusters. In this paper we tackle the problem of estimating the number of clusters in functional data analysis by introducing a new measure that can be used with different procedures in selecting the optimal KK. The main idea is to use a combination of two test statistics, which measure the lack of parallelism and the mean distance between curves, to compute criteria such as the within and between cluster sum of squares. Simulations in challenging scenarios suggest that procedures using this measure can detect the correct number of clusters more frequently than existing methods in the literature. The application of the proposed method is illustrated on several real datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@TechReport{Zhang-Maringer-2010,
  author      = {Jin Zhang and Dietmar Maringer},
  date        = {2010},
  institution = {COMISEF Computational Optimization Methods in Statistics, Econometrics and Finance},
  title       = {Asset Allocation under Hierarchical Clustering},
  url         = {https://ideas.repec.org/p/com/wpaper/036.html},
  abstract    = {This paper proposes a clustering asset allocation scheme which provides better risk-adjusted portfolio performance than those obtained from tradi- tional asset allocation approaches such as the equal weight strategy and the Markowitz minimum variance allocation. The clustering criterion used, which involves maximization of the in-sample Sharpe ratio (SR), is different from traditional clustering criteria reported in the literature. Two evolu- tionary methods, namely Differential Evolution and Genetic Algorithm, are employed to search for such an optimal clustering structure given a clus- ter number. To explore the clustering impact on the SR, the in-sample and the out-of-sample SR distributions of the portfolios are studied using bootstrapped data as well as simulated paths from the single index market model. It was found that the SR distributions of the portfolios under the clustering asset allocation structure have higher mean values and skewness but approximately the same standard deviation and kurtosis than those in the non-clustered case. Genetic Algorithm is suggested as a more efficient approach than Differential Evolution for the purpose of solving the cluster-ing problem.},
  groups      = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network},
  timestamp   = {2020-02-27 04:04},
}

@Article{Zhang-Maringer-2011,
  author               = {Zhang, Jin and Maringer, Dietmar},
  date                 = {2011-11},
  journaltitle         = {Expert Systems with Applications},
  title                = {Distributing weights under hierarchical clustering: A way in reducing performance breakdown},
  doi                  = {10.1016/j.eswa.2011.05.052},
  issn                 = {0957-4174},
  number               = {12},
  pages                = {14952--14959},
  volume               = {38},
  abstract             = {This paper proposes a clustering asset allocation scheme which provides better risk-adjusted portfolio performance than those obtained from traditional asset allocation approaches such as the equal weight strategy and the Markowitz minimum variance allocation. The clustering criterion used, which involves maximization of the in-sample Sharpe ratio (SR), is different from traditional clustering criteria reported in the literature. Two evolutionary methods, namely Differential Evolution and Genetic Algorithm, are employed to search for such an optimal clustering structure given a cluster number. To explore the clustering impact on the SR, the in-sample and the out-of-sample SR distributions of the portfolios are studied using bootstrapped data as well as simulated paths from the single index market model. It was found that the SR distributions of the portfolios under the clustering asset allocation structure have higher mean values and skewness but approximately the same standard deviation and kurtosis than those in the non-clustered case. Genetic Algorithm is suggested as a more efficient approach than Differential Evolution for the purpose of solving the clustering problem. We introduce a clustering scheme to improve portfolio Sharpe ratio. Mean and Skewness of Sharpe ratio can be improved by using the clustering scheme. Genetic Algorithm is apt at finding an optimal clustering structure. Clustering asset helps to improve portfolio risk-adjusted performance. Sharpe ratio maximization can be considered as a suitable clustering criterion.},
  citeulike-article-id = {9504815},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2011.05.052},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:47:37},
  timestamp            = {2020-02-27 04:04},
}

@InProceedings{Zhang-Zhu-2011,
  author         = {Zhang, Dabin and Zhu, Hou},
  booktitle      = {Fourth International Joint Conference on Computational Sciences and Optimization},
  date           = {2011-04-15},
  title          = {A clustering methodology for industry categorization using business cycle},
  doi            = {10.1109/{CSO}.2011.22},
  isbn           = {978-1-4244-9712-6},
  pages          = {318--321},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/5957670/},
  urldate        = {2019-12-04},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Zhong-Enke-2017a,
  author               = {Zhong, Xiao and Enke, David},
  date                 = {2017-12},
  journaltitle         = {Neurocomputing},
  title                = {A comprehensive cluster and classification mining procedure for daily stock market return forecasting},
  doi                  = {10.1016/j.neucom.2017.06.010},
  issn                 = {0925-2312},
  pages                = {152--168},
  volume               = {267},
  abstract             = {Data mining and big data analytic techniques are playing an important role in many application fields, including the financial markets. However, only few studies have focused on predicting daily stock market returns, and among these studies, the data mining procedures utilized are either incomplete or inefficient. This paper presents a comprehensive data mining process to forecast the daily direction of the S\&P 500 Index ETF (SPY) return based on 60 financial and economical features. The fuzzy c-means method (FCM) is initially used to cluster the preprocessed data. A principal component analysis (PCA) is applied next to the entire data set and each of seven clusters. The dimension of the entire cleaned data set is then reduced according to the combining results from the entire data set and each cluster. Corresponding to different levels of the dimensionality reduction, twelve new data sets are generated from the entire cleaned data. Artificial neural networks (ANNs) and logistic regression models are then used with the twelve transformed data sets for classification in order to forecast the daily direction of future market returns and indicate the efficiency of dimensionality reduction with PCA. A group of hypothesis tests are performed over the classification and simulation results to show that the ANNs give significantly higher classification accuracy than logistic regression, and that the trading strategies guided by the comprehensive cluster and classification mining procedure based on PCA and ANNs gain higher risk-adjusted profits than the comparison benchmarks, as well as those strategies guided by the forecasts based on PCA and logistic regression models.},
  citeulike-article-id = {14500341},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2017.06.010},
  groups               = {Data_Cleaning},
  posted-at            = {2017-12-11 05:03:50},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zhou-et-al-2016,
  author               = {Zhou, Shibing and Xu, Zhenyuan and Liu, Fei},
  date                 = {2016},
  journaltitle         = {IEEE Transactions on Neural Networks and Learning Systems},
  title                = {Method for Determining the Optimal Number of Clusters Based on Agglomerative Hierarchical Clustering},
  doi                  = {10.1109/tnnls.2016.2608001},
  issn                 = {2162-237X},
  pages                = {1--11},
  abstract             = {It is crucial to determine the optimal number of clusters for the clustering quality in cluster analysis. From the standpoint of sample geometry, two concepts, i.e., the sample clustering dispersion degree and the sample clustering synthesis degree, are defined, and a new clustering validity index is designed. Moreover, a method for determining the optimal number of clusters based on an agglomerative hierarchical clustering (AHC) algorithm is proposed. The new index and the method can evaluate the clustering results produced by the AHC and determine the optimal number of clusters for multiple types of datasets, such as linear, manifold, annular, and convex structures. Theoretical research and experimental results indicate the validity and good performance of the proposed index and the method.},
  citeulike-article-id = {14435140},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tnnls.2016.2608001},
  posted-at            = {2017-09-21 00:36:08},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zhu-et-al-2019c,
  author         = {Zhu, Dandan and Han, Tian and Zhou, Linqi and Yang, Xiaokang and Wu, Ying Nian},
  date           = {2019-11-19},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Unsupervised Clustering with Clustered Generator Model},
  url            = {https://arxiv.org/abs/1911.08459v1},
  urldate        = {2019-12-15},
  abstract       = {This paper addresses the problem of unsupervised clustering which remains one of the most fundamental challenges in machine learning and artificial intelligence. We propose the clustered generator model for clustering which contains both continuous and discrete latent variables. Discrete latent variables model the cluster label while the continuous ones model variations within each cluster. The learning of the model proceeds in a unified probabilistic framework and incorporates the unsupervised clustering as an inner step without the need for an extra inference model as in existing variational-based models. The latent variables learned serve as both observed data embedding or latent representation for data distribution. Our experiments show that the proposed model can achieve competitive unsupervised clustering accuracy and can learn disentangled latent representations to generate realistic samples. In addition, the model can be naturally extended to per-pixel unsupervised clustering which remains largely unexplored.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Online{Vigen-2019,
  author           = {Tyler Vigen},
  date             = {2019},
  title            = {Spurious Correlations},
  url              = {https://www.tylervigen.com/spurious-correlations},
  abstract         = {Military intelligence analyst and Harvard Law student Tyler Vigen illustrates the golden rule that "correlation does not equal causation" through hilarious graphs.

Is there a correlation between Nic Cage films and swimming pool accidents? What about beef consumption and people getting struck by lightning? Absolutely not. But that hasn't stopped millions of people from going to tylervigen.com and asking, "Wait, what?" Vigen has designed software that scours enormous data sets to find unlikely statistical correlations. He began pulling the funniest ones for his website and has since gained millions of views, hundreds of thousands of likes, and tons of media coverage. Subversive and clever, Spurious Correlations is geek humor at its finest, nailing our obsession with data and conspiracy theory.},
  creationdate     = {2023-06-22T21:26:41},
  modificationdate = {2023-06-22T21:26:41},
}

@Article{Laurinaityte-et-al-2019,
  author               = {Laurinaityte, Nora and Meinerding, Christoph and Schlag, Christian and Thimme, Julian},
  date                 = {2019},
  journaltitle         = {SSRN e-Print},
  title                = {Elephants and the Cross-Section of Expected Returns},
  url                  = {https://ssrn.com/abstract=3073197},
  abstract             = {The population growth of captive Asian elephants explains the cross-section of expected returns of size-value sorted portfolios with a cross-sectional R2 of 93\% and a t-statistic of 4.0 for the market price of risk. One may be tempted to conclude that elephants are the new outstanding factor in empirical asset pricing. We argue that one has to be careful with such conclusions. Standard GMM cross-sectional asset pricing tests can generate spurious explanatory power for factor models when the weight on certain moment conditions is set inappropriately. In fact, by shifting the weights in the GMM, any desired level of cross-sectional fit can be attained at the price of not matching the factor means. We run placebo tests with factors that by construction do not explain the cross-section of expected returns and obtain spuriously high cross-sectional R2's. Finally, we document some examples of factor models proposed in the literature that suffer from this bias.},
  citeulike-article-id = {14487873},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3073197},
  creationdate         = {2023-06-22T21:27:21},
  modificationdate     = {2023-06-22T21:27:21},
  posted-at            = {2017-12-03 23:39:42},
  timestamp            = {2020-07-23 13:37},
}

@Article{Anson-2022,
  author           = {Mark Anson},
  date             = {2022-01},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Diversification -- A Free Starbucks Cup of Coffee?},
  doi              = {10.3905/jpm.2022.1.330},
  number           = {4},
  pages            = {220-227},
  volume           = {48},
  abstract         = {Diversification of sophisticated portfolios has become more difficult. In the past, asset class and geographic dispersion were sufficient to ensure a properly diversified portfolio. Not so anymore, as major economies integrate and coordinate their monetary and fiscal policies. In addition, the growth of the exchange-traded fund market has impinged on what was previously the alpha hunting ground of active managers, blurring the distinction between what is beta and what is alpha. This article shows how multi-asset solutions can be used to expand the free lunch of diversification back to its full potential.},
  creationdate     = {2023-06-22T22:20:07},
  modificationdate = {2023-06-22T22:20:07},
  publisher        = {Pageant Media {US}},
}

@Article{Bratis-et-al-2020,
  author           = {Theodoros Bratis and Nikiforos T. Laopodis and Georgios P. Kouretas},
  date             = {2020},
  journaltitle     = {The European Journal of Finance},
  title            = {Dynamics among global asset portfolios},
  doi              = {10.1080/1351847x.2020.1791924},
  number           = {18},
  pages            = {1876-1899},
  volume           = {26},
  abstract         = {We examine the dynamic correlations among several global financial assets with an eye to potential portfolio diversification benefits during the 2008 US financial crisis and EMU sovereign debt crisis of the 2010s. Our findings are summarized as follows: First, evidence for rigorous, dynamic cross-correlations among global equities around the 2008 crisis suggested a weak global diversification potential. Second, financial spillovers strengthened in the post-crisis period thus, exhibiting cycles of inter-linkages among various assets classes. Third, heterogeneous global portfolios (that is, portfolios formed by various asset classes) offered greater returns than homogeneous portfolios for the whole period and especially in the period preceding the 2008 crisis. Overall, we conclude that the US and EMU crisis periods did not affect assets in the same way and hence, risk managers should follow portfolio-construction strategies with risk-offsetting assets (such as commodities) with regard to their cyclical/countercyclical movements.},
  creationdate     = {2023-06-22T22:22:20},
  modificationdate = {2023-06-22T22:22:20},
  publisher        = {Informa {UK} Limited},
  timestamp        = {2020-10-13 17:32},
}

@Article{Choi-et-al-2022d,
  author           = {Jaehyung Choi and Hyangju Kim and Young Shin Kim},
  date             = {2022},
  journaltitle     = {arXiv e-Print},
  title            = {Diversified reward-risk parity in portfolio construction},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2106.09055},
  abstract         = {We introduce diversified risk parity embedded with various reward-risk measures and more general allocation rules for portfolio construction. We empirically test advanced reward-risk parity strategies and compare their performance with an equally-weighted risk portfolio in various asset universes. All reward-risk parity strategies we tested exhibit consistent outperformance evidenced by higher average returns, Sharpe ratios, and Calmar ratios. The alternative allocations also reflect less downside risks in Value-at-Risk, conditional Value-at-Risk, and maximum drawdown. In addition to the enhanced performance and reward-risk profile, transaction costs can be reduced by lowering turnover rates. The Carhart four-factor analysis also indicates that the diversified reward-risk parity allocations gain superior performance.},
  creationdate     = {2023-06-22T22:23:20},
  file             = {:Choi-et-al-2022d - Diversified Reward Risk Parity in Portfolio Construction.pdf:PDF},
  keywords         = {q-fin.PM},
  modificationdate = {2023-06-22T22:23:22},
}

@Article{Dees-et-al-2020,
  author           = {Dees, Bruno Scalzo and Stankovic, Ljubisa and Constantinides, Anthony G. and Mandic, Danilo P.},
  date             = {2020},
  journaltitle     = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title            = {Portfolio Cuts: A Graph-Theoretic Framework to Diversification},
  doi              = {10.1109/ICASSP40776.2020.9054371},
  abstract         = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  creationdate     = {2023-06-22T22:24:06},
  day              = {12},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:24:06},
  timestamp        = {2020-07-23 13:32},
}

@Article{Eom-et-al-2021,
  author           = {Cheoljun Eom and Taisei Kaizoji and Giacomo Livan and Enrico Scalas},
  date             = {2021-04},
  journaltitle     = {The North American Journal of Economics and Finance},
  title            = {Limitations of portfolio diversification through fat tails of the return Distributions: Some empirical evidence},
  doi              = {10.1016/j.najef.2020.101358},
  pages            = {101358},
  volume           = {56},
  abstract         = {This study investigates the level of risk due to fat tails of the return distribution and the changes of tail fatness (TF) through portfolio diversification. TF is not eliminated through portfolio diversification, and, interestingly, the positive tail has declining fatness until a certain level is reached, while the negative tail has rising fatness. This indicates that fat tails are highly relevant to common factors on systematic risk and that the relevance of common factors is higher for the negative tail compared to the positive tail. In the portfolio diversification effect, the declining fatness of the positive tail further reduces risk, but the rising fatness of the negative tail does not contribute to this effect. The asymmetry between the fatness of the positive and negative tails in the return distribution corresponds to the asymmetry of the trade-off relationship between loss avoidance and profit sacrifice that is expected as a consequence of portfolio diversification. Investors use portfolio diversification to reduce their risk of suffering high losses, but following this strategy means sacrificing high-profit potential. Our study provides empirical confirmation for the practical limitation of portfolio diversification and explains why investors with diversified portfolios suffer high losses from market crashes. An examination of the Northeast Asian stock markets of China, Japan, Korea, and Taiwan show identical results.},
  creationdate     = {2023-06-22T22:25:06},
  modificationdate = {2023-06-22T22:25:06},
  publisher        = {Elsevier {BV}},
}

@Article{Flint-et-al-2021a,
  author           = {Emlyn Flint and Florence Chikurunhe and Ndinae Masutha},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Managing Tail Risk Part I: Option-Based Hedging},
  doi              = {10.2139/ssrn.3817440},
  abstract         = {There is nothing like a market crash to focus the mind on the importance of risk management and, more specifically, tail risk management. Because tail events are generally systemic in nature and are characterised by elevated correlations and liquidity squeezes, effective tail risk management is not just a diversification exercise but rather requires explicit and specialised strategies. The natural question that arises then is how do you create an optimal tail risk management strategy? Most studies identify four categories of tail risk management strategy: option-based hedging, asset allocation, dynamic trading and defensive equity. Within each category, we then find a number of strategies available to investors for managing tail risk. Unfortunately, it remains an open question as to which strategy works best for a given investment scenario, or if there even exists a universally optimal strategy to begin with. We attempt to answer this question by analysing a range of commonly used tail risk management strategies in a South African market setting. However, given the breadth of the four available strategy categories, we split our research on this topic into two parts. In this Part I, we firstly examine and quantify the tail risk inherent in South African markets. This is done through a review the long-term history of equity and bond market drawdowns. Thereafter, we discuss nine core principles that are applicable to all candidate strategies and that define good tail risk management. Finally, we provide a comprehensive analysis of option-based tail hedging strategies. The concepts of defensive, offensive, active and indirect tail hedging are discussed at length and examples of each are implemented on historical market data.},
  creationdate     = {2023-06-22T22:25:46},
  modificationdate = {2023-06-22T22:25:46},
  publisher        = {Elsevier {BV}},
}

@Article{Flint-et-al-2021,
  author           = {E. Flint and A. Seymour and F. Chikurunhe},
  date             = {2021-01},
  journaltitle     = {South African Actuarial Journal},
  title            = {Defining and measuring portfolio diversification},
  doi              = {10.4314/saaj.v20i1.2},
  number           = {1},
  pages            = {17--48},
  volume           = {20},
  abstract         = {It is often said that diversification is the only 'free lunch' available to investors; meaning that a properly diversified portfolio reduces total risk without necessarily sacrificing expected return. However, achieving true diversification is easier said than done, especially when we do not fully know what we mean when we are talking about diversification. While the qualitative purpose of diversification is well known, a satisfactory quantitative definition of portfolio diversification remains elusive. In this research, we summarise a wide range of diversification measures, focusing our efforts on those most commonly used in practice. We categorise each measure based on which portfolio aspect it focuses on: cardinality, weights, returns, risk or higher moments. We then apply these measures to a range of South African equity indices, thus giving a diagnostic review of historical local equity diversification and, perhaps more importantly, providing a description of the investable opportunity set available tofund managers in this space. Finally, we introduce the idea of diversification profiles. These regime dependent profiles give a much richer description of portfolio diversification than their single-value counterparts and also allow one to manage diversification proactively based on one's view of future market conditions.},
  creationdate     = {2023-06-22T22:25:46},
  modificationdate = {2023-06-22T22:25:46},
  publisher        = {African Journals Online ({AJOL})},
}

@Article{Fusai-et-al-2020,
  author           = {Gianluca Fusai and Domenico Mignacca and Andrea Nardon and Ben Human},
  date             = {2020},
  journaltitle     = {Risk (Cutting Edge)},
  title            = {Equally Diversified or Equally Weighted?},
  url              = {https://www.risk.net/cutting-edge/investments/7675551/equally-diversified-or-equally-weighted},
  abstract         = {Gianluca Fusai, Domenico Mignacca, Andrea Nardon and Ben Human show how to decompose portfolio volatility into undiversified volatility and a diversification component. The authors' decomposition has a clear statistical interpretation because it relates the diversification component to partial covariances. On this basis, they advocate the construction of an equally diversified portfolio. An empirical analysis illustrates the superior out-of-sample performance of the equally diversified portfolio with respect to an equally weighted portfolio},
  creationdate     = {2023-06-22T22:26:29},
  modificationdate = {2023-06-22T22:26:29},
  timestamp        = {2020-09-08 19:45},
}

@Article{Jacobsen-Scheiber-2022a,
  author           = {Brian Jacobsen and Matthias Scheiber},
  date             = {2022-01},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {When to Diversify Differently},
  doi              = {10.3905/jpm.2022.1.328},
  number           = {3},
  pages            = {91-107},
  volume           = {48},
  abstract         = {Government bonds are supposed to diversify equities, and they typically do. Assets do not have to be negatively correlated to diversify each other, although a negative correlation can help. The authors provide an analysis of what can cause the sign of the correlation coefficient between government bond returns and stock returns to flip. This can help inform whether and when to seek out diversifiers other than government bonds. Using a variety of statistical tests, the authors show that the correlation has gone through periods of structural change since the 1980s that tend to coincide with major crises, such as the Asian Financial Crisis and the Global Financial Crisis. They also show that the stock-bond correlation has tended to decline as the link between growth and inflation has weakened and as the level and volatility of inflation has declined.},
  creationdate     = {2023-06-22T22:27:22},
  modificationdate = {2023-06-22T22:27:22},
  publisher        = {Pageant Media {US}},
}

@Article{Jacobsen-Scheiber-2022,
  author           = {Brian Jacobsen and Matthias Scheiber},
  date             = {2022-03},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Harvesting Multi-Asset Carry, Value, and Momentum: Work Smarter, Not Harder},
  doi              = {10.3905/jfds.2022.1.087},
  number           = {2},
  pages            = {50-61},
  volume           = {4},
  abstract         = {Carry, value, and momentum are the trinity of systematic investing. As signals, it is important to know what they signify and how to interpret the signals. What is the cost of delay? How does their effectiveness change as a function of the holding period? The authors illustrate how these signals can differ in terms of their informational content and persistence. They also show how classification trees can be used to combine these signals to get the most meaning out of them.},
  creationdate     = {2023-06-22T22:27:22},
  modificationdate = {2023-06-22T22:27:22},
  publisher        = {Pageant Media {US}},
}

@Article{Jaeger-et-al-2021,
  author           = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date             = {2021},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Interpretable Machine Learning for Diversified Portfolio Construction},
  doi              = {10.3905/jfds.2021.1.066},
  number           = {3},
  pages            = {31-51},
  url              = {https://jfds.pm-research.com/content/early/2021/06/14/jfds.2021.1.066},
  volume           = {3},
  abstract         = {In this paper, the authors construct a pipeline to benchmark Hierarchical Risk Parity (HRP) relative to Equal Risk Contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage ("volatility target"). The authors use interpretable machine learning concepts ("explainable AI") to compare the robustness of the strategies and to back out implicit rules for decision making. The empirical dataset consists of 17 equity index, government bond and commodity futures markets across 20 years. The two strategies are backtested for the empirical dataset and for about 100 000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes.},
  creationdate     = {2023-06-22T22:28:40},
  modificationdate = {2023-06-22T22:28:40},
  timestamp        = {2021-01-09 15:53},
}

@Article{Jaeger-et-al-2021b,
  author           = {Markus Jaeger and Stephan Krugel and Jochen Papenbrock and Peter Schwendner},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Adaptive Seriational Risk Parity and other Extensions for Heuristic Portfolio Construction using Machine Learning and Graph Theory},
  doi              = {10.2139/ssrn.3806714},
  abstract         = {In this article, the authors present a conceptual framework named Adaptive Seriational Risk Parity (ASRP) to extend Hierarchical Risk Parity (HRP) as an asset allocation heuristic. The first step of HRP (quasi-diagonalization) determining the hierarchy of assets is required for the actual allocation in the second step of HRP (recursive bisectioning). In the original HRP scheme, this hierarchy is found using the single-linkage hierarchical clustering of the correlation matrix, which is a static tree-based method. The authors of this paper compare the performance of the standard HRP with other static and also adaptive tree-based methods, but also seriation-based methods that do not rely on trees. Seriation is a broader concept allowing to reorder the rows or columns of a matrix to best express similarities between the elements. Each discussed variation leads to a different time series reflecting portfolio performance using a 20-year backtest of a multi-asset futures universe. An unsupervised representation learning based on this time series data creates a taxonomy that groups the strategies in high correspondence to the structure of the various types of ASRP. The performance analysis of the variations shows that most of the static tree-based alternatives of HRP outperform the single linkage clustering used in HRP on a risk-adjusted basis. Adaptive tree methods show mixed results and most generic seriation-based approaches underperform.},
  creationdate     = {2023-06-22T22:28:40},
  modificationdate = {2023-06-22T22:28:40},
  publisher        = {Elsevier {BV}},
}

@Article{Jaeger-et-al-2021a,
  author           = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Interpretable Machine Learning for Diversified Portfolio Construction},
  doi              = {10.2139/ssrn.3528616},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3730144},
  urldate          = {2020-03-06},
  abstract         = {In this paper, the authors construct a pipeline to benchmark Hierarchical Risk Parity (HRP) relative to Equal Risk Contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage ("volatility target"). The authors use interpretable machine learning concepts ("explainable AI") to compare the robustness of the strategies and to back out implicit rules for decision making. The empirical dataset consists of 17 equity index, government bond and commodity futures markets across 20 years. The two strategies are backtested for the empirical dataset and for about 100 000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes.},
  creationdate     = {2023-06-22T22:29:03},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_ML},
  modificationdate = {2023-06-22T22:29:03},
  timestamp        = {2021-01-09 15:53},
}

@Article{Kelliher-et-al-2022,
  author           = {Chris Kelliher and Avishek Hazrachoudhury and Bill Irving},
  date             = {2022-01},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {A Novel Approach to Risk Parity: Diversification across Risk Factors and Market Regimes},
  doi              = {10.3905/jpm.2022.1.327},
  number           = {5},
  pages            = {73-90},
  volume           = {48},
  abstract         = {In this article, the authors describe a robust approach to portfolio diversification that balances risk contributions across risk factors and market regimes. After identifying four compensated macro risk factors-growth, inflation, real rates, and liquidity-the authors construct a factor portfolio for each based on a broad set of asset classes, including proxies for private equity and private real estate. Next, the authors identify five distinct market regimes characterized by unique asset class behaviors. The factor portfolios are then combined such that the risk contributions to the resulting total portfolio are as balanced as possible, regardless of which market regime materializes. By combining regime-aware correlations with dynamic volatility estimates for each factor and applying standard 1.5x to 2x leverage, the authors demonstrate a risk-parity portfolio with 10\% ex ante volatility and attractive absolute and risk-adjusted returns. Compared with a traditional 60/40 portfolio, the proposed risk-parity portfolio displays greater diversification, more consistent factor-risk contributions, and greater resilience to economic shocks.},
  creationdate     = {2023-06-22T22:29:48},
  modificationdate = {2023-06-22T22:29:48},
  publisher        = {Pageant Media {US}},
}

@Article{Kinlaw-et-al-2021b,
  author           = {Kinlaw, William B. and Kritzman, Mark and Turkington, David},
  date             = {2020},
  journaltitle     = {Journal of Investment Management},
  title            = {A new index of the business cycle},
  number           = {3},
  pages            = {4-19},
  volume           = {19},
  abstract         = {The authors introduce a new index of the business cycle that uses the Mahalanobis distance to measure the statistical similarity of current economic conditions to past episodes of recession and robust growth. Their index has several important features that distinguish it from the Conference Board leading, coincident, and lagging indicators. It is efficient because as a single index it conveys reliable information about the path of the business cycle. Their index gives an independent assessment of the state of the economy because it is constructed from variables that are different than those used by the NBER to identify recessions. It is strictly data driven; hence, it is unaffected by human bias or persuasion. It gives an objective assessment of the business cycle because it is expressed in units of statistical likelihood. And it explicitly accounts for the interaction, along with the level, of the economic variables from which it is constructed.},
  creationdate     = {2023-06-22T22:30:29},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:30:29},
  timestamp        = {2020-07-23 13:37},
}

@Article{Kinlaw-et-al-2021,
  author           = {William B. Kinlaw and Mark Kritzman and S{\'{e}}bastien Page and David Turkington},
  date             = {2021},
  journaltitle     = {The Journal Of Portfolio Management},
  title            = {The Myth of Diversification Reconsidered},
  doi              = {10.3905/jpm.2021.1.273},
  number           = {8},
  volume           = {47},
  abstract         = {That investors should diversify their portfolios is a core principle of modern finance. Yet there are some periods where diversification is undesirable. When the portfolio's main growth engine performs well, investors prefer the opposite of diversification. An ideal complement to the growth engine would provide diversification when it performs poorly and unification when it performs well. Numerous studies have presented evidence of asymmetric correlations between assets. Unfortunately, this asymmetry is often of the undesirable variety: it is characterized by downside unification and upside diversification. In other words, diversification often disappears when it is most needed. In this article we highlight a fundamental flaw in the way that some prior studies have measured correlation asymmetry. Because they estimate downside correlations from subsamples where both assets perform poorly, they ignore instances of "successful" diversification; that is, periods where one asset's gains offset the other's losses. We propose instead that investors measure what matters: the degree to which a given asset diversifies the main growth engine when it underperforms. This approach yields starkly different conclusions, particularly for asset pairs with low full sample correlation. In this paper we review correlation mathematics, highlight the flaw in prior studies, motivate the correct approach, and present an empirical analysis of correlation asymmetry across major asset classes.},
  creationdate     = {2023-06-22T22:30:29},
  modificationdate = {2023-06-22T22:30:29},
  publisher        = {Elsevier {BV}},
  timestamp        = {2021-03-06 14:23},
}

@Article{Kinlaw-et-al-2021a,
  author           = {William B. Kinlaw and Mark Kritzman and S{\'{e}}bastien Page and David Turkington},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {The Myth of Diversification Reconsidered},
  doi              = {10.2139/ssrn.3781844},
  abstract         = {That investors should diversify their portfolios is a core principle of modern finance. Yet there are some periods where diversification is undesirable. When the portfolio's main growth engine performs well, investors prefer the opposite of diversification. An ideal complement to the growth engine would provide diversification when it performs poorly and unification when it performs well. Numerous studies have presented evidence of asymmetric correlations between assets. Unfortunately, this asymmetry is often of the undesirable variety: it is characterized by downside unification and upside diversification. In other words, diversification often disappears when it is most needed. In this article we highlight a fundamental flaw in the way that some prior studies have measured correlation asymmetry. Because they estimate downside correlations from subsamples where both assets perform poorly, they ignore instances of "successful" diversification; that is, periods where one asset's gains offset the other's losses. We propose instead that investors measure what matters: the degree to which a given asset diversifies the main growth engine when it underperforms. This approach yields starkly different conclusions, particularly for asset pairs with low full sample correlation. In this paper we review correlation mathematics, highlight the flaw in prior studies, motivate the correct approach, and present an empirical analysis of correlation asymmetry across major asset classes.},
  creationdate     = {2023-06-22T22:30:29},
  modificationdate = {2023-06-22T22:30:29},
  publisher        = {Elsevier {BV}},
  timestamp        = {2021-03-06 14:23},
}

@Article{Koumou-2020,
  author           = {Gilles Boevi Koumou},
  date             = {2020},
  journaltitle     = {Financial Markets and Portfolio Management},
  title            = {Diversification and portfolio theory: a review},
  doi              = {10.1007/s11408-020-00352-6},
  pages            = {267-312},
  volume           = {34},
  abstract         = {Diversification is one of the major components of investment decision-making under risk or uncertainty. However, paradoxically, as the 2007-2009 financial crisis revealed, the concept remains misunderstood. Our goal in writing this paper is to correct this issue by reviewing the concept in portfolio theory. The core of our review focuses on the following diversification principles: law of large numbers, correlation, capital asset pricing model and risk contribution or risk parity diversification principles. These four diversification principles are the DNA of the existing portfolio selection rules and asset pricing theories and are instrumental to the understanding of diversification in portfolio theory. We review their definition. We also review their optimality, with respect to expected utility theory, and their usefulness. Finally, we explore their measurement.},
  creationdate     = {2023-06-22T22:31:07},
  modificationdate = {2023-06-22T22:31:07},
  timestamp        = {2020-07-25 11:33},
}

@MastersThesis{Kurtti-2020,
  author           = {Kurtti, Markku},
  date             = {2020},
  institution      = {University of Oulu},
  title            = {How many stocks make a diversified portfolio in a continuous-time world?},
  url              = {http://jultika.oulu.fi/Record/nbnfioulu-202011203162},
  abstract         = {This thesis aims to answer how many stocks make a diversified portfolio in a continuous-time world. The study investigates what are the factors determining diversification effects in a real, continuous-time, world as opposed to thoroughly studied theoretical single period world. Continuous-time world investors care about geometric, instead of arithmetic, rate of return.

\leavevmode\newline 

We show how methodology based on information theory can be utilized in investing context. Geometric risk premium is explained by the Shannon limit and its derivative, fractional Kelly criterion. Investing world counterpart for the Shannon limit, compounding process capacity, is derived. Geometric risk premium is decomposed to single stock risk premium and diversification premium. Method for estimating diversification premium is provided. Concept of realizable risk premium is derived and used in risk averse investor diversification metrics. Diversification effect is measured as a (realizable) risk premium ratio and as a (realizable) gross compound excess wealth ratio. Both ratios are between a randomly selected portfolio of selected size and fully diversified benchmark.

\leavevmode\newline 

We show, both analytically and empirically, that diversification in a continuous-time world is a negative price lunch as opposed to free lunch in a single period world. Investor is paid a diversification premium, implying higher geometric risk premium, for consuming a lunch. The magnitude of diversification premium difference to benchmark, the opportunity cost of foregone diversification, is shown to be equal to one half of portfolio's idiosyncratic variance scaled by squared investment fraction. To maintain a constant wealth ratio, required level of diversification for a long-term risk neutral investor is approximately directly proportional to investment time horizon length.

\leavevmode\newline 

The factors determining required level of diversification in a continuous-time world are number of stocks in the benchmark, Sharpe ratio and variance of the benchmark, idiosyncratic variance of an average stock, investment fraction and time. At investment fraction 1.0, risk averse investor requires more than 100, 200 or 1000 stocks to achieve 90\%, 95\% or 99\% of the maximum diversification benefit, respectively. For short-term risk neutral investor, the corresponding numbers are about 20, 40 or 200 stocks and yet significantly more for long-term risk neutral investor. The numbers increase and decrease as investment fraction increase and decrease, respectively. We find that small firms require substantially more diversification compared to large firms and that there are substantial and consistent differences in diversification premiums between investing styles.},
  creationdate     = {2023-06-22T22:39:58},
  modificationdate = {2023-06-22T22:39:58},
}

@Article{Laur-2020,
  author           = {Bhanu Laur},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Portfolio Optimization - Can Optimizing Portfolio Outperform Naive Diversification?},
  doi              = {10.2139/ssrn.3524277},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3524277},
  abstract         = {In this study we examined the performances of mean-variance and tangency portfolio investment strategies in order to determine if optimal diversification has benefits over 1/N strategy.},
  creationdate     = {2023-06-22T22:41:55},
  modificationdate = {2023-06-22T22:41:55},
  timestamp        = {2020-08-11 00:20},
}

@Article{Lim-Ong-2021,
  author           = {Tristan Lim and Chin Sin Ong},
  date             = {2021},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Portfolio Diversification Using Shape-Based Clustering},
  doi              = {10.3905/jfds.2020.1.054},
  number           = {1},
  pages            = {111-126},
  volume           = {3},
  abstract         = {Portfolio diversification involves lowering the correlation between portfolio assets to achieve improved risk-return exposure. It is reasonable to infer from the classic Anscombe quartet that relying on descriptive statistics, and specifically, correlation, to achieve portfolio diversification may not derive the most optimal multiperiod portfolio risk-adjusted return because stocks in a portfolio can exhibit different price trends over time, even with the same computed pairwise correlation. This research applied a shape-based time-series clustering technique of agglomerative hierarchical clustering using dynamic time-series warping as a distance measure to aggregate stocks into like-trending clusters across time as a portfolio diversification tool. Results support the use of the shape-based clustering technique for (1) portfolio allocation and rebalancing, (2) dynamic predictive portfolio construction, and (3) individual stock selection through outlier identification. The findings will be a useful addition to the existing literature in portfolio management by providing shape-based clustering as an alternative tool for portfolio construction and security selection.},
  creationdate     = {2023-06-22T22:43:53},
  modificationdate = {2023-06-22T22:43:53},
  publisher        = {Pageant Media {US}},
  timestamp        = {2021-01-05 11:51},
}

@InCollection{Lohre-et-al-2020,
  author           = {Harald Lohre and Carsten Rother and Kilian Axel Schafer},
  booktitle        = {Machine Learning for Asset Management: New Developments and Financial Applications},
  date             = {2020},
  title            = {Hierarchical Risk Parity: Accounting for Tail Dependencies in Multi-asset Multi-factor Allocations},
  doi              = {10.1002/9781119751182.ch9},
  editor           = {Emmanuel Jurczenko},
  pages            = {329--368},
  publisher        = {Wiley},
  abstract         = {This chapter examines the use and merits of hierarchical clustering techniques in the context of multi-asset multi-factor investing. In particular, it contrasts these techniques with several competing risk-based allocation paradigms, such as 1/N, minimum-variance, standard risk parity and diversified risk parity. The chapter introduces hierarchical risk parity (HRP) strategies based on the Pearson correlation coefficient and also introduces hierarchical clustering based on the lower tail dependence coefficient. The chapter provides an overview of traditional risk-based allocation strategies and outlines a framework to measure and manage portfolio diversification. It examines the performance of the introduced HRP strategies relative to the traditional alternatives. The chapter discusses Meucci's approach to managing diversification, which serves to construct a diversified risk parity strategy based on economic factors.},
  creationdate     = {2023-06-22T22:44:38},
  modificationdate = {2023-06-22T22:44:38},
  timestamp        = {2020-08-08 15:24},
}

@Article{dePrado-2023c,
  author           = {Marcos L{\'{o}}pez de Prado},
  date             = {2023},
  journaltitle     = {{SSRN} Electronic Journal},
  title            = {The Hierarchy of Empirical Evidence in Finance},
  doi              = {10.2139/ssrn.4425855},
  abstract         = {Recent progress in causal inference has opened a path, however difficult, for advancing financial economics beyond its current phenomenological stage. The goal of this article is to propose a hierarchy of empirical evidence, recognizing that not all types of observations have the same scientific weight, in the sense of enabling the falsification of causal claims.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2023a,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023},
  journaltitle     = {SSRN e-Print},
  title            = {Pseudo-Factors and Factor Investing},
  doi              = {10.2139/ssrn.4336002},
  abstract         = {In this article, I advocate for the use of causal graphs to modernize the field of factor investing, and set it on a logically-coherent foundation. In order to do that, first I must introduce the concepts of association and causations. Second, I explain the use of causal graphs and the real (causal) meaning of the "ceteris paribus" assumption that is so popular among economists. Third, I explain how causal graphs help us estimate causal effects in observational (non-experimental) studies. Fourth, I illustrate all of the earlier concepts with Monte Carlo experiments. Fifth, I conclude that the field of factor investing must embrace causal graphs in order to wake up from its associational slumber.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2023,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023-02},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Where Are the Factors in Factor Investing?},
  doi              = {10.3905/jpm.2023.1.477},
  number           = {5},
  pages            = {6-20},
  volume           = {49},
  abstract         = {In this article, the author advocates for the use of causal graphs to modernize the field of factor investing and set it on a logically coherent foundation. To do this, first he introduces the concepts of association and causation. Second, he explains the use of causal graphs and the real (causal) meaning of the ceteris paribus assumption that is so popular among economists. Third, he explains how causal graphs help us estimate causal effects in observational (nonexperimental) studies. Fourth, he illustrates all of the earlier concepts with Monte Carlo experiments. He concludes that the field of factor investing must embrace causal graphs in order to wake up from its associational slumber.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Pageant Media {US}},
}

@Article{dePrado-2023b,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023},
  journaltitle     = {SSRN e-Print},
  title            = {Can Factor Investing Become Scientific? (Seminar Slides)},
  doi              = {10.2139/ssrn.4324450},
  abstract         = {Virtually all journal articles in the factor investing literature make associational claims, instead of causal claims. Authors do not identify the causal graph consistent with the observed phenomenon, they justify their chosen model specification in terms of correlations, and they do not propose experiments for falsifying causal mechanisms. Absent a causal theory, their findings are likely false, due to rampant backtest overfitting and incorrect specification choices.

I differentiate between type-A and type-B spurious claims, and explain how both types prevent factor investing from advancing beyond its current pre-scientific stage. This seminar analyzes the current state of causal confusion in the factor investing literature, and proposes solutions with the potential to transform factor investing into a truly scientific discipline.

The full manuscript can be found here: http://ssrn.com/abstract=4205613.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2022a,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2022-07},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Machine Learning for Econometricians: The Readme Manual},
  doi              = {10.3905/jfds.2022.1.101},
  number           = {3},
  pages            = {10-30},
  volume           = {4},
  abstract         = {One of the most exciting recent developments in financial research is the availability of new administrative, private sector, and micro-level datasets that did not exist a few years ago. The unstructured nature of many of these observations, along with the complexity of the phenomena they measure, means that many of these datasets are beyond the grasp of econometric analysis. Machine learning (ML) techniques offer the numerical power and functional flexibility needed to identify complex patterns in a high-dimensional space. ML is often perceived as a black box, however, in contrast to the transparency of econometric approaches. In this article, the author demonstrates that each analytical step of the econometric process has a homologous step in ML analyses. By clearly stating this correspondence, the author's goal is to facilitate and reconcile the adoption of ML techniques among econometricians.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Pageant Media {US}},
}

@Article{dePrado-2022b,
  author           = {{L{\'{o}}pez de Prado}, Marcos},
  date             = {2022-07},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Type I and Type {II} Errors of the Sharpe Ratio under Multiple Testing},
  doi              = {10.3905/jpm.2022.1.403},
  number           = {1},
  pages            = {39-46},
  volume           = {49},
  abstract         = {Articles in financial literature typically estimate the p-value associated with an investment strategy's performance without reporting the power of the test used to make that discovery. In this article, the author provides analytic estimates to Type I and Type II errors for the Sharpe ratios of investments and derives their familywise counterparts. These estimates allow researchers to carefully design experiments and select investments with high confidence and power.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Pageant Media {US}},
}

@Article{dePrado-2022,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Escaping The Sisyphean Trap: How Quants Can Achieve Their Full Potential},
  doi              = {10.2139/ssrn.3916692},
  abstract         = {Investing can be characterized as a data science problem. While investment firms have attracted scientific talent, they have done a poor job at developing it. Firms hire specialists, but entice them to become generalists (e.g., portfolio managers). Under the ubiquitous silo/platform structure, quants succumb to the Sisyphean trap, and do not achieve their full potential.

A research lab structure offers a unique environment for developing scientists, by means of: (a) co-specialization, working in a highly cooperative lab environment; (b) tackling well-defined open investment problems; and (c) applying the scientific method.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2020g,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Advances in Financial Machine Learning: NumeraI's Tournament (Presentation Slides)},
  doi              = {10.2139/ssrn.3478927},
  abstract         = {Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. In this course, we discuss scientifically sound ML tools that have been successfully applied to the management of large pools of funds.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2019g,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019-11-09},
  journaltitle     = {SSRN e-Print},
  title            = {Estimation of Theory-Implied Correlation Matrices},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3484152},
  urldate          = {2019-11-14},
  abstract         = {Correlation matrices are ubiquitous in finance. Some key applications include portfolio construction, risk management, and factor/style analysis. Correlation matrices are usually estimated from historical empirical observations or derived from historically estimated factors. It is widely acknowledged that empirical correlation matrices: (a) have poor numerical properties that lead to unreliable estimators; and (b) have poor predictive power. Additionally, factor-based correlation matrices have their own caveats. In particular, estimated factors are typically non-hierarchical and do not allow for interactions at different levels. This contravenes the fact that financial instruments typically exhibit a nested cluster structure (e.g., MSCI GICS levels 1-4).This paper introduces a machine learning (ML) algorithm to estimate forward-looking correlation matrices implied by economic theory. Given a particular theoretical representation of the hierarchical structure that governs a universe of securities, the method fits the correlation matrix that complies with that theoretical representation of the future. This particular use case demonstrates how, contrary to popular perception, ML solutions are not black-boxes, and can be applied effectively to develop and test economic theories.},
  creationdate     = {2023-06-22T22:45:31},
  day              = {9},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:31},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2021a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Detection of False Investment Strategies through {FWER} and {FDR} (Seminar Slides)},
  doi              = {10.2139/ssrn.3799803},
  abstract         = {Financial systems rarely allow experimentation. For example, we cannot reproduce the flash crash of 2010 while controlling for environmental conditions. As a result, much financial research relies on the statistical analysis of finite (historical) datasets, where: (a) Time series datasets are limited, and (b) The investment universe is limited.

The implication is that a large number of hypotheses are tested on the same observations. In the context of asset management, this situation leads to false investment strategies and losses, particularly among quantitative funds.

This seminar explains how to detect false investment strategies by controlling for the familywise error rate (FWER) and the false discovery rate (FDR) of an organization. It is part of Cornell University's ORIE 5256 course.},
  creationdate     = {2023-06-22T22:45:31},
  modificationdate = {2023-06-22T22:45:31},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2013,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2013-04},
  journaltitle         = {SSRN e-Print},
  title                = {How Long Does It Take to Recover from a Drawdown?},
  url                  = {https://ssrn.com/abstract=2254668},
  abstract             = {Investment management firms routinely hire and fire employees based on the performance of their portfolios. Such performance is evaluated through popular metrics that assume IID Normal returns, like Sharpe ratio, Sortino ratio, Treynor ratio, Information ratio, etc.

Investment returns are far from IID Normal.

Conclusion 1: Firms evaluating performance through Sharpe ratio are firing up to three times more skillful managers than originally targeted. This is very costly to firms and investors, and is a direct consequence of wrongly assuming that returns are IID Normal.

Conclusion 2: An accurate performance evaluation methodology is worth a substantial portion of the fees paid to hedge funds. There is a 20 percent loss of the drawdown for every false positive. For a large firm, this amounts to tens of millions of dollars lost annually, as a result of wrongly assuming that returns are IID Normal.},
  citeulike-article-id = {13780321},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2254668},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2460364code434076.pdf?abstractid=2254668 and mirid=1},
  creationdate         = {2023-06-22T22:45:31},
  day                  = {22},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2254668},
  keywords             = {portfolio, portfolio-allocation, sharpe-ratio},
  modificationdate     = {2023-06-22T22:45:31},
  owner                = {zkgst0c},
  posted-at            = {2016-01-09 17:45:42},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2016a,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2015-12},
  journaltitle         = {SSRN e-Print},
  title                = {Building Diversified Portfolios that Outperform Out-of-Sample},
  url                  = {https://ssrn.com/abstract=2708678},
  abstract             = {This paper introduces the Hierarchical Risk Parity (HRP) approach. HRP portfolios address three major concerns of quadratic optimizers in general and Markowitz's CLA in particular: Instability, concentration and underperformance.HRP applies modern mathematics (graph theory and machine learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix, an impossible feat for quadratic optimizers. Monte Carlo experiments show that HRP delivers lower out-of-sample variance than CLA, even though minimum-variance is CLA's optimization objective. HRP also produces less risky portfolios out-of-sample compared to traditional risk parity methods.A presentation can be found at http://ssrn.com/abstract=2713516.},
  citeulike-article-id = {14130573},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2708678},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2783555code434076.pdf?abstractid=2708678 and mirid=1},
  creationdate         = {2023-06-22T22:45:31},
  day                  = {28},
  modificationdate     = {2023-06-22T22:45:31},
  owner                = {zkgst0c},
  posted-at            = {2016-09-05 23:16:46},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2016,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2016-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Building Diversified Portfolios that Outperform Out of Sample},
  doi                  = {10.3905/jpm.2016.42.4.059},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {59--69},
  volume               = {42},
  abstract             = {In this article, the author introduces the Hierarchical Risk Parity (HRP) approach to address three major concerns of quadratic optimizers, in general, and Markowitz's critical line algorithm (CLA), in particular: instability, concentration, and underperformance. HRP applies modern mathematics (graph theory and machine-learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix an impossible feat for quadratic optimizers. Monte Carlo experiments show that HRP delivers lower out-ofsample variance than CLA, even though minimum variance is CLA's optimization objective. HRP also produces less risky portfolios out of sample compared to traditional risk parity methods.},
  citeulike-article-id = {14130574},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2016.42.4.059},
  creationdate         = {2023-06-22T22:45:32},
  groups               = {Networks and investment management, Clustering and network analysis, Machine learning and investment strategies, ML_Network_QWIM, ML_PerfMetrics, Invest_Diversif},
  modificationdate     = {2023-06-22T22:45:32},
  owner                = {zkgst0c},
  posted-at            = {2016-09-05 23:17:29},
  timestamp            = {2019-10-11 18:45},
}

@Article{dePrado-2016b,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2016-08},
  journaltitle         = {SSRN e-Print},
  title                = {Mathematics and Economics: A Reality Check},
  url                  = {https://ssrn.com/abstract=2819847},
  abstract             = {Economics (and by extension finance) is arguably one of the most mathematical fields of research. However, economists' choice of math may be inadequate to model the complexity of social institutions.In a constructive spirit, this note offers some advice on how students could increase their chances of having a successful career in 21st century finance. Practitioners seeking to enhance their skillset may also draw some ideas.},
  citeulike-article-id = {14119852},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2819847},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2826891code434076.pdf?abstractid=2819847 and mirid=1},
  creationdate         = {2023-06-22T22:45:32},
  day                  = {13},
  modificationdate     = {2023-06-22T22:45:32},
  owner                = {zkgst0c},
  posted-at            = {2016-08-22 12:17:26},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2017,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2017-09},
  journaltitle         = {SSRN e-Print},
  title                = {The 7 Reasons Most Machine Learning Funds Fail},
  url                  = {https://ssrn.com/abstract=3031282},
  abstract             = {The rate of failure in quantitative finance is high, and particularly so in financial machine learning. The few managers who succeed amass a large amount of assets, and deliver consistently exceptional performance to their investors. However, that is a rare outcome, for reasons that will become apparent in this presentation. Over the past two decades, I have seen many faces come and go, firms started and shut down. In my experience, there are 7 critical mistakes underlying most of those failures.},
  citeulike-article-id = {14428199},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3031282},
  creationdate         = {2023-06-22T22:45:32},
  modificationdate     = {2023-06-22T22:45:32},
  posted-at            = {2017-09-09 20:49:08},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2018c,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2018},
  journaltitle         = {SSRN e-Print},
  title                = {The 10 Reasons Most Machine Learning Funds Fail},
  series               = {dePrado-2018a},
  url                  = {https://ssrn.com/abstract=3104816},
  abstract             = {The rate of failure in quantitative finance is high, and particularly so in financial machine learning. The few managers who succeed amass a large amount of assets, and deliver consistently exceptional performance to their investors. However, that is a rare outcome, for reasons that will become apparent in this article. Over the past two decades, I have seen many faces come and go, firms started and shut down. In my experience, there are ten critical mistakes underlying most of those failures.},
  citeulike-article-id = {14520323},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3104816},
  creationdate         = {2023-06-22T22:45:37},
  modificationdate     = {2023-06-22T22:45:37},
  posted-at            = {2018-01-19 21:50:44},
  timestamp            = {2020-07-23 13:38},
}

@Book{dePrado-2018,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2018},
  title                = {Advances in Financial Machine Learning},
  pagetotal            = {400},
  publisher            = {Wiley},
  url                  = {https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086},
  abstract             = {Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. Readers will learn how to structure Big data in a way that is amenable to ML algorithms; how to conduct research with ML algorithms on that data; how to use supercomputing methods; how to backtest your discoveries while avoiding false positives. The book addresses real-life problems faced by practitioners on a daily basis, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their particular setting. Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance.},
  citeulike-article-id = {14520319},
  creationdate         = {2023-06-22T22:45:37},
  groups               = {ML_BestPractices, ML_Interpretability, ML_Test_FalsePosNeg},
  modificationdate     = {2023-06-22T22:45:37},
  posted-at            = {2018-01-19 21:46:14},
  timestamp            = {2020-02-03 16:43},
}

@Article{dePrado-2018e,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-03},
  journaltitle     = {SSRN e-Print},
  title            = {How the Sharpe Ratio Died, and Came Back to Life},
  url              = {https://ssrn.com/abstract=3173146},
  creationdate     = {2023-06-22T22:45:37},
  day              = {3},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:37},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-16},
  journaltitle     = {SSRN e-Print},
  title            = {A Practical Solution to the Multiple-Testing Crisis in Financial Research},
  url              = {https://ssrn.com/abstract=3179826},
  abstract         = {Most discoveries in empirical finance are false, as a consequence of selection bias under multiple testing. This may explain why so many hedge funds fail to perform as advertised or as expected, particularly in the quantitative space. These false discoveries may have been prevented if academic journals and investors demanded that any reported investment performance incorporates the false positive probability, adjusted for selection bias under multiple testing. In this presentation, we demonstrate how this adjusted false positive probability can be computed and reported for public consumption. The full paper can be downloaded at http://ssrn.com/abstract=3177057},
  creationdate     = {2023-06-22T22:45:37},
  day              = {16},
  f1000-projects   = {QuantInvest},
  groups           = {Proba_Test},
  modificationdate = {2023-06-22T22:45:37},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018b,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-12},
  journaltitle     = {SSRN e-Print},
  title            = {Financial Machine Learning in 10 Minutes},
  url              = {https://ssrn.com/abstract=3177534},
  creationdate     = {2023-06-22T22:45:37},
  day              = {12},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:37},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018d,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-06-10},
  journaltitle     = {SSRN e-Print},
  title            = {Market Microstructure in the Age of Machine Learning},
  url              = {https://ssrn.com/abstract=3193702},
  abstract         = {In this presentation, we analyze the explanatory (in-sample) and predictive (out-of-sample) importance of some of the best known market microstructural features. Our conclusions are drawn over the entire universe of the 87 most liquid futures worldwide, covering all asset classes, going back through 10 years of tick-data history.},
  creationdate     = {2023-06-22T22:45:37},
  day              = {10},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:37},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018f,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018},
  journaltitle     = {SSRN e-Print},
  title            = {Significance thresholds of 5\% are suboptimal in finance},
  doi              = {10.2139/ssrn.3201981},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3201981},
  abstract         = {Full paper is available at: https://ssrn.com/abstract=3193697 Most papers in the financial literature control for Type I errors (false positive rate), while ignoring Type II errors (false negative rate). This is a mistake, because a low Type I error can only be achieved at the cost of a high Type II error.Contrary to long-held beliefs, a familywise significance level below 15\% is suboptimal (excessively conservative) in the context of most investment strategies.},
  creationdate     = {2023-06-22T22:45:37},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:37},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018g,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-16},
  journaltitle     = {SSRN e-Print},
  title            = {A Practical Solution to the Multiple-Testing Crisis in Financial Research},
  url              = {https://ssrn.com/abstract=3177057},
  abstract         = {Most discoveries in empirical finance are false, as a consequence of selection bias under multiple testing. This may explain why so many hedge funds fail to perform as advertised or as expected, particularly in the quantitative space. These false discoveries may have been prevented if academic journals and investors demanded that any reported investment performance incorporates the false positive probability, adjusted for selection bias under multiple testing. In this paper we demonstrate how this adjusted false positive probability can be computed and reported for public consumption. The full paper can be downloaded at http://ssrn.com/abstract=3177057},
  creationdate     = {2023-06-22T22:45:39},
  day              = {16},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{Fabozzi-dePrado-2018,
  author           = {Fabozzi, Frank J. and {Lopez de Prado}, Marcos},
  date             = {2018-10-31},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Being Honest in Backtest Reporting: A Template for Disclosing Multiple Tests},
  doi              = {10.3905/jpm.2018.45.1.141},
  number           = {1},
  pages            = {141-147},
  url              = {https://jpm.pm-research.com/content/45/1/141},
  volume           = {45},
  abstract         = {Selection bias under multiple testing is a serious problem. From a practitioner perspective, failure to disclose the impact of multiple tests of a proposed investment strategy to clients and senior management can lead to the adoption of a false discovery. Clients will lose money, senior management will misallocate resources, and the firm may be exposed to reputational, legal, and regulatory risks. From the perspective of academic journals that publish evidence supporting an investment strategy, the failure to address selection bias under multiple testing threatens to invalidate large portions of the literature in empirical finance. In this article, the authors propose a template that practitioners should use to fairly disclose multiple tests involved in an alleged discovery when pitching strategies to clients and senior management. The same template could be used by contributors to academic journals so that referees, and ultimately readers, can assess the strategy. By disclosing this information, those who are charged with making the final decision about a discovery can evaluate the probability that the purported discovery is false.},
  creationdate     = {2023-06-22T22:45:39},
  day              = {31},
  f1000-projects   = {QuantInvest},
  groups           = {Proba_Test},
  modificationdate = {2023-06-22T22:45:39},
  publisher        = {Institutional Investor Journals Umbrella},
  timestamp        = {2020-07-03 01:19},
}

@Article{dePrado-2019,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019-04-16},
  journaltitle     = {SSRN e-Print},
  title            = {The 7 Reasons Most Econometric Investments Fail},
  url              = {https://ssrn.com/abstract=3373116},
  urldate          = {2019-04-19},
  abstract         = {This presentation reviews the main reasons why investment strategies discovered through econometric methods fail. As a solution, it proposes the modernization of the statistical methods used by financial firms and academic authors.This material is part of Cornell University's ORIE 5256 graduate course at the School of Engineering.},
  creationdate     = {2023-06-22T22:45:39},
  day              = {16},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019-02-01},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {A Data Science Solution to the Multiple-Testing Crisis in Financial Research},
  doi              = {10.3905/jfds.2019.1.099},
  issn             = {2640-3951},
  number           = {1},
  pages            = {99-110},
  url              = {https://jfds.iijournals.com/content/1/1/99},
  urldate          = {2019-04-29},
  volume           = {1},
  abstract         = {Most discoveries in empirical finance are false, as a consequence of selection bias under multiple testing. Although many researchers are aware of this problem, the solutions proposed in the literature tend to be complex and hard to implement. In this article, the author reduces the problem of selection bias in the context of investment strategy development to two sub-problems: determining the number of essentially independent trials and determining the variance across those trials. The author explains what data researchers need to report to allow others to evaluate the effect that multiple testing has had on reported performance. He applies his method to a real case of strategy development and estimates the probability that a discovered strategy is false.},
  creationdate     = {2023-06-22T22:45:39},
  day              = {1},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2019-09-16 12:41},
}

@Article{dePrado-2019b,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Printl},
  title            = {The past and future of quantitative research},
  doi              = {10.2139/ssrn.3447561},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3447561},
  urldate          = {2019-10-02},
  abstract         = {Traditionally, the development of investment strategies has required domain-specific knowledge and access to restricted datasets. These two barriers exist by design: (a) Financial knowledge is hoarded by firms, and protected as trade secrets, and (b) Financial data is expensive, making it inaccessible to the broad scientific community.This presentation explores how these two barriers impact the quality of quantitative research, and how investment tournaments can help deliver better investment outcomes by overcoming those two barriers.},
  creationdate     = {2023-06-22T22:45:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019c,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Tactical Investment Algorithms},
  doi              = {10.2139/ssrn.3459866},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3459866},
  urldate          = {2019-10-24},
  abstract         = {There are three fundamental ways of testing the validity of an investment algorithm against historical evidence: a) the walk-forward method; b) the resampling method; and c) the Monte Carlo method. By far the most common approach followed among academics and practitioners is the walk-forward method. Implicit in that choice is the assumption that a given investment algorithm should be deployed throughout all market regimes. We denote such assumption the -weather hypothesis, and the algorithms based on that hypothesis investment algorithms (or strategies). The all-weather hypothesis is not necessarily true, as demonstrated by the fact that many investment strategies have floundered in a zero-rate environment. This motivates the problem of identifying investment algorithms that are optimal for specific market regimes, denoted investment algorithms. This paper argues that backtesting against synthetic datasets should be the preferred approach for developing tactical investment algorithms. A new organizational structure for asset managers is proposed, as a tactical algorithmic factory, consistent with the Monte Carlo backtesting paradigm.},
  creationdate     = {2023-06-22T22:45:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019d,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {A robust estimator of the efficient frontier},
  doi              = {10.2139/ssrn.3469961},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3469961},
  urldate          = {2019-10-24},
  abstract         = {Convex optimization solutions tend to be unstable, to the point of entirely offsetting the benefits of optimization. For example, in the context of financial applications, it is known that portfolios optimized in-sample often underperform the naive (equal weights) allocation out-of-sample. This instability can be traced back to two sources: (i) noise in the input variables; and (ii) signal structure that magnifies the estimation errors in the input variables. A first innovation of this paper is to introduce the nested clustered optimization algorithm ({NCO}), a method that tackles both sources of instability.Over the past 60 years, various approaches have been developed to address these two sources of instability. These approaches are flawed in the sense that different methods may be appropriate for different input variables, and it is unrealistic to expect that one method will dominate all the rest under all circumstances. Accordingly, a second innovation of this paper is to introduce {MCOS}, a Monte Carlo approach that estimates the allocation error produced by various optimization methods on a particular set of input variables. The result is a precise determination of what method is most robust to a particular case. Thus, rather than relying always on one particular approach, {MCOS} allows users to apply opportunistically whatever optimization method is best suited in a particular setting.Presentation materials are available at: https://ssrn.com/abstract=3469964.},
  creationdate     = {2023-06-22T22:45:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019e,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Machine Learning Asset Allocation},
  doi              = {10.2139/ssrn.3469964},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3469964},
  urldate          = {2019-10-24},
  abstract         = {Convex optimization solutions tend to be unstable, to the point of entirely offsetting the benefits of optimization. For example, in the context of financial applications, it is known that portfolios optimized in sample often underperform the naive (equal weights) allocation out of sample.This instability can be traced back to two sources: (1) noise in the input variables; and (2) signal structure that magnifies the estimation errors in the input variables.There is abundant literature discussing noise induced instability. In contrast, signal induced instability is often ignored or misunderstood.We introduce a new optimization method that is robust to signal induced instability.For additional details, see the full paper at: https://ssrn.com/abstract=3469961.},
  creationdate     = {2023-06-22T22:45:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Codependence},
  doi              = {10.2139/ssrn.3512994},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3512994},
  urldate          = {2020-01-19},
  abstract         = {Two random variables are codependent when knowing the value of one helps us determine the value of the other. This should not me confounded with the notion of causality.Correlation is perhaps the best known measure of codependence in econometric studies. Despite its popularity among economists, correlation has many known limitations in the contexts of financial studies.In this seminar we will explore more modern measures of codependence, based on information theory, which overcome some of the limitations of correlations.},
  creationdate     = {2023-06-22T22:45:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:39},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Clustering},
  doi              = {10.2139/ssrn.3512998},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3512998},
  urldate          = {2020-01-19},
  abstract         = {Many problems in finance require the clustering of variables or observations. Despite its usefulness, clustering is almost never taught in Econometrics courses. In this seminar we review two general clustering approaches: partitional and hierarchical.},
  creationdate     = {2023-06-22T22:45:41},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020b,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Overfitting: causes and solutions},
  doi              = {10.2139/ssrn.3544431},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3544431},
  urldate          = {2020-03-06},
  abstract         = {When used incorrectly, the risk of machine learning (ML) overfitting is extremely high. However, ML counts with sophisticated methods to prevent: (a) train set overfitting, and (b) test set overfitting.Thus, the popular belief that ML overfits is false. A more accurate statement would be that: (1) in the wrong hands, ML overfits, and (2) in the right hands, ML is more robust to overfitting than classical methods.When it comes to modelling unstructured data, ML is the only choice. Classical statistics should be taught as a preparation for ML courses, with a focus on overfitting prevention.},
  creationdate     = {2023-06-22T22:45:41},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020c,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Clustered feature importance (presentation slides)},
  doi              = {10.2139/ssrn.3517595},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3517595},
  urldate          = {2020-03-28},
  abstract         = {A substitution effect takes place when two or more explanatory variables share a substantial amount of information (predictive power).Under the presence of substitution effects, feature importance methods may not be able to determine robustly which variables are significant.This presentation discusses the Clustered Feature Importance (CFI) method, which is robust to linear as well as non-linear substitution effects.},
  creationdate     = {2023-06-22T22:45:41},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-07-23 13:38},
}

@Book{dePrado-2020f,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020-04-30},
  title            = {Machine learning for asset managers},
  doi              = {10.1017/9781108883658},
  isbn             = {9781108792899},
  pagetotal        = {190},
  publisher        = {Cambridge University Press},
  url              = {https://www.cambridge.org/core/product/identifier/9781108883658/type/element},
  urldate          = {2020-04-09},
  abstract         = {Successful investment strategies are specific implementations of general theories. An investment strategy that lacks a theoretical justification is likely to be false. Hence, an asset manager should concentrate her efforts on developing a theory rather than on backtesting potential trading rules. The purpose of this Element is to introduce machine learning ({ML}) tools that can help asset managers discover economic and financial theories. {ML} is not a black box, and it does not necessarily overfit. {ML} tools complement rather than replace the classical statistical methods. Some of {ML}'s strengths include (1) a focus on out-of-sample predictability over variance adjudication; (2) the use of computational methods to avoid relying on (potentially unrealistic) assumptions; (3) the ability to  learn  complex specifications, including nonlinear, hierarchical, and noncontinuous interaction effects in a high-dimensional space; and (4) the ability to disentangle the variable search from the specification search, robust to multicollinearity and other substitution effects.},
  creationdate     = {2023-06-22T22:45:41},
  day              = {30},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-08-30 04:47},
}

@Article{dePrado-2019f,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Beyond econometrics: A roadmap towards financial machine learning},
  doi              = {10.2139/ssrn.3365282},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3365282},
  urldate          = {2020-06-03},
  abstract         = {One of the most exciting recent developments in financial research is the availability of new administrative, private sector and micro-level datasets that did not exist a few years ago. The unstructured nature of many of these observations, along with the complexity of the phenomena they measure, means that many of these datasets are beyond the grasp of econometric analysis. Machine learning ({ML}) techniques offer the numerical power and functional flexibility needed to identify complex patterns in a high-dimensional space. However, {ML} is often perceived as a black box, in contrast with the transparency of econometric approaches. This article demonstrates that each analytical step of the econometric process has a homologous step in {ML} analyses. By clearly stating this correspondence, our goal is to facilitate and reconcile the adoption of {ML} techniques among econometricians.},
  creationdate     = {2023-06-22T22:45:41},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020d,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Three Machine Learning Solutions to the Bias-Variance Dilemma},
  doi              = {10.2139/ssrn.3588594},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3588594},
  urldate          = {2020-05-31},
  abstract         = {Classical statistics (e.g., Econometrics) relies on assumptions that are often unrealistic in finance. Two critical assumptions are that the researcher has perfect knowledge about the model  specification, and that the researcher knows all the variables involved in a phenomenon (including all interaction effects). When those assumptions are incorrect, classical estimators are not guaranteed to be unbiased, or to be the most efficient among the unbiased, leading to poor performance.In this presentation we explore why machine learning algorithms are generally more appropriate for financial datasets, how they outperform classical estimators, and how they solve the bias-variance dilemma.},
  creationdate     = {2023-06-22T22:45:41},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-08-30 04:27},
}

@Article{dePrado-2020e,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {Alternative Investment Analyst Review},
  title            = {Tactical Investment Algorithms},
  number           = {1},
  url              = {https://caia.org/aiar/4144#aiar-default-5},
  volume           = {9},
  abstract         = {Finance has two major limitations that prevent it from becoming a science, unlike physics, chemistry or biology. These two limitations, Popper's falsifiability criterion and complexity in the changing financial system, force researchers to rely on backtesting when creating investment algorithms. There are three types of backtests, which includes the walk-forward method, the resampling method, and the Monte Carlo (MC) method. In this paper, Lopez de Prado argues the MC method as the most useful of the three types of backtests. The MC method is further discussed with a practical example, a discussion of its advantages and criticisms, and finally a deeper dive into a key part of MC analysis referred to as the data-generating process (DGP).},
  creationdate     = {2023-06-22T22:45:41},
  modificationdate = {2023-06-22T22:45:41},
  timestamp        = {2020-06-24 22:00},
}

@Article{Madan-Wang-2021,
  author           = {Dilip B. Madan and King Wang},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Measuring the Benefits of Diversification Across Portfolios},
  doi              = {10.2139/ssrn.3813203},
  abstract         = {A portfolio diversification index is defined as the ratio of an equivalent number of independent assets to the number of assets. The equivalence is based on either attaining the same diversification benefit or spread reduction. The diversification benefit is the difference in value of a value maximizing portfolio and the maximum value of the components. The spread reduction is the percentage reduction attained by a spread minimizing portfolio relative to the smallest spread for the components. Asset values, bid and ask, are given by conservative valuation operators from the theory of two price economies. The diversification indices fall with the number of assets in the portfolio and they are explained by a measure of concentration applied to normalized eigenvalues of the correlation matrix along with the average level of correlation. A time series of the indices constructed on the basis of the S\&P 500 index and the nine sector ETF's reveals a collapse during the final crisis with no recovery until 2016, peaking in February 2020 and a COVID crash in March of 2020. Furthermore factor diversification possibilities are richer than those found in equity indices. Diversification benefits across global indices are not as strong as diversification across an equal number of domestic assets, but they rise substantially for longer horizons of up to three years.},
  creationdate     = {2023-06-22T22:46:51},
  modificationdate = {2023-06-22T22:46:51},
  publisher        = {Elsevier {BV}},
}

@Article{Molyboga-2020,
  author           = {Marat Molyboga},
  date             = {2020},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {A Modified Hierarchical Risk Parity Framework for Portfolio Management},
  number           = {3},
  pages            = {128-139},
  url              = {https://jfds.pm-research.com/content/early/2020/07/03/jfds.2020.1.038},
  volume           = {2},
  abstract         = {This article introduces a modified hierarchical risk parity (MHRP) approach that extends the HRP approach by incorporating three intuitive elements commonly used by practitioners. The new approach (1) replaces the sample covariance matrix with an exponentially weighted covariance matrix with Ledoit-Wolf shrinkage; (2) improves diversification across portfolio constituents both within and across clusters by relying on an equal volatility, rather than an inverse variance, allocation approach; and (3) improves diversification across time by applying volatility targeting to portfolios. The author examines the impact of the enhancements on portfolios of commodity trading advisors within a large-scale Monte Carlo simulation framework that accounts for the realistic constraints of institutional investors. The author finds a striking improvement in the out-of-sample Sharpe ratio of 50\%, on average, along with a reduction in downside risk.},
  creationdate     = {2023-06-22T22:47:37},
  modificationdate = {2023-06-22T22:47:37},
  timestamp        = {2020-08-05 09:42},
}

@Article{Molyboga-2020a,
  author           = {Marat Molyboga},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Back to Basis: A Universal Return Predictor Across Asset Classes},
  doi              = {10.2139/ssrn.3690628},
  abstract         = {This paper shows analytically that the basis between spot and futures contracts contains information about future returns of securities across the asset classes of commodities, equity indices, fixed income and foreign exchange. The bases in commodities are positively correlated with a leading indicator of the business cycle whereas the bases in the financial assets are negatively related to the short-term rate. The return predictability of the basis can be captured with a simple multi-asset long-short strategy which produces an out-of-sample Sharpe ratio of 0.5 and an alpha of 2.5\%-4.5\% per annum with respect to commonly used asset pricing models. Specifically, the analysis includes five Fama-French Factors, a bond index and futures risk premia of multi-asset momentum, value, time-series momentum, and four asset-specific carry factors. The strategy performance is counter-cyclical and robust to transaction costs.},
  creationdate     = {2023-06-22T22:47:37},
  modificationdate = {2023-06-22T22:47:37},
  publisher        = {Elsevier {BV}},
  timestamp        = {2020-10-31 16:22},
}

@Article{Olmo-2021,
  author           = {Jose Olmo},
  date             = {2021},
  journaltitle     = {Quantitative Finance},
  title            = {Optimal portfolio allocation and asset centrality revisited},
  doi              = {10.1080/14697688.2021.1937298},
  number           = {9},
  pages            = {1475-1490},
  volume           = {21},
  abstract         = {This paper revisits the relationship between eigenvector asset centrality and optimal asset allocation in a minimum variance portfolio. We show that the standard definition of eigenvector centrality is misleading when the adjacency matrix in a network can take negative values. This is, for example, the case when the network topology is induced by the correlation matrix between assets in a portfolio. To correct for this, we introduce the concept of positive and negative eigenvector centrality. Our results show that the loss function associated to the minimum variance portfolio is positively/negatively related to the positive and negative eigenvector centrality under short-selling constraints but cannot be generalized beyond that. Furthermore, in contrast to what is claimed in the related literature, this relationship does not imply any monotonic relationship between the centrality of an asset and its optimal portfolio allocation. These theoretical insights are illustrated empirically in a portfolio allocation exercise with assets from U.S. and U.K. financial markets.},
  creationdate     = {2023-06-22T22:48:17},
  modificationdate = {2023-06-22T22:48:17},
  publisher        = {Informa {UK} Limited},
}

@Article{Page-2020a,
  author           = {Sebastien Page},
  date             = {2020},
  journaltitle     = {Investments and Wealth Monitor},
  title            = {When Diversification Fails},
  url              = {https://investmentsandwealth.org/getattachment/a9eb2e62-c386-4466-8c4d-20d1242fd55e/IWM21SepOct-WhenDiversificationFails.pdf},
  abstract         = {This article summarizes the key concepts from Beyond Diversification: What Every Investor Needs to Know About Asset Allocation (McGraw Hill, 2020) by Sebastien Page},
  creationdate     = {2023-06-22T23:18:53},
  modificationdate = {2023-06-22T23:18:53},
  timestamp        = {2023-01-04},
}

@Book{Page-2020,
  author           = {Sebastien Page},
  date             = {2020},
  title            = {Beyond Diversification: What Every Investor Needs to Know About Asset Allocation},
  pagetotal        = {220},
  publisher        = {McGraw Hill},
  url              = {https://www.mhebooklibrary.com/doi/book/10.1036/9781260474886},
  abstract         = {Written by a leading allocation expert from T. Rowe Price, Beyond Diversification provides the knowledge, insights, and approaches you need to make the best allocation decisions for your goals. This deep dive into the how's and why's of asset allocation is organized by the three decisive components of a successfully allocated portfolio:

\begin{enumerate}
\item Return Forecasting discusses the desired return investors seek.
\item Risk Forecasting covers the level of risk investors are prepared to assume to achieve that return.
\item Portfolio Construction calibrates the stock-bond mix that balances the risks and returns.
\end{enumerate}},
  creationdate     = {2023-06-22T23:18:53},
  modificationdate = {2023-06-22T23:18:53},
}

@Article{Papenbrock-et-al-2021a,
  author           = {Jochen Papenbrock and Peter Schwendner and Philipp Sandner},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Can Adaptive Seriational Risk Parity Tame Crypto Portfolios?},
  doi              = {10.2139/ssrn.3877143},
  abstract         = {As cryptocoins are not tied to fundamental values or to investor protection regulation, their price dynamics is unhinged in both directions.
In institutional asset management of conventional asset classes, target volatility concepts and dynamic allocation heuristics are popular to improve the robustness of portfolio.
Can similar techniques also be used to construct delevered and diversified portfolios of crypto assets? A robust candidate approach for allocation is Hierarchical Risk Parity (HRP), as it incorporates a filtered correlation structure and is less sensitive to noise than quadratic optimization, as shown in several studies. Recent publications have extended the concept of HRP in several directions. We compare some of these extensions to determine which variant is most useful for constructing crypto baskets. We find that a particular type of adaptive HRP strategy outperforms other extensions on a risk-adjusted basis, leading us to a deeper investigation of the changing nature of correlation structures between cryptos - both quantitatively and visually. We find that structural breaks in crypto correlations are prevalent and that the best-fitting hierarchical cluster representations change over time, which is only captured by distance matrix-based adaptive HRP approaches.},
  creationdate     = {2023-06-22T23:19:37},
  modificationdate = {2023-06-22T23:19:37},
  publisher        = {Elsevier {BV}},
}

@Article{Papenbrock-et-al-2021,
  author           = {Jochen Papenbrock and Peter Schwendner and Markus Jaeger and Stephan Krugel},
  date             = {2021-03},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Matrix Evolutions: Synthetic Correlations and Explainable Machine Learning for Constructing Robust Investment Portfolios},
  doi              = {10.3905/jfds.2021.1.056},
  number           = {2},
  pages            = {51--69},
  volume           = {3},
  abstract         = {In this article, the authors present a novel and highly flexible concept to simulate correlation matrixes of financial markets. It produces realistic outcomes regarding stylized facts of empirical correlation matrixes and requires no asset return input data. The matrix generation is based on a multiobjective evolutionary algorithm, so the authors call the approach matrix evolutions. It is suitable for parallel implementation and can be accelerated by graphics processing units and quantum-inspired algorithms. The approach is useful for backtesting, pricing, and hedging correlation-dependent investment strategies and financial products. Its potential is demonstrated in a machine learning case study for robust portfolio construction in a multi-asset universe: An explainable machine learning program links the synthetic matrixes to the portfolio volatility spread of hierarchical risk parity versus equal risk contribution.},
  creationdate     = {2023-06-22T23:19:37},
  modificationdate = {2023-06-22T23:19:37},
  publisher        = {Pageant Media {US}},
}

@Article{Raffinot-2016b,
  author               = {Raffinot, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {Investing Through Economic Cycles with Ensemble Machine Learning Algorithms},
  url                  = {https://ssrn.com/abstract=2785583},
  abstract             = {Ensemble machine learning algorithms, referred to as random forest (Breiman (2001)) and as boosting (Schapire (1990)), are applied to quickly and accurately detect economic turning points in the United States and in the euro area. The two key features of those algorithms are their abilities to entertain a large number of predictors and to perform estimation and variable selection simultaneously. The real-time ability to nowcast economic turning points is gauged. To assess the value of the models, profit maximization measures are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. The investment strategies based on the models achieve impressive risk-adjusted returns: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146764},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2785583},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2785583code2270025.pdf?abstractid=2785583 and mirid=1},
  creationdate         = {2023-06-22T23:20:30},
  day                  = {28},
  groups               = {Machine learning and investment strategies, ML_InvestSelect},
  modificationdate     = {2023-06-22T23:20:30},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:33:00},
}

@Article{Raffinot-2016a,
  author               = {Raffinot, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {Can Macroeconomists Get Rich Nowcasting Economic Turning Points with a Simple Machine-Learning Algorithm?},
  url                  = {https://ssrn.com/abstract=2545256},
  abstract             = {To nowcast economic turning points, probabilistic indicators are created from a simple and transparent machine-learning algorithm known as Learning Vector Quantization (LVQ). The real-time ability of the indicators to quickly and accurately detect economic turning points in the United States and in the euro area is gauged. To assess the value of the indicators, profit maximization measures based on trading strategies are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. A substantial improvement in profit measures over the benchmark is found: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146765},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2545256},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2817407code2270025.pdf?abstractid=2545256 and mirid=1},
  creationdate         = {2023-06-22T23:20:30},
  day                  = {7},
  modificationdate     = {2023-06-22T23:20:30},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:36:04},
}

@Article{Raffinot-2015,
  author               = {Raffinot, Thomas},
  date                 = {2014-10},
  journaltitle         = {SSRN e-Print},
  title                = {A Simple and Efficient Macroeconomic Framework for Asset Allocation},
  url                  = {https://ssrn.com/abstract=2504378},
  abstract             = {The recent financial and economic crisis highlighted the need for a better understanding of the complex relationship between financial markets and the macroeconomy. Many so-called diversified portfolios performed very badly and faced widespread criticism afterwards. To alleviate the difficulties encountered in the context of the Great Recession, this paper attempts to provide a simple and efficient macroeconomic framework for asset allocation.

To this aim, this paper points out the importance of the growth cycle introduced by Mintz (1974) for euro and dollar-based investors. Moreover, Anas and Ferrara (2004) refine the description of different economic phases by jointly considering the classical business cycle and the growth cycle. This article emphasizes how this economic cyclical framework can improve strategic asset allocation choices and that dynamic macro-based regime-switching asset allocations achieve superior risk adjusted returns than static alternative in the euro area and in the United States.},
  citeulike-article-id = {13978621},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2504378},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2601269code2270025.pdf?abstractid=2504378 and mirid=1},
  creationdate         = {2023-06-22T23:20:30},
  day                  = {2},
  modificationdate     = {2023-06-22T23:20:30},
  owner                = {cristi},
  posted-at            = {2016-03-13 00:22:49},
}

@Article{Raffinot-2017,
  author               = {Raffinot, Thomas},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Time-Varying Risk Premiums and Economic Cycles},
  doi                  = {10.2139/ssrn.2949914},
  issn                 = {1556-5068},
  abstract             = {Asset returns are not correlated with the business cycle but are primarily caused by the economic cycles. To validate this claim, economic cycles are first rigorously defined, namely the classical business cycle and the growth cycle, better known as the output gap. The description of different economic phases is refined by jointly considering both economic cycles. It improves the classical analysis of economic cycles by considering sometimes two distinct phases and sometimes four distinct phases. The theoretical influence of economic cycles on time-varying risk premiums is then explained based on two key economic concepts: nominal GDP and adaptive expectations. Simple dynamic investment strategies confirm the importance of economical cycles, especially the growth cycle, for euro and dollar-based investors. At last, this economic cyclical framework can improve strategic asset allocation choices.},
  citeulike-article-id = {14337053},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2949914},
  creationdate         = {2023-06-22T23:20:44},
  groups               = {RiskRet_BusCycle},
  modificationdate     = {2023-06-22T23:20:44},
  posted-at            = {2017-04-13 20:17:53},
}

@Article{Raffinot-2018a,
  author           = {Raffinot, Thomas},
  date             = {2018-08-23},
  journaltitle     = {SSRN e-Print},
  title            = {The Hierarchical Equal Risk Contribution Portfolio},
  url              = {https://ssrn.com/abstract=3237540},
  abstract         = {Building upon the fundamental notion of hierarchy, the "Hierarchical Risk Parity" (HRP) and the "Hierarchical Clustering based Asset Allocation" (HCAA), the Hierarchical Equal Risk Contribution Portfolio (HERC) aims at diversifying capital allocation and risk allocation. HERC merges and enhances the machine learning approach of HCAA and the Top-Down recursive bisection of HRP. In more detail, the modified Top-Down recursive division is based on the shape of dendrogram, follows an Equal Risk Contribution allocation and is extended to downside risk measures such as conditional value at risk (CVaR) and Conditional Drawdown at Risk (CDaR). The out-of-sample performances of hierarchical clustering based portfolios are evaluated across two empirical datasets, which differ in terms of number of assets and composition of the universe (multi-assets and individual stocks). Empirical results highlight that HERC Portfolios based on downside risk measures achieve statistically better risk-adjusted performances, especially those based on the CDaR.},
  creationdate     = {2023-06-22T23:20:48},
  day              = {23},
  f1000-projects   = {QuantInvest},
  groups           = {Invest_Risk, ML_Network_QWIM, ML_InvestSelect},
  modificationdate = {2023-06-22T23:20:48},
}

@Article{Raffinot-Benoit-2018,
  author               = {Raffinot, Thomas and Sylvain Benoit},
  date                 = {2018},
  journaltitle         = {SSRN e-Print},
  title                = {Investing Through Economic Cycles with Ensemble Machine Learning Algorithms},
  url                  = {https://ssrn.com/abstract=2785583},
  abstract             = {Ensemble machine learning algorithms, referred to as random forest (Breiman (2001)) and as boosting (Schapire (1990)), are applied to quickly and accurately detect economic turning points in the United States and in the euro area. The two key features of those algorithms are their abilities to entertain a large number of predictors and to perform estimation and variable selection simultaneously. The real-time ability to nowcast economic turning points is gauged. To assess the value of the models, profit maximization measures are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. The investment strategies based on the models achieve impressive risk-adjusted returns: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146764},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2785583},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2785583code2270025.pdf?abstractid=2785583 and mirid=1},
  creationdate         = {2023-06-22T23:20:55},
  day                  = {28},
  groups               = {Machine learning and investment strategies, ML_InvestSelect},
  modificationdate     = {2023-06-22T23:20:55},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:33:00},
}

@Article{Roncalli-2021,
  author           = {Thierry Roncalli},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Advanced Course in Asset Management},
  doi              = {10.2139/ssrn.3773484},
  abstract         = {These presentation slides have been written for the Advanced Course in Asset Management (theory and applications) given at the University of Paris-Saclay. They contain 15 tutorial exercises and 5 main lectures: 
\begin{enumerate}
\item Portfolio Optimization
\item Risk Budgeting
\item Smart Beta, Factor Investing and Alternative Risk Premia \item Green and Sustainable Finance, ESG Investing and Climate Risk 
\item Machine Learning in Asset Management
\end{enumerate}

 The Table of contents is the following:

\leavevmode\newline  Part 1. Portfolio Optimization
1. Theory of portfolio optimization
1.a. The Markowitz framework
1.b. Capital asset pricing model (CAPM)
1.c. Portfolio optimization in the presence of a benchmark
1.d. Black-Litterman model
2. Practice of portfolio optimization
2.a. Covariance matrix
2.b. Expected returns
2.c. Regularization of optimized portfolios
2.d. Adding constraints
3. Tutorial exercises
3.a. Variations on the efficient frontier
3.b. Beta coefficient
3.c. Black-Litterman model

\leavevmode\newline 

Part 2. Risk Budgeting
1. The ERC portfolio
1.a. Definition
1.b. Special cases
1.c. Properties
1.d. Numerical solution
2. Extensions to risk budgeting portfolios
2.a. Definition of RB portfolios
2.b. Properties of RB portfolios
2.c. Diversification measures
2.d. Using risk factors instead of assets
3. Risk budgeting, risk premia and the risk parity strategy
3.a. Diversified funds
3.b. Risk premium
3.c. Risk parity strategies
3.d. Performance budgeting portfolios
4. Tutorial exercises
4.a. Variation on the ERC portfolio
4.b. Weight concentration of a portfolio
4.c. The optimization problem of the ERC portfolio
4.d. Risk parity funds

\leavevmode\newline 

Part 3. Smart Beta, Factor Investing and Alternative Risk Premia
1. Risk-based indexation
1.a. Capitalization-weighted indexation
1.b. Risk-based portfolios
1.c. Comparison of the four risk-based portfolios
1.d. The case of bonds
2. Factor investing
2.a. Factor investing in equities
2.b. How many risk factors?
2.c. Construction of risk factors
2.d. Risk factors in other asset classes
3. Alternative risk premia
3.a. Definition
3.b. Carry, value, momentum and liquidity
3.c. Portfolio allocation with ARP
4. Tutorial exercises
4.a. Equally-weighted portfolio
4.b. Most diversified portfolio
4.c. Computation of risk-based portfolios
4.d. Building a carry trade exposure

\leavevmode\newline 

Part 4. Green and Sustainable Finance, ESG Investing and Climate Risk
1. ESG investing
1.a. Introduction to sustainable finance
1.b. ESG scoring
1.c. Performance in the stock market
1.d. Performance in the corporate bond market
2. Climate risk
2.a. Introduction to climate risk
2.b. Climate risk modeling
2.c. Regulation of climate risk
2.d. Portfolio management with climate risk
3. Sustainable financing products
3.a. SRI Investment funds
3.b. Green bonds
3.c. Social bonds
3.d. Other sustainability-linked strategies
4. Impact investing
4.a. Definition
4.b. Sustainable development goals (SDG)
4.c. Voting policy, shareholder activism and engagement
4.d. The challenge of reporting
5. Tutorial exercises
5.a. Probability distribution of an ESG score
5.b. Enhanced ESG score and tracking error control

\leavevmode\newline 

Part 5. Machine Learning in Asset Management
1. Portfolio optimization
1.a. Standard optimization algorithms
1.b. Machine learning optimization algorithms
1.c. Application to portfolio allocation
2. Pattern learning and self-automated strategies
3. Market generators
4. Tutorial exercises
4.a. Portfolio optimization with CCD and ADMM algorithms
4.b. Regularized portfolio optimization},
  creationdate     = {2023-06-22T23:21:59},
  modificationdate = {2023-06-22T23:21:59},
  publisher        = {Elsevier {BV}},
}

@Article{Sakurai-et-al-2021,
  author           = {Yutaka Sakurai and Yusuke Yuki and Ryota Katsuki and Takashi Yazane and Fumio Ishizaki},
  date             = {2021},
  journaltitle     = {The Journal of Investment Strategies},
  title            = {Correlation diversified passive portfolio strategy based on permutation of assets},
  doi              = {10.21314/jois.2021.010},
  abstract         = {In this paper we develop a passive strategy to improve index investing, which we call the correlation diversified portfolio strategy. The proposed method adjusts the weight vector of the original index based on the permutation of the assets belonging to the original index. We seek the permutation of these assets such that those assets with a strong correlation to many other assets are placed in the center of the permutation. By reducing the weights of such central assets, we can construct portfolios that are more diversified and have better risk-return characteristics than the original index. We solve this asset-permutation problem by adopting a quantum-inspired approach. Concretely, we convert this permutation problem into a quadratic unconstrained binary optimization problem and use simulated annealing on a personal computer or annealing machine to find a near-optimal solution in a reasonable time. To examine the usefulness and computational feasibility of the proposed method, we apply it to three major indexes of the United States and Japan, and we provide numerical experiments that show portfolios constructed by the proposed method can achieve a higher return with lower volatility compared with the original indexes, while their behaviors are still similar to those of the original indexes.},
  creationdate     = {2023-06-22T23:22:43},
  modificationdate = {2023-06-22T23:22:43},
  publisher        = {Infopro Digital Services Limited},
}

@Article{Scherer-2021,
  author           = {Bernd Scherer},
  date             = {2021-08},
  journaltitle     = {Journal of Asset Management},
  title            = {Adding alternative assets: return enhancement, diversification or hedging?},
  doi              = {10.1057/s41260-021-00238-w},
  pages            = {437-442},
  volume           = {22},
  abstract         = {Adding assets (so-called extensions) to an already existing portfolio is a reoccurring question in times of rapidly expanding investment opportunity sets. Examples for this "how much" question are the incorporation of liquid alternative assets in the form of hedge funds or alternative risk premia in a global balanced portfolio, the addition of global equities to a domestic equity portfolios or simply the optimal allocation of corporate credit within a government debt portfolio. While this is hardly a new question and a variety of tools have already been established, we suggest a new framework to decompose the demand for risky assets in economically meaningful components. This allows us to identify whether a particular allocation is driven by demand created from noisy return estimates or by more predictable hedging and diversification demand.},
  creationdate     = {2023-06-22T23:23:17},
  modificationdate = {2023-06-22T23:23:17},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Schwendner-et-al-2021,
  author           = {Peter Schwendner and Jochen Papenbrock and Markus Jaeger and Stephan Krugel},
  date             = {2021},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Adaptive Seriational Risk Parity and Other Extensions for Heuristic Portfolio Construction Using Machine Learning and Graph Theory},
  doi              = {10.3905/jfds.2021.1.078},
  number           = {4},
  pages            = {65-83},
  volume           = {3},
  abstract         = {In this article, the authors present a conceptual framework named adaptive seriational risk parity (ASRP) to extend hierarchical risk parity (HRP) as an asset allocation heuristic. The first step of HRP (quasi-diagonalization), determining the hierarchy of assets, is required for the actual allocation done in the second step (recursive bisectioning). In the original HRP scheme, this hierarchy is found using single-linkage hierarchical clustering of the correlation matrix, which is a static tree-based method. The authors compare the performance of the standard HRP with other static and adaptive tree-based methods, as well as seriation-based methods that do not rely on trees. Seriation is a broader concept allowing reordering of the rows or columns of a matrix to best express similarities between the elements. Each discussed variation leads to a different time series reflecting portfolio performance using a 20-year backtest of a multi-asset futures universe. Unsupervised learningbased on these time-series creates a taxonomy that groups the strategies in high correspondence to the construction hierarchy of the various types of ASRP. Performance analysis of the variations shows that most of the static tree-based alternatives to HRP outperform the single-linkage clustering used in HRP on a risk-adjusted basis. Adaptive tree methods show mixed results, and most generic seriation-based approaches underperform.},
  creationdate     = {2023-06-22T23:24:19},
  modificationdate = {2023-06-22T23:24:19},
}

@Article{Serur-Avellaneda-2021,
  author           = {Juan Andr{\'{e}}s Serur and Marco Avellaneda},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Hierarchical {PCA} and Modeling Asset Correlations},
  doi              = {10.2139/ssrn.3903460},
  abstract         = {Modeling cross-sectional correlations between thousands of stocks, acrosscountries and industries, can be challenging. In this paper, we demonstratethe advantages of using Hierarchical Principal Component Analysis (HPCA)over the classic PCA. We also introduce a statistical clustering algorithmto identify homogeneous clusters of stocks or "synthetic sectors". We apply these methods to study cross-sectional correlations in the US, Europe, China,and Emerging Markets.},
  creationdate     = {2023-06-22T23:25:03},
  modificationdate = {2023-06-22T23:25:03},
  publisher        = {Elsevier {BV}},
}

@Article{Swedroe-2020a,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {The Importance of Diversification in Achieving Long-Term Goals},
  url              = {https://www.advisorperspectives.com/articles/2020/10/26/the-importance-of-diversification-in-achieving-long-term-goals},
  abstract         = {My 2007 book, Wise Investing Made Simple: Larry Swedroe's Tales to Enrich Your Future, contained 27 tales to educate investors about important investment concepts and strategies. This article is in the spirit of those tales. The examples are hypothetical.},
  creationdate     = {2023-06-22T23:25:38},
  modificationdate = {2023-06-22T23:25:38},
}

@Article{Taljaard-Mare-2021a,
  author           = {Byran H Taljaard and Eben Mar{\'{e}}},
  date             = {2021-01},
  journaltitle     = {Investment Analysts Journal},
  title            = {If the equal weighted portfolio is so great, why isn't it working in South Africa?},
  doi              = {10.1080/10293523.2020.1870863},
  number           = {1},
  pages            = {32--49},
  volume           = {50},
  abstract         = {This paper considers the recent underperformance of the equal weighted portfolio of South African Top 40 stocks relative to the market capitalisation weighted portfolio. It highlights the impact of the increased concentration of market capitalisation weights in the Top 40, which is currently at extreme levels. Furthermore, lower levels in the benefits of diversification, through higher average correlations, has reduced the positive impact of rebalancing. Finally, the turnover in index constituents has been higher than average in recent years and this has caused a further drag on performance. The combination of these effects has had a negative impact on the equal weighted portfolio's relative performance. A rudimentary linear model, with these factors as inputs, that highlights the importance of monitoring these drivers to improve the equal weighted portfolio's relative performance is presented.},
  creationdate     = {2023-06-22T23:26:15},
  modificationdate = {2023-06-22T23:26:15},
  publisher        = {Informa {UK} Limited},
}

@Article{Taljaard-Mare-2021,
  author           = {B. H. Taljaard and E. Mar{\'{e}}},
  date             = {2021-04},
  journaltitle     = {Quantitative Finance},
  title            = {Why has the equal weight portfolio underperformed and what can we do about it?},
  doi              = {10.1080/14697688.2021.1889020},
  number           = {11},
  pages            = {1855-1868},
  volume           = {21},
  abstract         = {It is widely noted that market capitalisation weighted portfolios are inefficient and underperform an equal weighted portfolio over the long-term. However, at least since 2016, an equal weighted portfolio of stocks in the S\&P500 has significantly underperformed the market capitalisation weighted portfolio. In this paper, we analyse this underperformance using stochastic portfolio theory. We show that the equal weighted portfolio does appear to outperform the market capitalisation weighted portfolio over the long-term but with periods of significant short-term underperformance. In addition, we find that concentration in the market capitalisation weighted portfolio has increased in recent years and has contributed to the recent underperformance together with a significantly lower level of diversification benefits. Furthermore, we highlight an approach to improve the performance of a portfolio by dynamically selecting a market cap or an equal weighting using a rudimentary linear regression model.},
  creationdate     = {2023-06-22T23:26:15},
  modificationdate = {2023-06-22T23:26:15},
  publisher        = {Informa {UK} Limited},
}

@Article{Thiagarajan-et-al-2021,
  author           = {Ramu Thiagarajan and Jiho Han and Aaron Hurd and Hanbin Im and Gaurav Mallik},
  date             = {2021-08},
  journaltitle     = {The Journal of Investing},
  title            = {Financial Globalization and Its Implications for Diversification of Portfolio Risk},
  doi              = {10.3905/joi.2021.1.197},
  number           = {6},
  pages            = {22--33},
  volume           = {30},
  abstract         = {Trade disputes and the impact of the COVID-19 pandemic on global supply chains have drawn much attention to the notion of "deglobalization." The common concern is that the steady trend of globalization and its many benefits may reverse. But the globalization trend is not a monolith. In this article, we show that although trade globalization has stalled since the Global Financial Crisis (GFC), financial globalization has continued to increase. We further show that financial globalization has a much more significant impact on portfolios than trade globalization. The primary mechanism of this impact, US dollar hegemony, impacts portfolios primarily through increased spillover of US monetary policy shocks. The two implications for investors are: (1) global equity markets have become increasingly correlated and are likely to stay that way, and (2) this increased correlation reduces the benefits of portfolio diversification and leads to a more concentrated exposure to US monetary policy shocks.},
  creationdate     = {2023-06-22T23:26:47},
  modificationdate = {2023-06-22T23:26:47},
  publisher        = {Pageant Media {US}},
}

@Article{Yuan-Zhou-2022,
  author           = {Yuan, Ming and Zhou, Guofu},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Why Naive 1/N Diversification Is Not So Naive, and How to Beat It?},
  url              = {https://ssrn.com/abstract=3991279},
  abstract         = {In this paper, we study portfolio choice problem under estimation risk and show why the 1/N rule is very difficult to beat in applications and studies. First, as long as the dimensionality is high relative to sample size, we show that the usual estimated investment strategies are biased even asymptotically. Second, we show that the 1/N rule is optimal in a one-factor model with diversifiable risks as dimensionality increases, irrespectively of the sample size, making investment theory-based rules inadequate as they suffer from estimation errors. Third, we provide strategies that can outperform the 1/N under suitable conditions.},
  creationdate     = {2023-06-22T23:27:18},
  modificationdate = {2023-06-22T23:27:18},
}

@Article{Zaimovic-et-al-2021,
  author           = {Azra Zaimovic and Adna Omanovic and Almira Arnaut-Berilo},
  date             = {2021-11},
  journaltitle     = {Journal of Risk and Financial Management},
  title            = {How Many Stocks Are Sufficient for Equity Portfolio Diversification? A Review of the Literature},
  doi              = {10.3390/jrfm14110551},
  number           = {11},
  pages            = {551},
  volume           = {14},
  abstract         = {Using extensive and comprehensive databases to select a subset of research papers, we aim to critically analyze previous empirical studies to identify certain patterns in determining the optimal number of stocks in well-diversified portfolios in different markets, and to compare how the optimal number of stocks has changed over different periods and how it has been affected by market turmoil such as the Global Financial Crisis (GFC) and the current COVID-19 pandemic. The main methods used are bibliometric analysis and systematic literature review. Evaluating the number of assets which lead to optimal diversification is not an easy task as it is impacted by a huge number of different factors: the way systematic risk is measured, the investment universe (size, asset classes and features of the asset classes), the investor's characteristics, the change over time of the asset features, the model adopted to measure diversification (i.e., equally weighted versus optimal allocation), the frequency of the data that is being used, together with the time horizon, conditions in the market that the study refers to, etc. Our paper provides additional support for the fact that (1) a generalized optimal number of stocks that constitute a well-diversified portfolio does not exist for whichever market, period or investor. Recent studies further suggest that (2) the size of a well-diversified portfolio is larger today than in the past, (3) this number is lower in emerging markets compared to developed financial markets, (4) the higher the stock correlations with the market, the lower the number of stocks required for a well-diversified portfolio for individual investors, and (5) machine learning methods could potentially improve the investment decision process. Our results could be helpful to private and institutional investors in constructing and managing their portfolios and provide a framework for future research.},
  creationdate     = {2023-06-22T23:27:53},
  modificationdate = {2023-06-22T23:27:53},
  publisher        = {{MDPI} {AG}},
}

@Article{Zhao-et-al-2021b,
  author           = {Zhihua Zhao and Fengmin Xu and Donglei Du and Wang Meihua},
  date             = {2021-03},
  journaltitle     = {Quantitative Finance},
  title            = {Robust portfolio rebalancing with cardinality and diversification constraints},
  doi              = {10.1080/14697688.2021.1879392},
  number           = {10},
  pages            = {1707-1721},
  volume           = {21},
  abstract         = {In this paper, we develop a robust conditional value at risk (CVaR) optimal portfolio rebalancing model under various financial constraints to construct sparse and diversified rebalancing portfolios. Our model includes transaction costs and double cardinality constraints in order to capture the trade-off between the limit of investment scale and the diversified industry coverage requirement. We first derive a closed-form solution for the robust CVaR portfolio rebalancing model with only transaction costs. This allows us to conduct an industry risk analysis for sparse portfolio rebalancing in the absence of diversification constraints. Then, we attempt to remedy the hidden industry risk by establishing a new robust portfolio rebalancing model with both sparse and diversified constraints. This is followed by the development of a distributed-version of the Alternating Direction Method of Multipliers (ADMM) algorithm, where each subproblem admits a closed-form solution. Finally, we conduct empirical tests to compare our proposed strategy with the standard sparse rebalancing and no-rebalancing strategies. The computational results demonstrate that our rebalancing approach produces sparse and diversified portfolios with better industry coverage. Additionally, to measure out-of-sample performance, two superiority indices are created based on worst-case CVaR and annualized return, respectively. Our ADMM strategy outperforms the sparse rebalancing and no-rebalancing strategies in terms of these indices.},
  creationdate     = {2023-06-22T23:28:44},
  modificationdate = {2023-06-22T23:28:44},
  publisher        = {Informa {UK} Limited},
}

@Article{Jaeger-et-al-2020,
  author           = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Understanding machine learning for diversified portfolio construction by explainable AI},
  doi              = {10.2139/ssrn.3528616},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3528616},
  urldate          = {2020-03-06},
  abstract         = {In this paper, we construct a pipeline to investigate heuristic diversification strategies in asset allocation. We use machine learning concepts ("explainable AI") to compare the robustness of different strategies and back out implicit rules for decision making.In a first step, we augment the asset universe (the empirical dataset) with a range of scenarios generated with a block bootstrap from the empirical dataset.Second, we backtest the candidate strategies over a long period of time, checking their performance variability. Third, we use XGBoost as a regression model to connect the difference between the measured performances between two strategies to a pool of statistical features of the portfolio universe tailored to the investigated strategy. Finally, we employ the concept of Shapley values to extract the relationships that the model could identify between the portfolio characteristics and the statistical properties of the asset universe. We test this pipeline for studying risk-parity strategies with a volatility target, and in particular, comparing the machine learning-driven Hierarchical Risk Parity (HRP) to the classical Equal Risk Contribution (ERC) strategy.In the augmented dataset built from a multi-asset investment universe of commodities, equities and fixed income futures, we find that HRP better matches the volatility target, and shows better risk-adjusted performances. Finally, we train XGBoost to learn the difference between the realized Calmar ratios of HRP and ERC and extract explanations.The explanations provide fruitful ex-post indications of the connection between the statistical properties of the universe and the strategy performance in the training set. For example, the model confirms that features addressing the hierarchical properties of the universe are connected to the relative performance of HRP respect to ERC.},
  creationdate     = {2023-06-22T23:30:46},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_ML},
  modificationdate = {2023-06-22T23:30:46},
  timestamp        = {2020-07-23 13:37},
}

@Comment{jabref-meta: databaseType:biblatex;}

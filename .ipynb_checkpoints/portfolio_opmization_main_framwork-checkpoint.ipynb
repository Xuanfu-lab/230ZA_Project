{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528ed59a-aa55-4865-84b3-b0be86e19853",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37164db-85c9-4333-af0c-d7de01514c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "# import mgarch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Additional Auxilliary Functions\n",
    "from scipy.stats import kurtosis, skew, jarque_bera\n",
    "\n",
    "# Cosmetics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2efd4-2c22-4251-9e52-ffc83bd6027e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ba89d-c709-43fd-bb9c-a1fdb85bd755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full data\n",
    "start_date = \"1995-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "price_long = pd.read_csv(\"data_clean/long_format_daily.csv\")\n",
    "price_long[\"Date\"] = pd.to_datetime(price_long[\"Date\"])\n",
    "price_long = price_long.loc[(price_long[\"Date\"] >= start_date) & (price_long[\"Date\"] <= end_date), :]\n",
    "\n",
    "# very short data for testing\n",
    "price_long_test = price_long.loc[(price_long[\"Date\"] >= '2022-01-01'), :]\n",
    "price_wide_test = price_long_test.pivot(index='Date', columns='Ticker', values='Price')\n",
    "return_wide_test = price_wide_test.pct_change().iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a3761-764e-49b2-9275-ea04f20918b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data used to replicate paper results\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2020-06-30\"\n",
    "\n",
    "price_long_paper = pd.read_csv(\"data_clean/long_format_daily_original_paper_data.csv\")\n",
    "price_long_paper[\"Date\"] = pd.to_datetime(price_long_paper[\"Date\"])\n",
    "price_long_paper = price_long_paper.loc[(price_long_paper[\"Date\"] >= start_date) & (price_long_paper[\"Date\"] <= end_date), :]\n",
    "price_wide_paper = price_long_paper.pivot(index='Date', columns='Ticker', values='Price')\n",
    "\n",
    "# VTI, AGG, DBC, VIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eaba33-7b2e-44d3-b24b-9e8e974c4a21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e2db7-0811-4e7c-8437-996e399ecd6c",
   "metadata": {},
   "source": [
    "## Optimization Framework - Traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd12c87-413a-472d-81e3-8cd2266300bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def portfolio_optimization(returns, loss_func:str, cov_estimation:str='historical'):\n",
    "    # return the portfolio weight for 1 rebalance period \n",
    "    def constraint_basic(weights):\n",
    "        return np.sum(weights) - 1\n",
    "    \n",
    "    def constraint_long_only(weights):\n",
    "        return weights    # make sure weights are positive\n",
    "    \n",
    "    def objective_variance(weights):\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        return portfolio_volatility\n",
    "    \n",
    "    def objective_sharpe(weights):\n",
    "        portfolio_return = np.dot(mean_returns, weights)\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        return - portfolio_return / portfolio_volatility\n",
    "    \n",
    "    def objective_markowitz3(weights):\n",
    "        a = 1000 # hyperparameter tuning through grid search\n",
    "        b = 1\n",
    "        portfolio_return = np.dot(mean_returns, weights)\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        sharpe = portfolio_return / portfolio_volatility\n",
    "        entropy = np.exp(-np.sum(weights * np.log(weights)))\n",
    "        return -(a * sharpe + b * entropy) / (a + b)\n",
    "    \n",
    "    def objective_divers_ratio(weights):\n",
    "        portfolio_variance = 0\n",
    "        weighted_sum = 0\n",
    "        for i in range(cov_matrix.shape[0]):\n",
    "            portfolio_variance += weights[i]**2*cov_matrix[i][i]\n",
    "            weighted_sum += weights[i]**2*cov_matrix[i][i]\n",
    "            for j in range(cov_matrix.shape[0]):\n",
    "                if i>j:\n",
    "                    portfolio_variance+= 2*weights[i]*weights[j]*cov_matrix[i][j]\n",
    "        DR = np.sqrt(weighted_sum/portfolio_variance)\n",
    "        return - DR\n",
    "\n",
    "    def objective_marginal_risk_contribution(weights):\n",
    "        portfolio_TR = 0\n",
    "        portfolio_variance = 0\n",
    "        for i in range(cov_matrix.shape[0]):\n",
    "            portfolio_variance += weights[i]**2*cov_matrix[i][i]\n",
    "            RC_i = 2*weights[i]**2*cov_matrix[i][i]\n",
    "            for j in range(cov_matrix.shape[0]):\n",
    "                if j!=i:\n",
    "                    portfolio_variance+= weights[i]*weights[j]*cov_matrix[i][j]\n",
    "                    RC_i += weights[i]*weights[j]*cov_matrix[i][j]\n",
    "            portfolio_TR+=RC_i\n",
    "        return portfolio_TR/np.sqrt(portfolio_variance)\n",
    "    \n",
    "    # basic feature\n",
    "    tickers = returns.columns\n",
    "    date = returns.index[-1]\n",
    "    num_assets = returns.shape[1]\n",
    "    mean_returns = np.mean(returns, axis=0)\n",
    "    annualized_vol = returns.std() * np.sqrt(252 / 66)\n",
    "    \n",
    "    # Covariance Matrix:\n",
    "    if cov_estimation == 'historical':\n",
    "        cov_matrix = np.cov(returns.T, ddof=1)\n",
    "    if cov_estimation == 'regularized':\n",
    "        c = 0.00005\n",
    "        cov_matrix = np.cov(returns.T, ddof=1) \n",
    "        cov_matrix += c * np.eye(cov_matrix.shape[0])\n",
    "    if cov_estimation == 'DCC_GARCH':\n",
    "        vol = mgarch.mgarch()\n",
    "        vol.fit(returns)\n",
    "        ndays = 1\n",
    "        cov_matrix = vol.predict(ndays)['cov']\n",
    "    \n",
    "    # calculate weight by minimizing objective, subject to constraint\n",
    "    constraints = [{'type': 'eq', 'fun': constraint_basic}, {'type': 'ineq', 'fun': constraint_long_only}]\n",
    "    init_weights = np.array([1 / num_assets] * num_assets)\n",
    "    if loss_func == 'equal_weight':\n",
    "        return pd.DataFrame({'Date':date, \n",
    "                             'Ticker':tickers, \n",
    "                             'Weight':init_weights, \n",
    "                             'Annualized_vol':annualized_vol\n",
    "                            })\n",
    "    objective = locals()['objective_' + loss_func] # call the objective function associated with input\n",
    "    weights = minimize(objective, init_weights, method='SLSQP', constraints=constraints).x\n",
    "    \n",
    "    return pd.DataFrame({'Date':date, \n",
    "                         'Ticker':tickers, \n",
    "                         'Weight':weights, \n",
    "                         'Annualized_vol':annualized_vol\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae835ef-b384-4ba8-8af4-6dd1d79e4766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_portfolio(price_long, loss_func:str, cov_estimation:str='historical', period=66):\n",
    "    price_wide = price_long.pivot(index='Date', columns='Ticker', values='Price')\n",
    "    return_wide = price_wide.pct_change().iloc[1:,:] #drop 1st row\n",
    "    n_rebalance = return_wide.shape[0] // period\n",
    "    \n",
    "    chunks = [return_wide.iloc[i:i+period, :] for i in range(0, period * n_rebalance, period)]\n",
    "    weights = pd.DataFrame(columns=['Date', \n",
    "                                    'Ticker', \n",
    "                                    'Weight', \n",
    "                                    'Annualized_vol'])\n",
    "    \n",
    "    for chunk in chunks: # each chunk is a return data (wide format)\n",
    "        na_threshold = 5 # drop ticker w/ >5 NaN, fill ticker w/ <=5 NaN with 0\n",
    "        returns = chunk.dropna(thresh = period-na_threshold, axis=1)\n",
    "        returns = returns.fillna(0)\n",
    "        chunk_weights = portfolio_optimization(returns, loss_func, cov_estimation)\n",
    "        weights = pd.concat([weights, chunk_weights], axis=0, ignore_index=True)\n",
    "    \n",
    "    weights = pd.merge(weights, price_long[['Date', 'Ticker', 'Price']], on=['Date', 'Ticker'], how='right')\n",
    "    weights.sort_values('Date',inplace=True)\n",
    "    weights[\"Weight\"] = weights.groupby(\"Ticker\")[\"Weight\"].fillna(method='ffill',axis=0)\n",
    "    weights[\"Annualized_vol\"] = weights.groupby(\"Ticker\")[\"Annualized_vol\"].fillna(method='ffill',axis=0)\n",
    "    return weights.reset_index(drop=True)\n",
    "\n",
    "# def optimize_portfolio(price_long, loss_func:str, cov_estimation:str='historical', period=66):\n",
    "#     price_wide = price_long.pivot(index='Date', columns='Ticker', values='Price')\n",
    "#     return_wide = price_wide.pct_change().iloc[1:,:] #drop 1st row\n",
    "#     n_rebalance = return_wide.shape[0] // period\n",
    "    \n",
    "#     chunks = [return_wide.iloc[i:i+period, :] for i in range(0, period * n_rebalance, period)]\n",
    "#     weights = pd.DataFrame(columns=['Date', \n",
    "#                                     'Ticker', \n",
    "#                                     'Weight', \n",
    "#                                     'Annualized_vol'])\n",
    "    \n",
    "#     for chunk in chunks: # each chunk is a return data (wide format)\n",
    "#         na_threshold = 5 # drop ticker w/ >5 NaN, fill ticker w/ <=5 NaN with 0\n",
    "#         returns = chunk.dropna(thresh = period-na_threshold, axis=1)\n",
    "#         returns = returns.fillna(0)\n",
    "#         chunk_weights = portfolio_optimization(returns, loss_func, cov_estimation)\n",
    "#         weights = pd.concat([weights, chunk_weights], axis=0, ignore_index=True)\n",
    "    \n",
    "#     weights = pd.merge(weights, price_long[['Date', 'Ticker', 'Price']], on=['Date', 'Ticker'], how='left')\n",
    "#     return weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b815176-a135-442f-ac43-48300b57a050",
   "metadata": {},
   "source": [
    "## Optimization Framework - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc6a9-100b-4920-942c-312b1b02ee16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        \n",
    "    def __build_model(self, input_shape, outputs):\n",
    "        '''\n",
    "        Builds and returns the Deep Neural Network that will compute the allocation ratios\n",
    "        that optimize the Sharpe Ratio of the portfolio\n",
    "        \n",
    "        inputs: input_shape - tuple of the input shape, outputs - the number of assets\n",
    "        returns: a Deep Neural Network model\n",
    "        '''\n",
    "        model = Sequential([\n",
    "            LSTM(64, input_shape=input_shape),\n",
    "            Flatten(),\n",
    "            Dense(outputs, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        def sharpe_loss(_, y_pred):\n",
    "            # make all time-series start at 1\n",
    "            data = tf.divide(self.data, self.data[0])  \n",
    "            \n",
    "            # value of the portfolio after allocations applied\n",
    "            portfolio_values = tf.reduce_sum(tf.multiply(data, y_pred), axis=1) \n",
    "            \n",
    "            portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "            sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "            # since we want to maximize Sharpe, while gradient descent minimizes the loss, \n",
    "            #   we can negate Sharpe (the min of a negated function is its max)\n",
    "            return -sharpe\n",
    "        \n",
    "        model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    def get_allocations(self, data: pd.DataFrame):\n",
    "        '''\n",
    "        Computes and returns the allocation ratios that optimize the Sharpe over the given data\n",
    "        \n",
    "        input: data - DataFrame of historical closing prices of various assets\n",
    "        \n",
    "        return: the allocations ratios for each of the given assets\n",
    "        '''\n",
    "        \n",
    "        # data with returns\n",
    "        data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "        data = data.iloc[1:]\n",
    "        self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "        fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "        self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=20, shuffle=False,\n",
    "                       verbose=0\n",
    "                      )\n",
    "        return self.model.predict(fit_predict_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3bed3-84f7-4bd4-a5b8-252e7668d3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def portfolio_optimization_LSTM(price_wide):\n",
    "    # basic feature\n",
    "    tickers = price_wide.columns\n",
    "    date = price_wide.index[-1]\n",
    "    return_wide = price_wide.pct_change().dropna()\n",
    "    num_assets = price_wide.shape[1]\n",
    "    annualized_vol = return_wide.std() * np.sqrt(252 / 65)\n",
    "    \n",
    "    model = Model()\n",
    "    weights = model.get_allocations(price_wide)\n",
    "    \n",
    "    return pd.DataFrame({'Date':date, \n",
    "                         'Ticker':tickers, \n",
    "                         'Weight':weights, \n",
    "                         'Annualized_vol':annualized_vol\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb872bce-9e30-4cc0-b542-b88a66f96a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_portfolio_LSTM(price_long, period=66):\n",
    "    price_wide = price_long.pivot(index='Date', columns='Ticker', values='Price')\n",
    "    n_rebalance = price_wide.shape[0] // period\n",
    "    \n",
    "    chunks = [price_wide.iloc[i:i+period, :] for i in range(0, period * n_rebalance, period)]\n",
    "    weights = pd.DataFrame(columns=['Date', \n",
    "                                    'Ticker', \n",
    "                                    'Weight', \n",
    "                                    'Annualized_vol'])\n",
    "    \n",
    "    for chunk in chunks: # each chunk is a price data (wide format)\n",
    "        na_threshold = 5 # drop ticker w/ >5 NaN, fill ticker w/ <=5 NaN with 0\n",
    "        prices = chunk.dropna(thresh = period-na_threshold, axis=1)\n",
    "        prices = prices.fillna(method = 'ffill')\n",
    "        prices = prices.iloc[-50:, :] # paper says only use that last 50 days info\n",
    "        \n",
    "        chunk_weights = portfolio_optimization_LSTM(prices)\n",
    "        weights = pd.concat([weights, chunk_weights], axis=0, ignore_index=True)\n",
    "    \n",
    "    weights = pd.merge(weights, price_long[['Date', 'Ticker', 'Price']], on=['Date', 'Ticker'], how='right')\n",
    "    weights.sort_values('Date',inplace=True)\n",
    "    weights[\"Weight\"] = weights.groupby(\"Ticker\")[\"Weight\"].fillna(method='ffill',axis=0)\n",
    "    weights[\"Annualized_vol\"] = weights.groupby(\"Ticker\")[\"Annualized_vol\"].fillna(method='ffill',axis=0)\n",
    "    return weights.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f18bf-3454-4d86-b588-5014cb7f594f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backtest Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea56d56-a4bb-4a28-92f4-35aeb106709d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxuliary Functions\n",
    "def maximum_drawdown(pnl: pd.Series):\n",
    "    nav = (1 + pnl).cumprod()\n",
    "    running_max = nav.cummax()\n",
    "    drawdown = (nav - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    max_drawdown_date = drawdown.idxmin()\n",
    "    \n",
    "    if max_drawdown == 0:\n",
    "        return [drawdown, 0, np.NaN, np.NaN]\n",
    "    \n",
    "    def find_nearest_zeros(series):\n",
    "        min_index = series.idxmin()\n",
    "        left_zero_index = series[series <= 0].loc[:min_index][::-1].idxmax()\n",
    "        righ_zero_index = series[series <= 0].loc[min_index:].idxmax()\n",
    "        return left_zero_index, righ_zero_index\n",
    "    \n",
    "    drawdown_start_date, drawdown_end_date = find_nearest_zeros(drawdown)\n",
    "    recovery_days = (drawdown_end_date - drawdown_start_date).total_seconds() / 86400.0\n",
    "    \n",
    "    return [drawdown, max_drawdown, max_drawdown_date, recovery_days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc96e15-b007-409d-b21d-189ee544ff29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backtest(weight_long):\n",
    "    data = weight_long.copy()\n",
    "    data[\"ret\"] = data.groupby(\"Ticker\")[\"Price\"].transform(lambda x: x.shift(-1)/x -1)\n",
    "    data[\"pnl\"] = data[\"ret\"] * data[\"Weight\"]\n",
    "    port_pnl = data.groupby(\"Date\").apply(lambda x: np.sum(x[\"pnl\"]))\n",
    "    port_pnl.index = pd.to_datetime(port_pnl.index)\n",
    "    port_nav = (1 + port_pnl).cumprod()\n",
    "    port_vol = weight_long.groupby('Date').apply(lambda x: np.sqrt(np.sum((x['Weight'] * x['Annualized_vol'])**2)))\n",
    "    \n",
    "    def calc_var(port_pnl, confidence_level = 0.05):\n",
    "        sorted_pnl = port_pnl.sort_values()\n",
    "        index = int(confidence_level * len(sorted_pnl))\n",
    "        return sorted_pnl.iloc[index]\n",
    "    \n",
    "    def calc_cvar(port_pnl, confidence_level = 0.05):\n",
    "        var = calc_var(port_pnl, confidence_level)\n",
    "        pnl_below_var = port_pnl[port_pnl <= var]\n",
    "        return pnl_below_var.mean()\n",
    "    \n",
    "    avg_annual_ret = port_pnl.mean() * 252\n",
    "    avg_annual_std = port_pnl.std() * np.sqrt(252)\n",
    "    s = skew(port_pnl)\n",
    "    k = kurtosis(port_pnl)\n",
    "    sharpe_ratio = avg_annual_ret / avg_annual_std\n",
    "    adj_sharpe_ratio = sharpe_ratio * (1 + s/6*sharpe_ratio - k/24*sharpe_ratio**2)\n",
    "    drawdown_results = maximum_drawdown(port_pnl)\n",
    "    \n",
    "    shannon_entropy = data.groupby(\"Date\")[\"Weight\"].apply(lambda x: np.exp(-np.sum(x * np.log(x))))\n",
    "    shannon_entropy.index = port_pnl.index\n",
    "    weighted_vol = weight_long.groupby('Date').apply(lambda x: np.sum(x['Weight'] * x['Annualized_vol']))\n",
    "    diversification_ratio = weighted_vol / port_vol\n",
    "    \n",
    "    return {'avg annualized ret': avg_annual_ret,\n",
    "            'avg annualized std': avg_annual_std,\n",
    "            'sharpe ratio': sharpe_ratio, \n",
    "            'adjusted sharpe ratio': adj_sharpe_ratio,\n",
    "            'skewness': s,\n",
    "            'excess kurtosis': k,\n",
    "            'maximum drawdown': drawdown_results[1],\n",
    "            'maximum drawdown length (days)': drawdown_results[3],\n",
    "            'VaR (95%)': calc_var(port_pnl),\n",
    "            'CVaR (95%)': calc_cvar(port_pnl),\n",
    "            # 'shannon entropy mean': shannon_entropy.mean(),\n",
    "            # 'shannon entropy std': shannon_entropy.std(),\n",
    "            # 'diversification ratio mean': diversification_ratio.mean(),\n",
    "            # 'diversification ratio std': diversification_ratio.std(),\n",
    "            'effective number of uncorrelated bets': np.square(diversification_ratio).mean(),\n",
    "            # time series data below\n",
    "            'pnl': port_pnl,\n",
    "            'nav': port_nav,\n",
    "            # 'annualized_realized_vol': port_vol,\n",
    "            'drawdown': drawdown_results[0],\n",
    "            # 'shannon_entropy': shannon_entropy,\n",
    "            # 'diversification_ratio': diversification_ratio,\n",
    "           }\n",
    "\n",
    "\n",
    "def comparison_table(result_list):\n",
    "    # this compiles all numerical results (not a timeseries)\n",
    "    comparison = pd.DataFrame()\n",
    "    for result in result_list:\n",
    "        float_values = {key: value for key, value in result.items() if isinstance(value, float)}\n",
    "        tmp_df = pd.DataFrame(float_values.values(), index=float_values.keys())\n",
    "        comparison = pd.concat([comparison, tmp_df], axis = 1)\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9065c-8306-4c02-a015-c282124de25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_backtest_results(list_of_weight_long, list_of_names = []):\n",
    "    list_of_results = []\n",
    "    for weight_long in list_of_weight_long:\n",
    "        list_of_results.append(backtest(weight_long))\n",
    "\n",
    "    table = comparison_table(list_of_results)\n",
    "    if list_of_names != []:\n",
    "        table.columns = list_of_names\n",
    "    display(table)\n",
    "\n",
    "    plot_lists = ['nav', \n",
    "                  'pnl', \n",
    "                  # 'annualized_realized_vol', \n",
    "                  'drawdown',\n",
    "                  # 'shannon_entropy', \n",
    "                  # 'diversification_ratio'\n",
    "                 ]\n",
    "    fig, axs = plt.subplots(len(plot_lists), figsize = (10, 5*len(plot_lists)))\n",
    "    for i in range(len(plot_lists)):\n",
    "        for j in range(len(list_of_results)):\n",
    "            if list_of_names != []:\n",
    "                axs[i].plot(list_of_results[j][plot_lists[i]], label = f\"{list_of_names[j]}\")\n",
    "            else:\n",
    "                axs[i].plot(list_of_results[j][plot_lists[i]], label = f\"Strategy {j + 1}\")\n",
    "        axs[i].legend()\n",
    "        axs[i].set_title(f\"{plot_lists[i]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7bc95-f570-44d8-94b8-f35279cc6a20",
   "metadata": {},
   "source": [
    "# Paper Data - Static Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe56bee-329a-4358-8508-14f29cd719bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VTI = price_wide_paper['VTI'] / price_wide_paper['VTI'][0]\n",
    "AGG = price_wide_paper['AGG'] / price_wide_paper['AGG'][0]\n",
    "DBC = price_wide_paper['DBC'] / price_wide_paper['DBC'][0]\n",
    "VIX = price_wide_paper['VIX'] / price_wide_paper['VIX'][0]\n",
    "\n",
    "assets = [VTI, AGG, DBC, VIX]\n",
    "\n",
    "allocation_1 = [1/4, 1/4, 1/4, 1/4]\n",
    "allocation_2 = [1/2, 1/10, 1/5, 1/5]\n",
    "allocation_3 = [1/10, 1/2, 1/5, 1/5]\n",
    "allocation_4 = [2/5, 2/5, 1/10, 1/10]\n",
    "\n",
    "\n",
    "NAV_1 = pd.Series(pd.DataFrame(assets).T.to_numpy() @ np.array(allocation_1), index = price_wide_paper.index)\n",
    "NAV_2 = pd.Series(pd.DataFrame(assets).T.to_numpy() @ np.array(allocation_2), index = price_wide_paper.index)\n",
    "NAV_3 = pd.Series(pd.DataFrame(assets).T.to_numpy() @ np.array(allocation_3), index = price_wide_paper.index)\n",
    "NAV_4 = pd.Series(pd.DataFrame(assets).T.to_numpy() @ np.array(allocation_4), index = price_wide_paper.index)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(NAV_1, label = 'Allocation 1')\n",
    "plt.plot(NAV_2, label = 'Allocation 2')\n",
    "plt.plot(NAV_3, label = 'Allocation 3')\n",
    "plt.plot(NAV_4, label = 'Allocation 4')\n",
    "plt.legend()\n",
    "plt.title(\"Replication of Static Allocations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a6363-a08e-4bdc-bf82-ab7f2e2113f5",
   "metadata": {},
   "source": [
    "# Paper Data - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338490b6-118f-46bb-86b9-fe47c4dcffc3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.keras.utils.set_random_seed(100)\n",
    "weight_paper_LSTM = optimize_portfolio_LSTM(price_long_paper, period = 252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1fd5d-cdf1-4e63-919d-672011cf6bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_backtest_results([weight_paper_LSTM], [\"LSTM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93348d1-aea1-4f9a-bf3c-78639c6ac890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(NAV_1, label = 'Allocation 1')\n",
    "plt.plot(NAV_2, label = 'Allocation 2')\n",
    "plt.plot(NAV_3, label = 'Allocation 3')\n",
    "plt.plot(NAV_4, label = 'Allocation 4')\n",
    "plt.plot(backtest(weight_paper_LSTM)['nav'], label = 'LSTM')\n",
    "plt.legend()\n",
    "plt.title(\"Replication of Paper Result\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d38f5-fe80-43a4-b0a7-dde380ac5fe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Paper Data - Traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb3420-95b4-4e01-9455-43d957a4fe5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_paper_variance = optimize_portfolio(price_long_paper, loss_func='variance', period = 252)\n",
    "weight_paper_sharpe = optimize_portfolio(price_long_paper, loss_func='sharpe', period = 252)\n",
    "weight_paper_markowitz3 = optimize_portfolio(price_long_paper, loss_func='markowitz3', period = 252)\n",
    "# weight_paper_DCC_GARCH = optimize_portfolio(price_long_paper, loss_func='variance', cov_estimation='DCC_GARCH', period = 252)\n",
    "weight_paper_DR = optimize_portfolio(price_long_paper, loss_func='divers_ratio', period = 252)\n",
    "weight_paper_MRC = optimize_portfolio(price_long_paper, loss_func='marginal_risk_contribution', period = 252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998db916-b913-44e0-8d41-38f02216e38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_backtest_results(\n",
    "    [weight_paper_LSTM,\n",
    "     weight_paper_variance,\n",
    "     weight_paper_sharpe,\n",
    "     weight_paper_markowitz3,\n",
    "     weight_paper_DR,\n",
    "     weight_paper_MRC\n",
    "    ],\n",
    "    [\"LSTM\", \"minVar\", \"maxSharpe\", \"Markowitz 3.0\", \"maxDR\", \"minMRC\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c2423-e1ca-4ed2-9950-21caa447340c",
   "metadata": {},
   "source": [
    "# Better Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92280d9-ece3-4e7c-9b63-c5f580bc3bc6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight_better_LSTM = optimize_portfolio_LSTM(price_long, period = 252)\n",
    "# weight_better_variance = optimize_portfolio(price_long, loss_func='variance', period = 252)\n",
    "# weight_better_sharpe = optimize_portfolio(price_long, loss_func='sharpe', period = 252)\n",
    "# weight_better_markowitz3 = optimize_portfolio(price_long, loss_func='markowitz3', period = 252)\n",
    "# weight_better_DR = optimize_portfolio(price_long, loss_func='divers_ratio', period = 252)\n",
    "# weight_better_MRC = optimize_portfolio(price_long, loss_func='marginal_risk_contribution', period = 252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225c136-e809-4f34-aa95-d47ccf74fc56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display_backtest_results(\n",
    "#     [weight_better_LSTM,\n",
    "#      weight_better_variance,\n",
    "#      weight_better_sharpe,\n",
    "#      weight_better_markowitz3,\n",
    "#      weight_better_DR,\n",
    "#      weight_better_MRC\n",
    "#     ],\n",
    "#     [\"LSTM\", \"minVar\", \"maxSharpe\", \"Markowitz 3.0\", \"maxDR\", \"minMRC\"]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
